conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
TAFFC2021,affective,Editorial: Transactions on Affective Computing - Affective Computing in the Times of Pandemics.,2,Personal Information Management and User Behavior,"Affective computing,Pandemic,Computer science,Internet privacy,Psychology,Computer security,Data science,Cognitive psychology,Human–computer interaction,Coronavirus disease 2019 (COVID-19),Medicine,Disease,Pathology,Infectious disease (medical specialty)",,,Elisabeth André,
TAFFC2023,affective,Editorial Transactions on Affective Computing-News on the Journal.,0,Personal Information Management and User Behavior,"Affective computing,Computer science,Data science,World Wide Web,Psychology,Internet privacy,Information retrieval,Cognitive science,Human–computer interaction",,,Elisabeth André,
TAFFC2022,affective,Editorial: Transactions on Affective Computing - Another Year in the Shade of Covid-19.,0,Mental Health Research Topics,"Coronavirus disease 2019 (COVID-19),Affective computing,2019-20 coronavirus outbreak,Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2),Computer science,Psychology,Cognitive psychology,Data science,Artificial intelligence,Medicine,Virology,Disease,Pathology,Outbreak,Infectious disease (medical specialty)",,,Elisabeth André,
TAFFC2021,emotions,The Ordinal Nature of Emotions: An Emerging Approach.,102,"Advanced Text Analysis Techniques,Emotion and Mood Recognition,Rough Sets and Fuzzy Logic","Task (project management),Cognitive psychology,Computer science,Representation (politics),Value (mathematics),Preference,Class (philosophy),Artificial intelligence,Annotation,Psychology,Natural language processing,Machine learning,Mathematics,Statistics,Management,Politics,Political science,Law,Economics",,,"Georgios N. Yannakakis,Roddy Cowie,Carlos Busso",
TAFFC2023,affective,"Guest Editorial: Special Issue on Affective Speech and Language Synthesis, Generation, and Conversion.",3,"Emotion and Mood Recognition,Infant Health and Development,Speech Recognition and Synthesis","Speech synthesis,Speech recognition,Computer science,Psychology,Linguistics,Cognitive science,Natural language processing,Cognitive psychology,Philosophy",,,"Shahin Amiriparian,Björn W. Schuller,Nabiha Asghar,Heiga Zen,Felix Burkhardt",
TAFFC2024,affective,Guest Editorial: Ethics in Affective Computing.,0,"Ethics and Social Impacts of AI,Privacy, Security, and Data Protection,Innovative Human-Technology Interaction","Affective computing,Psychology,Computer science,Cognitive science,Human–computer interaction,Engineering ethics,Engineering",,,"Jonathan Gratch,Gretchen Greene,Rosalind W. Picard,Lachlan Urquhart,Michel F. Valstar",
TAFFC2021,emotions,Recognizing Induced Emotions of Movie Audiences from Multimodal Information.,63,"Emotion and Mood Recognition,Color perception and design,Music and Audio Processing","Perception,Psychology,Affect (linguistics),Content (measure theory),Affective computing,Cognitive psychology,Emotion recognition,Emotion perception,Task (project management),Discriminative model,Social psychology,Computer science,Communication,Artificial intelligence,Mathematical analysis,Mathematics,Management,Neuroscience,Economics",,,"Michal Muszynski,Leimin Tian,Catherine Lai,Johanna D. Moore,Theodoros Kostoulas,Patrizia Lombardo,Thierry Pun,Guillaume Chanel",
TAFFC2022,affect,Affect Estimation in 3D Space Using Multi-Task Active Learning for Regression.,28,"Emotion and Mood Recognition,Music and Audio Processing,Face and Expression Recognition","Machine learning,Artificial intelligence,Computer science,Task (project management),Affective computing,Regression,Affect (linguistics),Valence (chemistry),Sample (material),Selection (genetic algorithm),Mathematics,Statistics,Psychology,Engineering,Systems engineering,Communication,Physics,Chemistry,Chromatography,Quantum mechanics",,,"Dongrui Wu,Jian Huang",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
IF2024,emotion,Online multi-hypergraph fusion learning for cross-subject emotion recognition.,3,"Emotion and Mood Recognition,Face and Expression Recognition,Human Pose and Action Recognition","Hypergraph,Computer science,Subject (documents),Artificial intelligence,Fusion,Emotion recognition,Natural language processing,Pattern recognition (psychology),Machine learning,World Wide Web,Mathematics,Linguistics,Combinatorics,Philosophy",,,"Tongjie Pan,Yalan Ye,Yangwuyong Zhang,Kunshu Xiao,Hecheng Cai",
IF2024,emotion,Emotion detection for misinformation: A review.,9,"Misinformation and Its Impacts,Sentiment Analysis and Opinion Mining,Spam and Phishing Detection","Misinformation,Computer science,Emotion detection,Artificial intelligence,Computer security,Emotion recognition",,,"Zhiwei Liu,Tianlin Zhang,Kailai Yang,Paul Thompson,Zeping Yu,Sophia Ananiadou",
IF2023,emotion,Emotion recognition from unimodal to multimodal analysis: A review.,55,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,EEG and Brain-Computer Interfaces","Multimodality,Modality (human–computer interaction),Computer science,Variety (cybernetics),Artificial intelligence,Multimodal learning,Field (mathematics),Multimodal therapy,Deep learning,Omnipresence,Human–computer interaction,Psychology,World Wide Web,Philosophy,Mathematics,Epistemology,Pure mathematics,Psychotherapist",,,"Kaouther Ezzameli,Hela Mahersia",
IF2023,emotions,"Human-centered neural reasoning for subjective content processing: Hate speech, emotions, and humor.",23,"Hate Speech and Cyberbullying Detection,Sentiment Analysis and Opinion Mining,Misinformation and Its Impacts","Computer science,Offensive,Natural language processing,Task (project management),Artificial intelligence,Representation (politics),Personalization,Quality (philosophy),Lexical diversity,Content (measure theory),Linguistics,Mathematical analysis,Philosophy,Mathematics,Management,Epistemology,Politics,World Wide Web,Political science,Law,Economics,Vocabulary",,,"Przemyslaw Kazienko,Julita Bielaniewicz,Marcin Gruza,Kamil Kanclerz,Konrad Karanowski,Piotr Milkowski,Jan Kocon",
IF2023,"affective,emomv",EmoMV: Affective music-video correspondence learning datasets for classification and retrieval.,10,"Music and Audio Processing,Neuroscience and Music Perception,Emotion and Mood Recognition","Computer science,Benchmark (surveying),Artificial intelligence,Ground truth,Deep learning,Modalities,Binary classification,Machine learning,Benchmarking,Binary number,Artificial neural network,Pattern recognition (psychology),Support vector machine,Social science,Arithmetic,Mathematics,Geodesy,Marketing,Sociology,Business,Geography",,,"Ha Thi Phuong Thao,Gemma Roig,Dorien Herremans",
IF2024,emotion,Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations.,103,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Affective computing,Modalities,Emotion recognition,Systematic review,Facial expression,Artificial intelligence,Human–computer interaction,MEDLINE,Social science,Political science,Law,Sociology",,,"Smith K. Khare,Victoria Blanes-Vidal,Esmaeil S. Nadimi,U. Rajendra Acharya",
IF2024,emotion,"Multimodal Emotion Recognition with Deep Learning: Advancements, challenges, and future directions.",39,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques","Computer science,Realm,Deep learning,Data science,Domain (mathematical analysis),Affective computing,Sentiment analysis,Artificial intelligence,Key (lock),Comprehension,Focus (optics),Human–computer interaction,Mathematical analysis,Physics,Mathematics,Computer security,Optics,Political science,Law,Programming language",,,"Geetha Vijayaraghavan,Mala T.,Priyanka D.,Uma E.",
TAFFC2024,affective,"Opacity, Transparency, and the Ethics of Affective Computing.",3,"Ethics and Social Impacts of AI,Free Will and Agency,Neuroethics, Human Enhancement, Biomedical Innovations","Transparency (behavior),Opacity,Autonomy,Deception,Psychology,Cognitive psychology,Social psychology,Computer science,Political science,Computer security,Law,Physics,Optics",,,"Manohar Kumar,Aisha Aijaz,Omkar Chattar,Jainendra Shukla,Raghava Mutharaju",
TAFFC2023,emotion,The Acoustically Emotion-Aware Conversational Agent With Speech Emotion Recognition and Empathetic Responses.,10,"Emotion and Mood Recognition,Social Robot Interaction and HRI,AI in Service Interactions","Emotion recognition,Psychology,Natural (archaeology),Emotional intelligence,Negative emotion,Emotion classification,Interface (matter),Affective computing,Cognitive psychology,Computer science,Social psychology,Archaeology,Bubble,Maximum bubble pressure method,Parallel computing,History,Neuroscience",,,"Jiaxiong Hu,Yun Huang,Xiaozhu Hu,Yingqing Xu",
TAFFC2024,"affective,affectivity,affect",An (E)Affective Bind: Situated Affectivity and the Prospect of Affect Recognition.,1,"Embodied and Extended Cognition,Emotions and Moral Behavior,Face Recognition and Perception","Situated,Affect (linguistics),The arts,Psychology,Social psychology,Epistemology,Negative affectivity,Sociology,Ambiguity,Aesthetics,Political science,Computer science,Personality,Communication,Artificial intelligence,Philosophy,Law,Programming language",,,Jason Branford,
TAFFC2021,emotional,Induction of Emotional States in Educational Video Games Through a Fuzzy Control System.,25,"Intelligent Tutoring Systems and Adaptive Learning,Online Learning and Analytics,Reinforcement Learning in Robotics","Control (management),Fuzzy logic,Psychology,Process (computing),State (computer science),Computer science,Video game,Fuzzy control system,Multimedia,Cognitive psychology,Human–computer interaction,Artificial intelligence,Algorithm,Operating system",,,"Carlos Lara-Alvarez,Hugo A. Mitre-Hernández,Juan J. Flores,Humberto Pérez Espinosa",
TAFFC2023,"emotion,emotional",Emotion Intensity and its Control for Emotional Voice Conversion.,35,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Embedding,Utterance,Psychology,Valence (chemistry),Similarity (geometry),Style (visual arts),Speech recognition,Computer science,Cognitive psychology,Artificial intelligence,Image (mathematics),Archaeology,History,Physics,Quantum mechanics",,,"Kun Zhou,Berrak Sisman,Rajib Rana,Björn W. Schuller,Haizhou Li",
TAFFC2024,"emotional,affect",Spatio-Temporal Graph Analytics on Secondary Affect Data for Improving Trustworthy Emotional AI.,0,"Mental Health Research Topics,Digital Mental Health Interventions,Emotion and Mood Recognition","Computer science,Affect (linguistics),Transparency (behavior),Data science,Perspective (graphical),Analytics,Data mining,Graph,Data modeling,Harm,Accountability,Software deployment,Trustworthiness,Corporate governance,Artificial intelligence,Computer security,Psychology,Theoretical computer science,Social psychology,Database,Communication,Finance,Political science,Law,Economics,Operating system",,,"Md Taufeeq Uddin,Lijun Yin,Shaun J. Canavan",
TAFFC2021,affective,Leveraging Affective Hashtags for Ranking Music Recommendations.,22,"Music and Audio Processing,Neuroscience and Music Perception,Sentiment Analysis and Opinion Mining","Ranking (information retrieval),Computer science,Leverage (statistics),Context (archaeology),Affective computing,Information retrieval,Set (abstract data type),Mood,Artificial intelligence,Machine learning,Psychology,Social psychology,Paleontology,Biology,Programming language",,,"Eva Zangerle,Chih-Ming Chen,Ming-Feng Tsai,Yi-Hsuan Yang",
TAFFC2023,emotion,A Survey of Textual Emotion Recognition and Its Challenges.,78,"Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques,Text and Document Classification Technologies","Deep learning,Computer science,Artificial intelligence,Field (mathematics),Data science,Word embedding,Emotion recognition,Sentiment analysis,Natural language understanding,Quality (philosophy),Affective computing,Natural language processing,Natural language,Embedding,Epistemology,Philosophy,Mathematics,Pure mathematics",,,"Jiawen Deng,Fuji Ren",
TAFFC2024,affective,Ethical Considerations and Checklist for Affective Research With Wearables.,11,"Digital Mental Health Interventions,Emotion and Mood Recognition,Mental Health via Writing","Usability,Checklist,Wearable computer,Popularity,Psychology,Wearable technology,Research ethics,Applied psychology,Computer science,Internet privacy,Human–computer interaction,Social psychology,Cognitive psychology,Psychiatry,Embedded system",,,"Maciej Behnke,Stanislaw Saganowski,Dominika Kunc,Przemyslaw Kazienko",
TAFFC2021,emotion,Inter-Brain EEG Feature Extraction and Analysis for Continuous Implicit Emotion Tagging During Video Watching.,56,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Neural dynamics and brain function","Electroencephalography,Valence (chemistry),Feature extraction,Arousal,Emotion recognition,Computer science,Brain activity and meditation,Speech recognition,Pattern recognition (psychology),Artificial intelligence,Affective computing,Feature (linguistics),Psychology,Neuroscience,Linguistics,Philosophy,Physics,Quantum mechanics",,,"Yue Ding,Xin Hu,Zhenyi Xia,Yong-Jin Liu,Dan Zhang",
TAFFC2023,emotion,Audio Features for Music Emotion Recognition: A Survey.,93,"Music and Audio Processing,Neuroscience and Music Perception,Speech and Audio Processing","Affective computing,Key (lock),Computer science,Music and emotion,Musical,Tone (literature),Emotion recognition,Music psychology,Feature (linguistics),Rhythm,Audio signal processing,Speech recognition,Artificial intelligence,Audio signal,Aesthetics,Linguistics,Music history,Visual arts,Art,Philosophy,Speech coding,Computer security",,,"Renato Panda,Ricardo Malheiro,Rui Pedro Paiva",
TAFFC2024,affective,Measuring and Fostering Diversity in Affective Computing Research.,4,"Gender and Technology in Education,Youth Education and Societal Dynamics","Diversity (politics),Affective computing,Psychology,Cognitive psychology,Social psychology,Computer science,Human–computer interaction,Sociology,Anthropology",,,"Isabelle Hupont,Songül Tolan,Pedro Frau,Lorenzo Porcaro,Emilia Gómez",
TAFFC2023,emotion,Automatic Emotion Recognition for Groups: A Review.,48,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Anomaly Detection Techniques and Applications","Modalities,Computer science,Emotion recognition,Crowds,Robustness (evolution),Affective computing,Emotion detection,Artificial intelligence,Machine learning,Natural language processing,Data science,Social science,Biochemistry,Chemistry,Computer security,Sociology,Gene",,,"Emmeke Veltmeijer,Charlotte Gerritsen,Koen V. Hindriks",
TAFFC2022,affective,Utilizing Deep Learning Towards Multi-Modal Bio-Sensing and Vision-Based Affective Computing.,174,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Advanced Chemical Sensor Technologies","Computer science,Artificial intelligence,Deep learning,Machine learning,Transfer of learning,Affective computing,Modalities,Modal,Salient,Emotion classification,Pattern recognition (psychology),Social science,Chemistry,Sociology,Polymer chemistry",,,"Siddharth,Tzyy-Ping Jung,Terrence J. Sejnowski",
IF2023,emotion,Dynamic interactive multiview memory network for emotion recognition in conversation.,51,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Computer science,Conversation,Fuse (electrical),Crossmodal,Focus (optics),Modalities,Process (computing),Human–computer interaction,Multimodal interaction,Artificial intelligence,Speech recognition,Perception,Visual perception,Social science,Philosophy,Linguistics,Physics,Optics,Neuroscience,Sociology,Electrical engineering,Biology,Engineering,Operating system",,,"Jintao Wen,Dazhi Jiang,Geng Tu,Cheng Liu,Erik Cambria",
IF2024,emotion,Multi-view domain-adaptive representation learning for EEG-based emotion recognition.,21,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Computer science,Discriminative model,Artificial intelligence,Pattern recognition (psychology),Discriminator,Convolutional neural network,Electroencephalography,Feature (linguistics),Speech recognition,Short-time Fourier transform,Machine learning,Fourier transform,Mathematics,Psychology,Telecommunications,Fourier analysis,Linguistics,Philosophy,Mathematical analysis,Psychiatry,Detector",,,"Chao Li,Ning Bian,Ziping Zhao,Haishuai Wang,Björn W. Schuller",
IF2024,emotion,Incongruity-aware multimodal physiology signals fusion for emotion recognition.,3,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Color perception and design","Concatenation (mathematics),Modality (human–computer interaction),Computer science,Modalities,Redundancy (engineering),Artificial intelligence,Feature (linguistics),Pattern recognition (psychology),Fusion,Transformer,Speech recognition,Mathematics,Engineering,Social science,Linguistics,Philosophy,Combinatorics,Voltage,Sociology,Electrical engineering,Operating system",,,"Jing Li,Ning Chen,Hongqing Zhu,Guangqiang Li,Zhangyong Xu,Dingxin Chen",
IF2024,emotional,Identifying the hierarchical emotional areas in the human brain through information fusion.,1,"Functional Brain Connectivity Studies,Neural dynamics and brain function,Mental Health Research Topics","Pairwise comparison,Computer science,Perception,Key (lock),Strict constructionism,Emotion perception,Cognitive psychology,Process (computing),Artificial intelligence,Cognitive science,Psychology,Machine learning,Epistemology,Philosophy,Computer security,Neuroscience,Facial expression,Operating system",,,"Zhongyu Huang,Changde Du,Chaozhuo Li,Kaicheng Fu,Huiguang He",
IF2023,emotion,Finding hate speech with auxiliary emotion detection from self-training multi-label learning perspective.,19,"Hate Speech and Cyberbullying Detection,Internet Traffic Analysis and Secure E-voting,Spam and Phishing Detection","Perspective (graphical),Computer science,Benchmark (surveying),Emotion detection,Mainstream,Task (project management),Speech recognition,Voice activity detection,Artificial intelligence,Sentiment analysis,Emotion recognition,Natural language processing,Machine learning,Speech processing,Philosophy,Theology,Management,Geodesy,Economics,Geography",,,"Changrong Min,Hongfei Lin,Ximing Li,He Zhao,Junyu Lu,Liang Yang,Bo Xu",
IF2023,emotion,Emotion fusion for mental illness detection from social media: A survey.,50,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Complex Network Analysis Techniques","Popularity,Interpretability,Social media,Mental illness,Mental health,Quality (philosophy),Emotion detection,Computer science,Psychology,Data science,Applied psychology,Psychiatry,Artificial intelligence,World Wide Web,Emotion recognition,Social psychology,Philosophy,Epistemology",,,"Tianlin Zhang,Kailai Yang,Shaoxiong Ji,Sophia Ananiadou",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
INTERSPEECH2023,"emotion,emotional",Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech,1,"Speech Recognition and Synthesis,Speech and Audio Processing","Speech recognition,Emotion recognition,Computer science,Motor theory of speech perception,Speech processing,Natural language processing,Speech corpus,Artificial intelligence,Speech synthesis,Speech perception,Psychology,Perception,Neuroscience",,https://www.isca-archive.org//interspeech_2023/wang23ka_interspeech.pdf,"Shijun Wang, Jón Guðnason, Damian Borth","Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech."
INTERSPEECH2023,"emotion,emotional",ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition,5,"Speech Recognition and Synthesis,Phonetics and Phonology Research,Speech and dialogue systems","Speech recognition,Computer science,Word (group theory),Emotion recognition,Natural language processing,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2023/li23ea_interspeech.pdf,"Yuanchao Li, Zeyu Zhao, Ondřej Klejch, Peter Bell, Catherine Lai","In Speech Emotion Recognition (SER), textual data is often used alongside audio signals to address their inherent variability. However, the reliance on human annotated text in most research hinders the development of practical SER systems. To overcome this challenge, we investigate how Automatic Speech Recognition (ASR) performs on emotional speech by analyzing the ASR performance on emotion corpora and examining the distribution of word errors and confidence scores in ASR transcripts to gain insight into how emotion affects ASR. We utilize four ASR systems, namely Kaldi ASR, wav2vec2, Conformer, and Whisper, and three corpora: IEMOCAP, MOSI, and MELD to ensure generalizability. Additionally, we conduct text-based SER on ASR transcripts with increasing word error rates to investigate how ASR affects SER. The objective of this study is to uncover the relationship and mutual impact of ASR and SER, in order to facilitate ASR adaptation to emotional speech and the use of SER in real world."
TAFFC2024,affect,Facial Expression Recognition in Classrooms: Ethical Considerations and Proposed Guidelines for Affect Detection in Educational Settings.,4,"Emotion and Mood Recognition,Emotions and Moral Behavior,Face Recognition and Perception","Affect (linguistics),Reflexivity,Typology,Facial expression,Psychology,Tracking (education),Psychological intervention,Affective computing,Social psychology,Engineering ethics,Pedagogy,Sociology,Engineering,Social science,Communication,Neuroscience,Psychiatry,Anthropology",,,"Allison Macey Banzon,Jonathan Beever,Michelle Taub",
TAFFC2024,affective,ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes.,13,"Noise Effects and Management,Hearing Loss and Rehabilitation,Acoustic Wave Phenomena Research","Soundscape,Benchmark (surveying),Set (abstract data type),Perception,Baseline (sea),Computer science,Artificial intelligence,Speech recognition,Machine learning,Psychology,Geography,Sound (geography),Cartography,Acoustics,Oceanography,Physics,Neuroscience,Geology,Programming language",,,"Kenneth Ooi,Zhen-Ting Ong,Karn N. Watcharasupat,Bhan Lam,Joo Young Hong,Woon-Seng Gan",
TAFFC2024,emotion,Prompt Consistency for Multi-Label Textual Emotion Detection.,8,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Computer science,Natural language processing,Emotion detection,Consistency (knowledge bases),Redundancy (engineering),Artificial intelligence,Multi-label classification,Task (project management),Semantics (computer science),Machine learning,Emotion recognition,Management,Economics,Programming language,Operating system",,,"Yangyang Zhou,Xin Kang,Fuji Ren",
TAFFC2021,emotion,Empirical Evidence Relating EEG Signal Duration to Emotion Classification Performance.,33,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neural dynamics and brain function","Electroencephalography,Stimulus (psychology),Computer science,Recall,Speech recognition,Artificial intelligence,Classifier (UML),Emotion recognition,Pattern recognition (psychology),Emotion classification,Psychology,Cognitive psychology,Neuroscience",,,"Eanes Torres Pereira,Herman Martins Gomes,Luciana Ribeiro Veloso,Moisés Roberto A. Mota",
TAFFC2023,emotional,The Analysis of Driver's Behavioral Tendency Under Different Emotional States Based on a Bayesian Network.,877,"Traffic and Road Safety,Injury Epidemiology and Prevention,Occupational Health and Safety Research","Crash,Poison control,Human factors and ergonomics,Motor vehicle crash,Injury prevention,Transport engineering,Occupational safety and health,Engineering,Range (aeronautics),Risk analysis (engineering),Computer science,Forensic engineering,Medicine,Environmental health,Pathology,Programming language,Aerospace engineering",,,"Yaqi Liu,Xiao-Yuan Wang",
TAFFC2024,emotion,GA2MIF: Graph and Attention Based Two-Stage Multi-Source Information Fusion for Conversational Emotion Detection.,26,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Computer science,Emotion detection,Information fusion,Emotion recognition,Fusion,Sensor fusion,Affective computing,Psychology,Artificial intelligence,Graph,Speech recognition,Cognitive psychology,Natural language processing,Theoretical computer science,Linguistics,Philosophy",,,"Jiang Li,Xiaoping Wang,Guoqing Lv,Zhigang Zeng",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
MM2022,emotions,A Multi-view Spectral-Spatial-Temporal Masked Autoencoder for Decoding Emotions with Self-supervised Learning.,27,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neural dynamics and brain function","Computer science,Autoencoder,Electroencephalography,Artificial intelligence,Decoding methods,Pattern recognition (psychology),Speech recognition,Data modeling,Deep learning,Machine learning,Psychology,Telecommunications,Database,Psychiatry",,,"Rui Li,Yiting Wang,Wei-Long Zheng,Bao-Liang Lu",
MM2022,affective,"MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild.",23,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and Audio Processing","Sadness,Disgust,Computer science,Facial expression,Modality (human–computer interaction),Emotion classification,Surprise,Affective computing,Annotation,Anger,Happiness,Artificial intelligence,Natural language processing,Speech recognition,Psychology,Social psychology,Psychiatry",,,"Yuanyuan Liu,Wei Dai,Chuanxu Feng,Wenbin Wang,Guanghao Yin,Jiabei Zeng,Shiguang Shan",
MM2022,emotion,SER30K: A Large-Scale Dataset for Sticker Emotion Recognition.,5,"Visual Attention and Saliency Detection,Image Retrieval and Classification Techniques,Advanced Image and Video Retrieval Techniques","Computer science,Convolutional neural network,Artificial intelligence,Popularity,Focus (optics),Facial expression,Emotion recognition,Emotion classification,Theme (computing),World Wide Web,Psychology,Social psychology,Physics,Optics",,,"Shengzhe Liu,Xin Zhang,Jufeng Yang",
MM2022,affective,Representation Learning through Multimodal Attention and Time-Sync Comments for Affective Video Content Analysis.,10,"Emotion and Mood Recognition,Music and Audio Processing,Video Analysis and Summarization","Computer science,Artificial intelligence,Representation (politics),Leverage (statistics),Modalities,Benchmark (surveying),Feature learning,Embedding,Pattern recognition (psychology),Machine learning,Politics,Political science,Law,Social science,Geodesy,Sociology,Geography",,,"Jicai Pan,Shangfei Wang,Lin Fang",
MM2022,emotion,Towards Unbiased Visual Emotion Recognition via Causal Intervention.,11,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Face and Expression Recognition","Spurious relationship,Computer science,Artificial intelligence,Machine learning,Inference,Robustness (evolution),Causal inference,Pattern recognition (psychology),Mathematics,Econometrics,Biochemistry,Chemistry,Gene",,,"Yuedong Chen,Xu Yang,Tat-Jen Cham,Jianfei Cai",
MM2022,affect,Learning from Label Relationships in Human Affect.,4,"Emotion and Mood Recognition,Mental Health Research Topics,Machine Learning in Healthcare","Computer science,Artificial intelligence,Machine learning,Margin (machine learning),Overfitting,Generalizability theory,Context (archaeology),Regression,Feature (linguistics),Affect (linguistics),Feature learning,Deep learning,Artificial neural network,Statistics,Mathematics,Paleontology,Linguistics,Philosophy,Biology",,,"Niki Maria Foteinopoulou,Ioannis Patras",
MM2022,emotion,Unsupervised Domain Adaptation Integrating Transformer and Mutual Information for Cross-Corpus Speech Emotion Recognition.,10,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Computer science,Artificial intelligence,Utterance,Mutual information,Transformer,Speech recognition,Pattern recognition (psychology),Encoder,Unsupervised learning,Domain adaptation,Natural language processing,Physics,Quantum mechanics,Voltage,Operating system,Classifier (UML)",,,"Shiqing Zhang,Ruixin Liu,Yijiao Yang,Xiaoming Zhao,Jun Yu",
TAFFC2021,emotion,Feature Pooling of Modulation Spectrum Features for Improved Speech Emotion Recognition in the Wild.,53,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Reverberation,Pooling,Computer science,Speech recognition,Benchmark (surveying),Affective computing,Noise (video),Feature (linguistics),Valence (chemistry),Background noise,Artificial intelligence,Feature extraction,Engineering,Telecommunications,Linguistics,Philosophy,Physics,Geodesy,Quantum mechanics,Geography,Electrical engineering,Image (mathematics)",,,"Anderson R. Avila,Zahid Akhtar,João Felipe Santos,Douglas D. O'Shaughnessy,Tiago H. Falk",
TAFFC2022,emotions,On Emotions as Features for Speech Overlaps Classification.,3,"Speech and dialogue systems,Emotion and Mood Recognition,Speech Recognition and Synthesis","Valence (chemistry),Computer science,Speech recognition,Focus (optics),Emotional valence,Artificial intelligence,Natural language processing,Psychology,Cognition,Physics,Quantum mechanics,Neuroscience,Optics",,,"Olga Egorow,Andreas Wendemuth",
TAFFC2024,emotion,Transformer-Based Self-Supervised Multimodal Representation Learning for Wearable Emotion Recognition.,23,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Computer science,Artificial intelligence,Overfitting,Pattern recognition (psychology),Wearable computer,Machine learning,Encoder,Feature extraction,Autoencoder,Speech recognition,Deep learning,Artificial neural network,Embedded system,Operating system",,,"Yujin Wu,Mohamed Daoudi,Ali Amad",
TAFFC2021,emotions,Induction and Profiling of Strong Multi-Componential Emotions in Virtual Reality.,72,"Emotion and Mood Recognition,Neural dynamics and brain function,Cognitive Science and Education Research","Profiling (computer programming),Virtual reality,Psychology,Human–computer interaction,Cognitive psychology,Computer science,Operating system",,,"Ben Meuleman,David Rudrauf",
TAFFC2023,emotional,Detecting Mental Disorders in Social Media Through Emotional Patterns - The Case of Anorexia and Depression.,33,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Interpretability,Anorexia,Psychology,Depression (economics),Social media,Eating disorders,Cognitive psychology,Clinical psychology,Computer science,Artificial intelligence,Medicine,World Wide Web,Internal medicine,Economics,Macroeconomics",,,"Mario Ezra Aragón,Adrián Pastor López-Monroy,Luis Carlos González-Gurrola,Manuel Montes-y-Gómez",
TAFFC2022,emotional,Emotional Conversation Generation Orientated Syntactically Constrained Bidirectional-Asynchronous Framework.,7,"Topic Modeling,Natural Language Processing Techniques,Sentiment Analysis and Opinion Mining","Computer science,Asynchronous communication,Conversation,Fluency,Artificial intelligence,Decoding methods,Constraint (computer-aided design),Natural language processing,Speech recognition,Linguistics,Psychology,Algorithm,Communication,Mechanical engineering,Computer network,Philosophy,Engineering",,,"Xiao Sun,Jingyuan Li,Jianhua Tao",
TAFFC2022,emotion,Emotion Recognition and EEG Analysis Using ADMM-Based Sparse Group Lasso.,10,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Interpretability,Pattern recognition (psychology),Artificial intelligence,Computer science,Feature (linguistics),Electroencephalography,Feature selection,Feature extraction,Lasso (programming language),Sparse approximation,Machine learning,Psychology,Philosophy,Linguistics,Psychiatry,World Wide Web",,,"Kin Ming Puk,Shouyi Wang,Jay M. Rosenberger,Kellen C. Gandy,Haley Nicole Harris,Yuan Bo Peng,Anne Nordberg,Peter Lehmann,Jodi Tommerdahl,Jung-Chih Chiao",
INTERSPEECH2023,emotion,Leveraging Label Information for Multimodal Emotion Recognition,2,Emotion and Mood Recognition,"Computer science,Emotion recognition,Artificial intelligence,Human–computer interaction,Pattern recognition (psychology),Speech recognition",,https://www.isca-archive.org//interspeech_2023/wang23ma_interspeech.pdf,"Peiying Wang, Sunlu Zeng, Junqing Chen, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He","Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance."
INTERSPEECH2022,emotional,Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training,1,Speech Recognition and Synthesis,"Computer science,Training (meteorology),Quality (philosophy),Speech recognition,Natural language processing,Artificial intelligence,Philosophy,Physics,Epistemology,Meteorology",,https://www.isca-archive.org//interspeech_2022/he22d_interspeech.pdf,"Jiaxu He, Cheng Gong, Longbiao Wang, Di Jin, Xiaobao Wang, Junhai Xu, Jianwu Dang","Due to the lack of high-quality emotional speech synthesis datasets, the naturalness and expressiveness of synthesized speech are still lacking in order to achieve human-like communication. And existing emotional speech synthesis system usually extracts emotional information only from reference audio and ignores sentiment information implicit in the text. Therefore, we propose a novel model to improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised learning. In addition to explicit emotional representations from reference audio, we propose an implicit emotion representations learning method based on graph neural network, considering dependency relations of a sentence and text sentiment classification (TSC) task. For the lack of emotion-annotated datasets, we leverage large amounts of expressive datasets to reinforce training the proposed model with semi-supervised learning. Experiments show that the proposed method can improve the naturalness and expressiveness of synthetic speech and is better than the baseline model."
TAFFC2024,emotion,Cross-Day Data Diversity Improves Inter-Individual Emotion Commonality of Spatio-Spectral EEG Signatures Using Independent Component Analysis.,5,"EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies,Emotion and Mood Recognition","Generalizability theory,Electroencephalography,Psychology,Arousal,Representation (politics),Computer science,Cognitive psychology,Artificial intelligence,Pattern recognition (psychology),Developmental psychology,Social psychology,Neuroscience,Politics,Political science,Law",,,"Yi-Wei Shen,Yuan-Pin Lin",
TAFFC2023,affective,Silicon Coppélia and the Formalization of the Affective Process.,6,"Social Robot Interaction and HRI,Reinforcement Learning in Robotics,Emotion and Mood Recognition","COPP,Computer science,Action (physics),Process (computing),Feeling,Human–computer interaction,Scrutiny,Artificial intelligence,Psychology,Social psychology,Political science,Heme,Biochemistry,Chemistry,Physics,Heme oxygenase,Quantum mechanics,Enzyme,Operating system,Law",,,"Johan F. Hoorn,Thomas Baier,Jeroen van Maanen,Jeroen Wester",
TAFFC2022,affective,Affective Audio Annotation of Public Speeches with Convolutional Clustering Neural Network.,3,"Music and Audio Processing,Speech and Audio Processing,Video Analysis and Summarization","Computer science,Annotation,Convolutional neural network,Pooling,Artificial intelligence,Cluster analysis,Speech recognition,Machine learning,Deep learning",,,"Jiahao Xu,Boyan Zhang,Zhiyong Wang,Yang Wang,Fang Chen,Junbin Gao,David Dagan Feng",
TAFFC2022,affective,Participatory Design of Affective Technology: Interfacing Biomusic and Autism.,25,"Innovative Human-Technology Interaction,Assistive Technology in Communication and Mobility,Tactile and Sensory Interactions","Participatory design,Stakeholder,Knowledge management,Product design,Product (mathematics),Design technology,Computer science,Human–computer interaction,Psychology,Engineering ethics,Engineering,Systems engineering,Public relations,Political science,Mechanical engineering,Parallels,Geometry,Mathematics",,,"Florian Grond,Rossio Motta-Ochoa,Natalie Miyake,Tamar Tembeck,Melissa Park,Stefanie Blain-Moraes",
TAFFC2024,affective,The Role of Preprocessing for Word Representation Learning in Affective Tasks.,7,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Preprocessor,Computer science,Sentiment analysis,Pipeline (software),Artificial intelligence,Lemmatisation,Natural language processing,Affect (linguistics),Representation (politics),Punctuation,Sarcasm,Data pre-processing,Emotion detection,Mood,Machine learning,Psychology,Emotion recognition,Linguistics,Communication,Irony,Psychiatry,Programming language,Philosophy,Politics,Political science,Law",,,"Nastaran Babanejad,Heidar Davoudi,Ameeta Agrawal,Aijun An,Manos Papagelis",
TAFFC2022,affective,Affective Impression: Sentiment-Awareness POI Suggestion via Embedding in Heterogeneous LBSNs.,20,"Recommender Systems and Techniques,Complex Network Analysis Techniques,Human Mobility and Location-Based Analysis","Computer science,Similarity (geometry),Sentiment analysis,Information retrieval,Artificial intelligence,Image (mathematics)",,,"Xi Xiong,Shaojie Qiao,Nan Han,Yuanyuan Li,Fei Xiong,Ling He",
TAFFC2024,emotion,Empirical Validation of an Agent-Based Model of Emotion Contagion.,3,"Opinion Dynamics and Social Influence,Psychology of Social Influence,Emotions and Moral Behavior","Emotional contagion,Psychology,Lagging,Empirical research,Contagion effect,Mechanism (biology),Cognitive psychology,Empirical evidence,Emotion classification,Social psychology,Economics,Medicine,Philosophy,Financial crisis,Epistemology,Pathology,Macroeconomics",,,"Erik van Haeringen,Emmeke Veltmeijer,Charlotte Gerritsen",
TAFFC2021,emotion,Introduction to the Special Issue On Computational Modelling of Emotion.,3,"Sentiment Analysis and Opinion Mining,Cognitive Science and Education Research","Affective computing,Field (mathematics),Function (biology),Focus (optics),Affective science,Computational model,Computer science,Cognitive science,Cognition,Discipline,Emotion recognition,Cognitive psychology,Psychology,Emotion classification,Human–computer interaction,Artificial intelligence,Sociology,Social science,Physics,Mathematics,Evolutionary biology,Neuroscience,Pure mathematics,Optics,Biology",,,"Celso de Melo,Dean Petters,Joel Parthemore,David C. Moffatt,Christian Becker-Asano",
TAFFC2022,emotion,"All-in-One: Emotion, Sentiment and Intensity Prediction Using a Multi-Task Ensemble Framework.",99,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Artificial intelligence,Computer science,Leverage (statistics),Sentiment analysis,Natural language processing",,,"Md. Shad Akhtar,Deepanway Ghosal,Asif Ekbal,Pushpak Bhattacharyya,Sadao Kurohashi",
TAFFC2021,emotion,Towards a Prediction and Data Driven Computational Process Model of Emotion.,31,"Mental Health Research Topics,Emotion and Mood Recognition,Emotions and Moral Behavior","Process (computing),Computer science,Computational model,Variety (cybernetics),Component (thermodynamics),Focus (optics),Basis (linear algebra),Artificial intelligence,Machine learning,Physics,Geometry,Mathematics,Optics,Thermodynamics,Operating system",,,Klaus R. Scherer,
TAFFC2023,"affective,emotional",Touching Virtual Humans: Haptic Responses Reveal the Emotional Impact of Affective Agents.,19,"Action Observation and Synchronization,Virtual Reality Applications and Impacts,Face Recognition and Perception","Haptic technology,Virtual reality,Psychology,Emotional expression,Perception,Immersion (mathematics),Arousal,Affect (linguistics),Valence (chemistry),Interpersonal communication,Virtual agent,Cognitive psychology,Social psychology,Human–computer interaction,Computer science,Communication,Simulation,Neuroscience,Physics,Mathematics,Quantum mechanics,Pure mathematics",,,"Imtiaj Ahmed,Ville J. Harjunen,Giulio Jacucci,Niklas Ravaja,Tuukka Ruotsalo,Michiel M. A. Spapé",
TAFFC2022,emotion,"PersEmoN: A Deep Network for Joint Analysis of Apparent Personality, Emotion and Their Relationship.",27,"Emotion and Mood Recognition,Personality Traits and Psychology,Mental Health Research Topics","Joint (building),Personality,Psychology,Cognitive psychology,Artificial intelligence,Computer science,Social psychology,Engineering,Architectural engineering",,,"Le Zhang,Songyou Peng,Stefan Winkler",
TAFFC2024,affective,Crowdsourcing Affective Annotations Via fNIRS-BCI.,5,"EEG and Brain-Computer Interfaces,Neural dynamics and brain function,Neural and Behavioral Psychology Studies","Brain–computer interface,Crowdsourcing,Annotation,Affective computing,Computer science,Interface (matter),Process (computing),Affect (linguistics),Psychology,Human–computer interaction,Cognitive psychology,Electroencephalography,Artificial intelligence,Communication,World Wide Web,Neuroscience,Bubble,Maximum bubble pressure method,Parallel computing,Operating system",,,"Tuukka Ruotsalo,Kalle Mäkelä,Michiel M. A. Spapé",
TAFFC2021,emotional,An Architecture for Emotional Facial Expressions as Social Signals.,4,"Emotion and Mood Recognition,Social Robot Interaction and HRI,Emotions and Moral Behavior","Architecture,Facial expression,Cognitive architecture,Focus (optics),Computer science,Cognitive science,Cognition,State (computer science),Human–computer interaction,Artificial intelligence,Psychology,Cognitive psychology,Programming language,Art,Physics,Neuroscience,Optics,Visual arts",,,"Ruth Aylett,Christopher Ritter,Mei Yii Lim,Frank Broz,Peter E. McKenna,Ingo Keller,Gnanathusharan Rajendran",
TAFFC2023,emotion,Variational Instance-Adaptive Graph for EEG Emotion Recognition.,63,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Electroencephalography,Computer science,Discriminative model,Pattern recognition (psychology),Artificial intelligence,Graph,Probabilistic logic,Constraint (computer-aided design),Inference,Theoretical computer science,Mathematics,Psychology,Geometry,Psychiatry",,,"Tengfei Song,Suyuan Liu,Wenming Zheng,Yuan Zong,Zhen Cui,Yang Li,Xiaoyan Zhou",
TAFFC2021,affective,Applying Probabilistic Programming to Affective Computing.,19,"Emotions and Moral Behavior,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Probabilistic logic,Artificial intelligence,Executable,Inductive programming,Programming paradigm,Context (archaeology),Field (mathematics),Inference,Machine learning,Theoretical computer science,Programming language,Paleontology,Mathematics,Pure mathematics,Biology",,,"Desmond C. Ong,Harold Soh,Jamil Zaki,Noah D. Goodman",
TAFFC2023,"emotion,emotional",Emotional Attention Detection and Correlation Exploration for Image Emotion Distribution Learning.,13,"Image Retrieval and Classification Techniques,Multimodal Machine Learning Applications,Generative Adversarial Networks and Image Synthesis","Correlation,Perspective (graphical),Computer science,Artificial intelligence,Image (mathematics),Cognitive psychology,Emotion classification,Class (philosophy),Graph,Psychology,Mathematics,Theoretical computer science,Geometry",,,"Zhiwei Xu,Shangfei Wang",
TAFFC2024,emotion,"A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm, Sentiment and Emotion Analysis.",5,"Advanced Text Analysis Techniques,Sentiment Analysis and Opinion Mining,Spam and Phishing Detection","Sarcasm,Modal,Sentiment analysis,Joint (building),Computer science,Psychology,Cognitive psychology,Artificial intelligence,Engineering,Linguistics,Irony,Philosophy,Architectural engineering,Chemistry,Polymer chemistry",,,"Yaochen Liu,Yazhou Zhang,Dawei Song",
TAFFC2021,affect,A Hybrid Cognitive Architecture with Primal Affect and Physiology.,8,"Mental Health Research Topics,Neuroendocrine regulation and behavior,Stress Responses and Cortisol","Affect (linguistics),Cognition,Cognitive architecture,Construct (python library),Cognitive science,Cognitive psychology,Architecture,Psychology,Process (computing),Computer science,Affective computing,Cognitive model,Human–computer interaction,Neuroscience,Communication,Art,Visual arts,Programming language,Operating system",,,Christopher L. Dancy,
TAFFC2023,emotion,Multi-Target Positive Emotion Recognition From EEG Signals.,15,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Heart Rate Variability and Autonomic Control","Electroencephalography,Correlation,Happiness,Artificial intelligence,Pattern recognition (psychology),Psychology,Amusement,Brain activity and meditation,Speech recognition,Linear regression,Pearson product-moment correlation coefficient,Ranking (information retrieval),Mathematics,Computer science,Statistics,Social psychology,Psychiatry,Geometry",,,"Guozhen Zhao,Yulin Zhang,Guanhua Zhang,Dan Zhang,Yong-Jin Liu",
TAFFC2023,emotion,EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention.,302,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Discriminative model,Electroencephalography,Computer science,Artificial intelligence,Convolutional neural network,Pattern recognition (psychology),Brain–computer interface,Speech recognition,Emotion recognition,Channel (broadcasting),Task (project management),Affective computing,Machine learning,Psychology,Neuroscience,Engineering,Computer network,Systems engineering",,,"Wei Tao,Chang Li,Rencheng Song,Juan Cheng,Yu Liu,Feng Wan,Xun Chen",
IF2023,emotion,"A Multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations.",54,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Sarcasm,Computer science,Conversation,Artificial intelligence,Multi-task learning,Leverage (statistics),Sentiment analysis,Task (project management),Machine learning,Human–computer interaction,Natural language processing,Cognitive psychology,Psychology,Communication,Art,Literature,Irony,Management,Economics",,,"Yazhou Zhang,Jinglin Wang,Yaochen Liu,Lu Rong,Qian Zheng,Dawei Song,Prayag Tiwari,Jing Qin",
IF2024,emotion,"Editor's note: ""Emotion recognition from unimodal to multimodal analysis: A review"" [Information Fusion, 99 (2023) 101847 https://doi.org/10.1016/j.inffus.2023.101847].",-1,,,,,,
IF2024,emotion,Bi-stream graph learning based multimodal fusion for emotion recognition in conversation.,2,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Conversation,Computer science,Modal,Graph,Artificial intelligence,Modalities,Machine learning,Context (archaeology),Natural language processing,Theoretical computer science,Communication,Psychology,Paleontology,Social science,Chemistry,Sociology,Polymer chemistry,Biology",,,"Nannan Lu,Zhiyuan Han,Min Han,Jiansheng Qian",
IF2024,"emotion,emotional",Learning emotional prompt features with multiple views for visual emotion analysis.,1,"Human Pose and Action Recognition,Anomaly Detection Techniques and Applications,Video Analysis and Summarization","Computer science,Cognitive psychology,Psychology",,,"Qinfu Xu,Yiwei Wei,Shaozu Yuan,Jie Wu,Leiquan Wang,Chunlei Wu",
IF2024,emotion,GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition.,11,"Emotion and Mood Recognition,Human Pose and Action Recognition,Multimodal Machine Learning Applications","Benchmark (surveying),Computer science,Emotion recognition,Emotion detection,Facial expression,Exploit,Emotion classification,Scope (computer science),Code (set theory),Facial recognition system,Artificial intelligence,Speech recognition,Machine learning,Pattern recognition (psychology),Geodesy,Computer security,Set (abstract data type),Programming language,Geography",,,"Zheng Lian,Licai Sun,Haiyang Sun,Kang Chen,Zhuofan Wen,Hao Gu,Bin Liu,Jianhua Tao",
IF2024,emotion,HiCMAE: Hierarchical Contrastive Masked Autoencoder for self-supervised Audio-Visual Emotion Recognition.,4,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Autoencoder,Audio visual,Computer science,Speech recognition,Artificial intelligence,Psychology,Pattern recognition (psychology),Natural language processing,Deep learning,Multimedia",,,"Licai Sun,Zheng Lian,Bin Liu,Jianhua Tao",
IF2024,emotion,Automatic movie genre classification & emotion recognition via a BiProjection Multimodal Transformer.,0,"Human Pose and Action Recognition,Video Analysis and Summarization,Face recognition and analysis","Transformer,Computer science,Speech recognition,Emotion recognition,Artificial intelligence,Natural language processing,Electrical engineering,Engineering,Voltage",,,"Diego Aarón Moreno-Galván,Roberto López-Santillán,Luis Carlos González-Gurrola,Manuel Montes-y-Gómez,Fernando Sánchez-Vega,Adrián Pastor López-Monroy",
IF2024,emotion,A review on semi-supervised learning for EEG-based emotion recognition.,9,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Popularity,Computer science,Context (archaeology),Electroencephalography,Artificial intelligence,Field (mathematics),Deep learning,Machine learning,Psychology,Social psychology,Paleontology,Mathematics,Psychiatry,Pure mathematics,Biology",,,"Sen Qiu,Yongtao Chen,Yulin Yang,Pengfei Wang,Zhelong Wang,Hongyu Zhao,Yuntong Kang,Ruicheng Nie",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ECCV2022,emotion,Emotion-Aware Multi-View Contrastive Learning for Facial Emotion Recognition,9,"Face recognition and analysis,Emotion and Mood Recognition,Face and Expression Recognition","Facial expression,Computer science,Valence (chemistry),Arousal,Representation (politics),Artificial intelligence,Emotional expression,Feature (linguistics),Expression (computer science),Feature learning,Pattern recognition (psychology),Speech recognition,Cognitive psychology,Psychology,Linguistics,Philosophy,Physics,Quantum mechanics,Neuroscience,Politics,Political science,Law,Programming language",,https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730181.pdf,"Daeha Kim, Byung Cheol Song","""When a person recognizes another’s emotion, he or she recognizes the (facial) features associated with emotional expression. So, for a machine to recognize facial emotion(s), the features related to emotional expression must be represented and described properly. However, prior arts based on label supervision not only failed to explicitly capture features related to emotional expression, but also were not interested in learning emotional representations. This paper proposes a novel approach to generate features related to emotional expression through feature transformation and to use them for emotional representation learning. Specifically, the contrast between the generated features and overall facial features is quantified through contrastive representation learning, and then facial emotions are recognized based on understanding of angle and intensity that describe the emotional representation in the polar coordinate, i.e., the Arousal-Valence space. Experimental results show that the proposed method improves the PCC/CCC performance by more than 10% compared to the runner-up method in the wild datasets and is also qualitatively better in terms of neural activation map."""
INTERSPEECH2022,emotion,Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis,6,"Speech Recognition and Synthesis,Speech and Audio Processing","Prosody,End-to-end principle,Speech recognition,Computer science,Speech synthesis,Compensation (psychology),Transfer (computing),Artificial intelligence,Psychology,Parallel computing,Psychoanalysis",,https://www.isca-archive.org//interspeech_2022/li22h_interspeech.pdf,"Tao Li, Xinsheng Wang, Qicong Xie, Zhichao Wang, Mingqi Jiang, Lei Xie","Cross-speaker emotion transfer speech synthesis aims to synthesize emotional speech for a target speaker by transferring the emotion from reference speech recorded by another (source) speaker. In this task, extracting speaker-independent emotion embedding from reference speech plays an important role. However, the emotional information conveyed by such emotion embedding tends to be weakened in the process to squeeze out the source speaker's timbre information. In response to this problem, a prosody compensation module (PCM) is proposed in this paper to compensate for the emotional information loss. Specifically, the PCM tries to obtain speaker-independent emotional information from the intermediate feature of a pre-trained ASR model. To this end, a prosody compensation encoder with global context (GC) blocks is introduced to obtain global emotional information from the ASR model's intermediate feature. Experiments demonstrate that the proposed PCM can effectively compensate the emotion embedding for the emotional information loss, and meanwhile maintain the timbre of the target speaker. Comparisons with state-of-the-art models show that our proposed method presents obvious superiority on the cross-speaker emotion transfer task."
INTERSPEECH2022,emotion,Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning,7,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Artificial intelligence,Deep learning,Natural language processing,Speech recognition,Emotion recognition",https://github.com/ttslr/StrengthNet.,https://www.isca-archive.org//interspeech_2022/liu22i_interspeech.pdf,"Rui Liu, Berrak Sisman, Björn Schuller, Guanglai Gao, Haizhou Li","Emotion classification of speech and assessment of the emotion strength are required in applications such as emotional text-to-speech and voice conversion. The emotion attribute ranking function based on Support Vector Machine (SVM) was proposed to predict emotion strength for emotional speech corpus. However, the trained ranking function doesn't generalize to new domains, which limits the scope of applications, especially for out-of-domain or unseen speech. In this paper, we propose a data-driven deep learning model, i.e. StrengthNet, to improve the generalization of emotion strength assessment for seen and unseen speech. This is achieved by the fusion of emotional data from various domains. We follow a multi-task learning network architecture that includes an acoustic encoder, a strength predictor, and an auxiliary emotion predictor. Experiments show that the predicted emotion strength of the proposed StrengthNet is highly correlated with ground truth scores for both seen and unseen speech. We release the source codes at: https://github.com/ttslr/StrengthNet."
TAFFC2022,affect,Affect in Multimedia: Benchmarking Violent Scenes Detection.,42,"Human Pose and Action Recognition,Anomaly Detection Techniques and Applications,Generative Adversarial Networks and Image Synthesis","Benchmarking,Affect (linguistics),Multimedia,Computer science,Emotion detection,Affective computing,Artificial intelligence,Computer vision,Psychology,Emotion recognition,Communication,Marketing,Business",,,"Mihai Gabriel Constantin,Liviu-Daniel Stefan,Bogdan Ionescu,Claire-Hélène Demarty,Mats Sjöberg,Markus Schedl,Guillaume Gravier",
TAFFC2021,emotion,Embodied Robot Models for Interdisciplinary Emotion Research.,17,"Emotions and Moral Behavior,Face Recognition and Perception,Psychology of Moral and Emotional Judgment","Embodied cognition,Perspective (graphical),Robot,Focus (optics),Psychology,Cognitive science,Computer science,Cognitive psychology,Human–computer interaction,Artificial intelligence,Physics,Optics",,,Lola Cañamero,
TAFFC2021,emotional,Towards Transparent Robot Learning Through TDRL-Based Emotional Expressions.,29,"Reinforcement Learning in Robotics,Social Robot Interaction and HRI","Robot,Robot learning,Computer science,Human–computer interaction,Transparency (behavior),Process (computing),Reinforcement learning,Expression (computer science),Artificial intelligence,Mobile robot,Computer security,Programming language,Operating system",,,"Joost Broekens,Mohamed Chetouani",
TAFFC2023,emotion,Does Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?,15,"Speech and Audio Processing,Music and Audio Processing,Hearing Loss and Rehabilitation","Computer science,Overfitting,Speech recognition,Modalities,Artificial intelligence,Feature (linguistics),Feature learning,Audio visual,Task (project management),Machine learning,Artificial neural network,Multimedia,Social science,Linguistics,Philosophy,Management,Sociology,Economics",,,"Abhinav Shukla,Stavros Petridis,Maja Pantic",
TAFFC2022,emotion,GCB-Net: Graph Convolutional Broad Network and Its Application in Emotion Recognition.,279,"Machine Learning and ELM,EEG and Brain-Computer Interfaces,Advanced Computing and Algorithms","Concatenation (mathematics),Computer science,Graph,Convolutional neural network,Pattern recognition (psychology),Artificial intelligence,Affective computing,Emotion classification,Sentiment analysis,Machine learning,Theoretical computer science,Mathematics,Combinatorics",,,"Tong Zhang,Xuehan Wang,Xiangmin Xu,C. L. Philip Chen",
TAFFC2024,affect,From the Lab to the Wild: Affect Modeling Via Privileged Information.,5,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Mental Health Research Topics","Affect (linguistics),Modalities,Computer science,Intrusiveness,Artificial intelligence,Human–computer interaction,Affective computing,Machine learning,Psychology,Social psychology,Communication,Social science,Sociology",,,"Konstantinos Makantasis,Kosmas Pinitas,Antonios Liapis,Georgios N. Yannakakis",
TAFFC2021,emotions,Deep Learning for Spatio-Temporal Modeling of Dynamic Spontaneous Emotions.,42,"Face recognition and analysis,Face and Expression Recognition,Emotion and Mood Recognition","Computer science,Artificial intelligence,Pooling,Convolutional neural network,Face (sociological concept),Context (archaeology),Facial expression,Pattern recognition (psychology),Pyramid (geometry),Expression (computer science),Deep learning,Spatial contextual awareness,Computer vision,Geography,Social science,Archaeology,Sociology,Programming language,Physics,Optics",,,"Dawood Al Chanti,Alice Caplier",
TAFFC2023,emotion,EEG Feature Selection via Global Redundancy Minimization for Emotion Recognition.,23,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Electroencephalography,Feature selection,Discriminative model,Pattern recognition (psychology),Artificial intelligence,Redundancy (engineering),Computer science,Feature (linguistics),Emotion recognition,Feature extraction,Speech recognition,Machine learning,Psychology,Linguistics,Philosophy,Psychiatry,Operating system",,,"Xueyuan Xu,Tianyuan Jia,Qing Li,Fulin Wei,Long Ye,Xia Wu",
TAFFC2022,emotion,Multiple Instance Learning for Emotion Recognition Using Physiological Signals.,40,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Neural dynamics and brain function","Ambiguity,Computer science,Artificial intelligence,Machine learning,Reliability (semiconductor),Power (physics),Physics,Quantum mechanics,Programming language",,,"Luca Romeo,Andrea Cavallo,Lucia Pepa,Nadia Bianchi-Berthouze,Massimiliano Pontil",
TAFFC2021,emotions,Automatic Recognition of Facial Displays of Unfelt Emotions.,40,"Face recognition and analysis,Emotion and Mood Recognition,Face and Expression Recognition","Facial expression,Disgust,Contempt,Discriminative model,Artificial intelligence,Computer science,Face (sociological concept),Face hallucination,Psychology,Emotion recognition,Speech recognition,Pattern recognition (psychology),Facial recognition system,Face detection,Social psychology,Anger,Social science,Sociology",,,"Kaustubh Kulkarni,Ciprian Adrian Corneanu,Ikechukwu Ofodile,Sergio Escalera,Xavier Baró,Sylwia Julia Hyniewska,Jüri Allik,Gholamreza Anbarjafari",
TAFFC2022,emotional,MES-P: An Emotional Tonal Speech Dataset in Mandarin with Distal and Proximal Labels.,11,"Emotion and Mood Recognition,Speech and Audio Processing,Infant Health and Development","Sadness,Mandarin Chinese,Psychology,Emotional intelligence,Emotional expression,Anger,Valence (chemistry),Speech recognition,Facial expression,Emotion classification,Contrast (vision),Cognitive psychology,Computer science,Communication,Social psychology,Artificial intelligence,Linguistics,Philosophy,Physics,Quantum mechanics",,,"Zhongzhe Xiao,Ying Chen,Weibei Dou,Zhi Tao,Liming Chen",
TAFFC2024,emotion,Text-Based Fine-Grained Emotion Prediction.,5,"Sentiment Analysis and Opinion Mining,Topic Modeling,Mental Health via Writing","Leverage (statistics),Computer science,Artificial intelligence,Transfer of learning,Task (project management),Emotion classification,Class (philosophy),Natural language processing,Generalization,Multi-task learning,Machine learning,Benchmark (surveying),Field (mathematics),Mathematics,Mathematical analysis,Management,Geodesy,Pure mathematics,Economics,Geography",,,"Gargi Singh,Dhanajit Brahma,Piyush Rai,Ashutosh Modi",
TAFFC2021,affect,On the Influence of Affect in EEG-Based Subject Identification.,37,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,User Authentication and Security Systems","Identification (biology),Electroencephalography,Biometrics,Context (archaeology),Affect (linguistics),Computer science,Authentication (law),Affective computing,Component (thermodynamics),SIGNAL (programming language),Artificial intelligence,Speech recognition,Pattern recognition (psychology),Brain activity and meditation,Machine learning,Data mining,Psychology,Computer security,Communication,Paleontology,Botany,Physics,Psychiatry,Biology,Programming language,Thermodynamics",,,"Pablo Arnau-González,Miguel Arevalillo-Herráez,Stamos Katsigiannis,Naeem Ramzan",
TAFFC2022,affect,Leveraging the Deep Learning Paradigm for Continuous Affect Estimation from Facial Expressions.,8,"Emotion and Mood Recognition,Face and Expression Recognition,Face recognition and analysis","Computer science,Facial expression,Artificial intelligence,Convolutional neural network,Affective computing,Kalman filter,Deep learning,Machine learning,Pattern recognition (psychology),Speech recognition",,,"Meshia Cédric Oveneke,Yong Zhao,Ercheng Pei,Abel Díaz Berenguer,Dongmei Jiang,Hichem Sahli",
TAFFC2024,emotion,"The Deep Method: Towards Computational Modeling of the Social Emotion Shame Driven by Theory, Introspection, and Social Signals.",2,Opinion Dynamics and Social Influence,"Shame,Introspection,Psychology,Cognitive psychology,Cognitive science,Artificial intelligence,Social psychology,Computer science",,,"Tanja Schneeberger,Mirella Hladký,Ann-Kristin Thurner,Jana Volkert,Alexander Heimerl,Tobias Baur,Elisabeth André,Patrick Gebhard",
TAFFC2021,emotionally,Predicting Emotionally Salient Regions Using Qualitative Agreement of Deep Neural Network Regressors.,14,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Deception detection and forensic psychology","Salient,Valence (chemistry),Computer science,Emotion recognition,Artificial intelligence,Arousal,Affective computing,Artificial neural network,Random forest,Machine learning,Pattern recognition (psychology),Psychology,Physics,Quantum mechanics,Neuroscience",,,"Srinivas Parthasarathy,Carlos Busso",
TAFFC2024,emotion,Towards Emotion-Aware Agents for Improved User Satisfaction and Partner Perception in Negotiation Dialogues.,5,Conflict Management and Negotiation,"Negotiation,Perception,Psychology,Expression (computer science),Computer science,Artificial intelligence,Social psychology,Natural language processing,Cognitive psychology,Sociology,Social science,Neuroscience,Programming language",,,"Kushal Chawla,Rene Clever,Jaysa Ramirez,Gale M. Lucas,Jonathan Gratch",
TAFFC2021,affect,Brain Dynamics During Arousal-Dependent Pleasant/Unpleasant Visual Elicitation: An Electroencephalographic Study on the Circumplex Model of Affect.,10,"Functional Brain Connectivity Studies,Neural and Behavioral Psychology Studies,Neural dynamics and brain function","Valence (chemistry),International Affective Picture System,Arousal,Electroencephalography,Psychology,Prefrontal cortex,Affect (linguistics),Amygdala,Insular cortex,Brain activity and meditation,Cognitive psychology,Lateralization of brain function,Neuroscience,Cognition,Communication,Chemistry,Organic chemistry",,,"Alberto Greco,Gaetano Valenza,Enzo Pasquale Scilingo",
TAFFC2023,emotion,Multi-Label Emotion Detection via Emotion-Specified Feature Extraction and Emotion Correlation Learning.,49,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Correlation,Computer science,Feature (linguistics),Context (archaeology),Feature extraction,Emotion classification,Artificial intelligence,Focus (optics),Sentence,Task (project management),Pattern recognition (psychology),Emotion recognition,Emotion perception,Mathematics,Facial expression,Engineering,Paleontology,Philosophy,Linguistics,Physics,Geometry,Systems engineering,Optics,Biology",,,"Jiawen Deng,Fuji Ren",
TAFFC2021,affective,Layered-Modeling of Affective and Sensory Experiences using Structural Equation Modeling: Touch Experiences of Plastic Surfaces as an Example.,12,"Color perception and design,Sensory Analysis and Statistical Methods,Design Education and Practice","Adjective,Structural equation modeling,Perception,Sensory system,Layer (electronics),Psychology,Computer science,Cognitive psychology,Artificial intelligence,Materials science,Machine learning,Noun,Neuroscience,Composite material",,,"Shogo Okamoto,Haruyo Kojima,Atsushi Yamagishi,Kyoichi Kato,Atsuko Tamada",
TAFFC2024,emotions,From What You See to What We Smell: Linking Human Emotions to Bio-Markers in Breath.,0,"Advanced Chemical Sensor Technologies,Time Series Analysis and Forecasting,Music and Audio Processing","Psychology,Cluster analysis,Cognitive psychology,Computer science,Artificial intelligence",,,"Joshua Bensemann,Hasnain Cheena,David Tse Jung Huang,Elizabeth Broadbent,Jonathan Williams,Jörg Wicker",
TAFFC2021,emotion,Compensation Techniques for Speaker Variability in Continuous Emotion Prediction.,4,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Speech recognition,Feature (linguistics),Computer science,Emotion recognition,Arousal,Psychology,Philosophy,Linguistics,Neuroscience",,,"Ting Dang,Vidhyasaharan Sethu,Eliathamby Ambikairajah",
TAFFC2023,affective,Computation of Sensory-Affective Relationships Depending on Material Categories of Pictorial Stimuli.,3,"Color perception and design,Multisensory perception and integration,Sensory Analysis and Statistical Methods","Psychology,Stimulus (psychology),Cognitive psychology,Set (abstract data type),Sensory system,Hierarchy,Adjective,Social psychology,Computer science,Artificial intelligence,Noun,Economics,Market economy,Programming language",,,"Shogo Okamoto,Kohta Wakamatsu,Shigeki Nakauchi,Jinhwan Kwon,Maki Sakamoto",
TAFFC2024,affective,Towards Human-Compatible Autonomous Car: A Study of Non-Verbal Turing Test in Automated Driving With Affective Transition Modelling.,2,"Human-Automation Interaction and Safety,Ethics and Social Impacts of AI,Death Anxiety and Social Exclusion","Ascription,Test (biology),Transition (genetics),Perspective (graphical),Psychology,Judgement,Driving simulator,Cognitive psychology,Turing test,Social psychology,Computer science,Simulation,Human–computer interaction,Artificial intelligence,Epistemology,Paleontology,Philosophy,Biochemistry,Chemistry,Gene,Biology",,,"Zhaoning Li,Qiaoli Jiang,Zhengming Wu,Anqi Liu,Haiyan Wu,Miner Huang,Kai Huang,Yixuan Ku",
TAFFC2022,emotion,DepecheMood++: A Bilingual Emotion Lexicon Built Through Simple Yet Powerful Techniques.,51,"Sentiment Analysis and Opinion Mining,Humor Studies and Applications,Emotion and Mood Recognition","Lexicon,Computer science,Natural language processing,Sentiment analysis,Artificial intelligence,Sadness,Word embedding,Vocabulary,Domain (mathematical analysis),Emotion classification,Happiness,Machine learning,Word (group theory),Simple (philosophy),Embedding,Linguistics,Psychology,Anger,Mathematical analysis,Social psychology,Philosophy,Mathematics,Epistemology,Psychiatry",,,"Oscar Araque,Lorenzo Gatti,Jacopo Staiano,Marco Guerini",
TAFFC2023,emotion,Active Learning With Complementary Sampling for Instructing Class-Biased Multi-Label Text Emotion Classification.,18,"Text and Document Classification Technologies,Sentiment Analysis and Opinion Mining,Machine Learning and Algorithms","Artificial intelligence,Representativeness heuristic,Computer science,Classifier (UML),Annotation,Machine learning,Class (philosophy),Natural language processing,Semi-supervised learning,Supervised learning,Sampling (signal processing),Labeled data,Quality (philosophy),Artificial neural network,Mathematics,Statistics,Philosophy,Filter (signal processing),Epistemology,Computer vision",,,"Xin Kang,Xuefeng Shi,Yunong Wu,Fuji Ren",
TAFFC2024,emotion,Disentangled Variational Autoencoder for Emotion Recognition in Conversations.,7,Emotion and Mood Recognition,"Autoencoder,Emotion recognition,Artificial intelligence,Speech recognition,Psychology,Computer science,Cognitive psychology,Pattern recognition (psychology),Artificial neural network",,,"Kailai Yang,Tianlin Zhang,Sophia Ananiadou",
INTERSPEECH2022,affect,Contribution of the glottal flow residual in affect-related voice transformation,0,"Voice and Speech Disorders,Speech Recognition and Synthesis,Phonetics and Phonology Research","Residual,Naturalness,Speech recognition,Transformation (genetics),Computer science,Acoustics,Inverse filter,SIGNAL (programming language),Filter (signal processing),Mathematics,Inverse,Algorithm,Physics,Biochemistry,Chemistry,Geometry,Gene,Quantum mechanics,Computer vision,Programming language",,https://www.isca-archive.org//interspeech_2022/wang22ca_interspeech.pdf,"Zihan Wang, Christer Gobl","This paper explores the contribution of the glottal flow residual in affect-related voice transformation. This signal, which is defined as the difference between the output of the inverse filter estimating the glottal flow signal and the modelled source signal, was analysed using multiple regression analysis. Results show that the strength of the residual varies as a function of the source parameters and this variation is frequency dependent: low frequency energy in the residual is mainly determined by the glottal excitation strength, whereas mid to high frequencies are more influenced by the glottal pulse shape. A method for modelling the residual is presented, which enables modifications based on the changes in source parameters used for voice transformation. This method makes it possible to use the residual as part of the voice source signal when transforming the voice quality in expressive speech synthesis. The result of a listening test, involving the transformation of a neutral voice to an angry or a sad voice, shows that including the glottal flow residual can improve the perceived naturalness of the synthesis. However, the fact that the transformed utterances are still relatively degraded indicates that other factors also need to be considered."
INTERSPEECH2022,emotion,User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning,23,"Privacy-Preserving Technologies in Data,Adversarial Robustness in Machine Learning,Internet Traffic Analysis and Secure E-voting","Computer science,Adversary,Differential privacy,Information leakage,Adversarial system,Inference,Computer security,Information privacy,Intuition,Machine learning,Artificial intelligence,Data mining,Philosophy,Epistemology",https://github.com/usc-sail/fed-ser-leakage}{https://github.com/usc-sail/fed-ser-leakage}.,https://www.isca-archive.org//interspeech_2022/feng22b_interspeech.pdf,"Tiantian Feng, Raghuveer Peri, Shrikanth Narayanan","Many existing privacy-enhanced speech emotion recognition (SER) frameworks focus on perturbing the original speech data through adversarial training within a centralized machine learning setup. However, this privacy protection scheme can fail since the adversary can still access the perturbed data. In recent years, distributed learning algorithms, especially federated learning (FL), have gained popularity to protect privacy in machine learning applications. While FL provides good intuition to safeguard privacy by keeping the data on local devices, prior work has shown that privacy attacks, such as attribute inference attacks, are achievable for SER systems trained using FL. In this work, we propose to evaluate the user-level differential privacy (UDP) in mitigating the privacy leaks of the SER system in FL. UDP provides theoretical privacy guarantees with privacy parameters $\epsilon$ and $\delta$. Our results show that the UDP can effectively decrease attribute information leakage while keeping the utility of the SER system with the adversary accessing one model update. However, the efficacy of the UDP suffers when the FL system leaks more model updates to the adversary. We make the code publicly available to reproduce the results in \href{https://github.com/usc-sail/fed-ser-leakage}{https://github.com/usc-sail/fed-ser-leakage}."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
IJCAI2022,emotion,CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for Single-Corpus and Cross-Corpus Speech Emotion Recognition,11,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Discriminative model,Artificial intelligence,Natural language processing,Speech recognition,Focus (optics),Deep learning,Task (project management),Physics,Management,Optics,Economics",https://github.com/MLDMXM2017/CTLMTNet.,https://www.ijcai.org/proceedings/2022/0320.pdf,"Xin-Cheng Wen, JiaXin Ye, Yan Luo, Yong Xu, Xuan-Ze Wang, Chang-Li Wu, Kun-Hong Liu","Speech Emotion Recognition (SER) has become a growing focus of research in human-computer interaction. An essential challenge in SER is to extract common attributes from different speakers or languages, especially when a specific source corpus has to be trained to recognize the unknown data coming from another speech corpus. To address this challenge, a Capsule Network (CapsNet) and Transfer Learning based Mixed Task Net (CTL-MTNet) are proposed to deal with both the single-corpus and cross-corpus SER tasks simultaneously in this paper. For the single-corpus task, the combination of Convolution-Pooling and Attention CapsNet module (CPAC) is designed by embedding the self-attention mechanism to the CapsNet, guiding the module to focus on the important features that can be fed into different capsules. The extracted high-level features by CPAC provide sufficient discriminative ability. Furthermore, to handle the cross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module (CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can learn the domain-invariant emotion representations through extracting the strong emotion commonness. Experiments including ablation studies and visualizations on both single- and cross-corpus tasks using four well-known SER datasets in different languages are conducted for performance evaluation and comparison. The results indicate that in both tasks the CTL-MTNet showed better performance in all cases compared to a number of state-of-the-art methods. The source code and the supplementary materials are available at: https://github.com/MLDMXM2017/CTLMTNet."
IJCAI2024,emotional,Inside Out: Emotional Multiagent Multimodal Dialogue Systems,0,"Speech and dialogue systems,Multi-Agent Systems and Negotiation","Computer science,Multi-agent system,Human–computer interaction,Artificial intelligence",,https://www.ijcai.org/proceedings/2024/1032.pdf,"Andrey V. Savchenko, Lyudmila V. Savchenko","In this paper, we introduce the novel technological framework for the development of emotional dialogue systems. Inspired by the ""Inside Out"" film, we propose to use multiple emotional agents based on Large Language Models (LLMs) to prepare answers to a user query. Their answers are aggregated into a single response, taking into account the current emotional state of a user. The latter is estimated by video-based facial expression recognition (FER). We introduce several publicly available lightweight neural networks that show near state-of-the-art results on the AffectNet dataset. Qualitative examples using either GPT-3.5 or LLama2 and Mistral demonstrate that the proposed approach leads to more emotional responses in LLMs."
TAFFC2021,affect,"AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups.",471,"Emotion and Mood Recognition,Mental Health Research Topics,Heart Rate Variability and Autonomic Control","Valence (chemistry),Arousal,Psychology,Mood,Affect (linguistics),Personality,Big Five personality traits,Context (archaeology),Cognitive psychology,Developmental psychology,Clinical psychology,Social psychology,Communication,Paleontology,Physics,Quantum mechanics,Biology",,,"Juan Abdon Miranda Correa,Mojtaba Khomami Abadi,Nicu Sebe,Ioannis Patras",
TAFFC2023,emotion,SparseDGCNN: Recognizing Emotion From Multichannel EEG Signals.,83,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Electroencephalography,Constraint (computer-aided design),Notation,Pattern recognition (psychology),Graph,Artificial intelligence,Convolutional neural network,Computer science,Support vector machine,Mathematics,Theoretical computer science,Arithmetic,Psychology,Geometry,Psychiatry",,,"Guanhua Zhang,Minjing Yu,Yong-Jin Liu,Guozhen Zhao,Dan Zhang,Wenming Zheng",
TAFFC2021,emotion,A Bi-Hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition.,225,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Electroencephalography,Discriminative model,Computer science,Artificial intelligence,Artificial neural network,Classifier (UML),Pattern recognition (psychology),Speech recognition,Adversarial system,Emotion recognition,Right hemisphere,Generality,Psychology,Cognitive psychology,Neuroscience,Psychotherapist",,,"Yang Li,Wenming Zheng,Yuan Zong,Zhen Cui,Tong Zhang,Xiaoyan Zhou",
TAFFC2024,emotions,Are 3D Face Shapes Expressive Enough for Recognising Continuous Emotions and Action Unit Intensities?,10,"Emotion and Mood Recognition,Face recognition and analysis,Face Recognition and Perception","Valence (chemistry),Arousal,Facial expression,Computer science,Face (sociological concept),Artificial intelligence,Cognitive psychology,Psychology,Pattern recognition (psychology),Speech recognition,Physics,Social psychology,Social science,Quantum mechanics,Sociology",,,"Mani Kumar Tellamekala,Ömer Sümer,Björn W. Schuller,Elisabeth André,Timo Giesbrecht,Michel F. Valstar",
TAFFC2021,emotional,Survey on Emotional Body Gesture Recognition.,360,"Hand Gesture Recognition Systems,Video Surveillance and Tracking Methods,Emotion and Mood Recognition","Gesture,Emotion recognition,Psychology,Gesture recognition,Computer science,Cognitive psychology,Human–computer interaction,Artificial intelligence,Computer vision,Speech recognition",,,"Fatemeh Noroozi,Ciprian Adrian Corneanu,Dorota Kaminska,Tomasz Sapinski,Sergio Escalera,Gholamreza Anbarjafari",
TAFFC2021,affect,Deep Learning for Human Affect Recognition: Insights and New Developments.,90,"Emotion and Mood Recognition,Music and Audio Processing,Anomaly Detection Techniques and Applications","Deep learning,Artificial intelligence,Computer science,Field (mathematics),Machine learning,Feature (linguistics),Affective computing,Artificial neural network,Focus (optics),Affect (linguistics),Deep neural networks,Psychology,Linguistics,Philosophy,Physics,Mathematics,Communication,Pure mathematics,Optics",,,"Philipp V. Rouast,Marc T. P. Adam,Raymond Chiong",
TAFFC2022,emotion,From Regional to Global Brain: A Novel Hierarchical Spatial-Temporal Neural Network Model for EEG Emotion Recognition.,205,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Discriminative model,Artificial intelligence,Electroencephalography,Pattern recognition (psychology),Computer science,Classifier (UML),Artificial neural network,Discriminator,Speech recognition,Psychology,Neuroscience,Telecommunications,Detector",,,"Yang Li,Wenming Zheng,Lei Wang,Yuan Zong,Zhen Cui",
TAFFC2021,emotion,Facial Expression Recognition with Identity and Emotion Joint Learning.,107,"Emotion and Mood Recognition,Face and Expression Recognition,Face recognition and analysis","Artificial intelligence,Facial expression,Computer science,Feature (linguistics),Convolutional neural network,Expression (computer science),Pattern recognition (psychology),Face (sociological concept),Identity (music),Deep learning,Joint (building),Speech recognition,Feature learning,Facial expression recognition,Facial recognition system,Engineering,Architectural engineering,Social science,Philosophy,Linguistics,Physics,Sociology,Acoustics,Programming language",,,"Ming Li,Hao Xu,Xingchang Huang,Zhanmei Song,Xiaolin Liu,Xin Li",
TAFFC2023,emotion,An Emotion Recognition Method for Game Evaluation Based on Electroencephalogram.,21,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Face and Expression Recognition","Emotion recognition,Electroencephalography,Computer science,Artificial intelligence,Speech recognition,Emotion classification,Psychology,Pattern recognition (psychology),Neuroscience",,,"Guanglong Du,Wenpei Zhou,Chunquan Li,Di Li,Peter X. Liu",
TAFFC2022,"emolabel,emotion",EmoLabel: Semi-Automatic Methodology for Emotion Annotation of Social Media Text.,16,"Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques,Spam and Phishing Detection","Annotation,Computer science,Process (computing),Natural language processing,Temporal annotation,Artificial intelligence,Information retrieval,Sentiment analysis,Social media,World Wide Web,Natural language,Language technology,Comprehension approach,Operating system",,,"Lea Canales,Walter Daelemans,Ester Boldrini,Patricio Martínez-Barco",
TAFFC2024,emotion,End-to-End Label Uncertainty Modeling in Speech Emotion Recognition Using Bayesian Neural Networks and Label Distribution Learning.,3,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Artificial neural network,Bayesian probability,Computer science,Artificial intelligence,Emotion recognition,Speech recognition,Multi-label classification,Machine learning,Probability distribution,Pattern recognition (psychology),Mathematics,Statistics",,,"Navin Raj Prabhu,Nale Lehmann-Willenbrock,Timo Gerkmann",
TAFFC2021,affect,Special Issue on Automated Perception of Human Affect from Longitudinal Behavioral Data.,0,Mental Health Research Topics,"Affect (linguistics),Affective computing,Modalities,Perception,Inference,Cognitive psychology,Artificial intelligence,Representation (politics),Cognition,Computer science,Psychology,Behavioural sciences,Human intelligence,Cognitive science,Communication,Social science,Neuroscience,Sociology,Politics,Political science,Law,Psychotherapist",,,"Pablo V. A. Barros,Stefan Wermter,Ognjen Rudovic,Hatice Gunes",
TAFFC2021,"emotion,emobed",EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings.,55,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Crossmodal,Modalities,Computer science,Embedding,Leverage (statistics),Artificial intelligence,Inference,Modality (human–computer interaction),Robustness (evolution),Cognitive psychology,Categorical variable,Emotion recognition,Machine learning,Speech recognition,Natural language processing,Psychology,Visual perception,Neuroscience,Perception,Social science,Biochemistry,Chemistry,Sociology,Gene",,,"Jing Han,Zixing Zhang,Zhao Ren,Björn W. Schuller",
TAFFC2022,"emotion,affect",Emotion Prediction with Weighted Appraisal Models - Towards Validating a Psychological Theory of Affect.,5,"Emotions and Moral Behavior,Emotion and Mood Recognition,Mental Health Research Topics","Affect (linguistics),Psychology,Appraisal theory,Cognitive psychology,Affective computing,Psychological Theory,Social psychology,Artificial intelligence,Computer science,Communication",,,"Laura S. F. Israel,Felix D. Schönbrodt",
TAFFC2024,emotion,MGEED: A Multimodal Genuine Emotion and Expression Detection Database.,6,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Color perception and design","Expression (computer science),Computer science,Affective computing,Artificial intelligence,Psychology,Natural language processing,Information retrieval,Human–computer interaction,Programming language",,,"Yiming Wang,Hui Yu,Weihong Gao,Yifan Xia,Charles Nduka",
TAFFC2021,emotion,Spatio-Temporal Encoder-Decoder Fully Convolutional Network for Video-Based Dimensional Emotion Recognition.,33,"Emotion and Mood Recognition,Human Pose and Action Recognition,Color perception and design","Computer science,Convolutional neural network,Encoder,Benchmark (surveying),Discriminative model,Artificial intelligence,Convolution (computer science),Key (lock),Encoding (memory),Convolutional code,Pattern recognition (psychology),Joint (building),Decoding methods,Computer vision,Speech recognition,Artificial neural network,Algorithm,Computer security,Geodesy,Geography,Operating system,Architectural engineering,Engineering",,,"Zhengyin Du,Suowei Wu,Di Huang,Weixin Li,Yunhong Wang",
TAFFC2022,emotion,ICA-Evolution Based Data Augmentation with Ensemble Deep Neural Networks Using Time and Frequency Kernels for Emotion Recognition from EEG-Data.,30,"EEG and Brain-Computer Interfaces,Blind Source Separation Techniques,Neural dynamics and brain function","Spectrogram,Artificial intelligence,Computer science,Pattern recognition (psychology),Convolutional neural network,Electroencephalography,Deep learning,Independent component analysis,Time–frequency analysis,Benchmark (surveying),Crossover,Artificial neural network,Speech recognition,Computer vision,Psychology,Geodesy,Filter (signal processing),Psychiatry,Geography",,,"Jun-Su Kang,Swathi Kavuri,Minho Lee",
TAFFC2021,"emotion,emotional",Modeling Emotion in Complex Stories: The Stanford Emotional Narratives Dataset.,41,"Emotion and Mood Recognition,Mental Health Research Topics,Sentiment Analysis and Opinion Mining","Affective computing,Narrative,Discriminative model,Computer science,Benchmark (surveying),Artificial intelligence,Generative grammar,Valence (chemistry),Emotion recognition,Cognitive psychology,Psychology,Machine learning,Philosophy,Linguistics,Physics,Geodesy,Quantum mechanics,Geography",,,"Desmond C. Ong,Zhengxuan Wu,Zhi-Xuan Tan,Marianne Reddan,Isabella Kahhale,Alison Mattek,Jamil Zaki",
TAFFC2023,emotion,Human Emotion Recognition With Relational Region-Level Analysis.,16,"Emotion and Mood Recognition,Human Pose and Action Recognition,Face and Expression Recognition","Computer science,Perception,Artificial intelligence,Feature (linguistics),Object (grammar),Emotion perception,Emotion recognition,Image (mathematics),Dependency (UML),Affective computing,Key (lock),Pattern recognition (psychology),Computer vision,Psychology,Facial expression,Philosophy,Linguistics,Computer security,Neuroscience",,,"Weixin Li,Xuan Dong,Yunhong Wang",
TAFFC2022,affective,Affective Dynamics: Causality Modeling of Temporally Evolving Perceptual and Affective Responses.,18,"Sensory Analysis and Statistical Methods,Color perception and design,Multisensory perception and integration","Perception,Granger causality,Cognitive psychology,Psychology,Causality (physics),Dynamics (music),Computer science,Machine learning,Neuroscience,Pedagogy,Physics,Quantum mechanics",,,"Takumu Okada,Shogo Okamoto,Yoji Yamada",
TAFFC2021,emotion,Exploiting Multi-CNN Features in CNN-RNN Based Dimensional Emotion Recognition on the OMG in-the-Wild Dataset.,131,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Video Surveillance and Tracking Methods","Computer science,Emotion recognition,Artificial intelligence,Ranking (information retrieval),Recurrent neural network,Boosting (machine learning),Multimodal learning,Modalities,Machine learning,Valence (chemistry),Modality (human–computer interaction),Pattern recognition (psychology),Visualization,Speech recognition,Artificial neural network,Social science,Physics,Quantum mechanics,Sociology",,,"Dimitrios Kollias,Stefanos Zafeiriou",
TAFFC2023,emotion,The Pixels and Sounds of Emotion: General-Purpose Representations of Arousal in Games.,43,"Emotion and Mood Recognition,Humor Studies and Applications,Media Influence and Health","Computer science,Affect (linguistics),Arousal,Preference,Domain (mathematical analysis),Artificial intelligence,Pixel,Representation (politics),Session (web analytics),Test (biology),Human–computer interaction,Multimedia,Psychology,Mathematics,Paleontology,Statistics,Communication,World Wide Web,Biology,Neuroscience,Politics,Political science,Law,Mathematical analysis",,,"Konstantinos Makantasis,Antonios Liapis,Georgios N. Yannakakis",
TAFFC2024,emotion,PR-PL: A Novel Prototypical Representation Based Pairwise Learning Framework for Emotion Recognition Using EEG Signals.,20,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Artificial intelligence,Generalizability theory,Computer science,Pairwise comparison,Electroencephalography,Discriminative model,Feature learning,Feature (linguistics),Pattern recognition (psychology),Benchmark (surveying),Machine learning,ENCODE,Representation (politics),Speech recognition,Psychology,Developmental psychology,Linguistics,Philosophy,Biochemistry,Chemistry,Geodesy,Psychiatry,Politics,Political science,Law,Gene,Geography",,,"Rushuang Zhou,Zhiguo Zhang,Hong Fu,Li Zhang,Linling Li,Gan Huang,Fali Li,Xin Yang,Yining Dong,Yuan-Ting Zhang,Zhen Liang",
TAFFC2024,emotion,Fine-Grained Interpretability for EEG Emotion Recognition: Concat-Aided Grad-CAM and Systematic Brain Functional Network.,15,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Interpretability,Computer science,Artificial intelligence,Electroencephalography,Pattern recognition (psychology),Feature (linguistics),Graph,Machine learning,Psychology,Neuroscience,Theoretical computer science,Linguistics,Philosophy",,,"Bingxiu Liu,Jifeng Guo,C. L. Philip Chen,Xia Wu,Tong Zhang",
TAFFC2021,"affective,emotion",Longitudinal Observational Evidence of the Impact of Emotion Regulation Strategies on Affective Expression.,22,"Mental Health Research Topics,Emotion and Mood Recognition,Neural and Behavioral Psychology Studies","Affect (linguistics),Psychology,Observational study,Facial expression,Expression (computer science),Cognition,Affective science,Longitudinal study,Negative emotion,Scale (ratio),Developmental psychology,Cognitive psychology,Emotion classification,Neuroscience,Communication,Medicine,Computer science,Programming language,Physics,Pathology,Quantum mechanics",,,"Daniel McDuff,Eunice Jun,Kael Rowan,Mary Czerwinski",
TAFFC2023,emotional,Automatic Detection of Emotional Changes Induced by Social Support Loss Using fMRI.,4,"Functional Brain Connectivity Studies,Neural dynamics and brain function,EEG and Brain-Computer Interfaces","Computer science,Task (project management),Artificial intelligence,Pattern recognition (psychology),Noise (video),Set (abstract data type),Process (computing),Feature (linguistics),Psychology,Machine learning,Engineering,Image (mathematics),Linguistics,Philosophy,Systems engineering,Programming language,Operating system",,,"Cemre Candemir,Ali Saffet Gonul,M. Alper Selver",
IF2024,emotion,Fusing pairwise modalities for emotion recognition in conversations.,17,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Modalities,Computer science,Pairwise comparison,Artificial intelligence,Benchmark (surveying),Modality (human–computer interaction),Context (archaeology),Feature (linguistics),Machine learning,Process (computing),Scalability,Database,Social science,Sociology,Paleontology,Linguistics,Philosophy,Geodesy,Biology,Geography,Operating system",,,"Chunxiao Fan,Jie Lin,Rui Mao,Erik Cambria",
IF2024,emotion,Hierarchical multimodal-fusion of physiological signals for emotion recognition with scenario adaption and contrastive alignment.,14,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Artificial intelligence,Modality (human–computer interaction),Feature (linguistics),Pattern recognition (psychology),Modalities,Speech recognition,Machine learning,Social science,Philosophy,Linguistics,Sociology",,,"Jiehao Tang,Zhuang Ma,Kaiyu Gan,Jianhua Zhang,Zhong Yin",
IF2024,emotion,Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Bottleneck,Computer science,Adversarial system,Emotion recognition,Artificial intelligence,Graph,Information fusion,Natural language processing,Speech recognition,Theoretical computer science,Embedded system",,,"Yuntao Shou,Tao Meng,Wei Ai,Fuchen Zhang,Nan Yin,Keqin Li",
IF2022,"affective,emotion","A systematic review on affective computing: emotion models, databases, and recent advances.",276,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Affective computing,Computer science,Gesture,Benchmark (surveying),Emotion recognition,Affect (linguistics),Facial expression,Sentiment analysis,Domain (mathematical analysis),Database,Artificial intelligence,Human–computer interaction,Psychology,Mathematical analysis,Mathematics,Communication,Geodesy,Geography",,,"Yan Wang,Wei Song,Wei Tao,Antonio Liotta,Dawei Yang,Xinlei Li,Shuyong Gao,Yixuan Sun,Weifeng Ge,Wei Zhang,Wenqiang Zhang",
IF2022,emotion,An emotion role mining approach based on multiview ensemble learning in social networks.,22,"Sentiment Analysis and Opinion Mining,Complex Network Analysis Techniques,Spam and Phishing Detection","Computer science,Microblogging,Random forest,Artificial intelligence,Emotional contagion,Sentiment analysis,Ensemble learning,Social media,Graph,Machine learning,Psychology,Social psychology,World Wide Web,Theoretical computer science",,,"Yajun Du,Yakun Wang,Jinrong Hu,Xianyong Li,Xiaoliang Chen",
IF2021,emotion,Conversational transfer learning for emotion recognition.,86,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Computer science,Transfer of learning,Artificial intelligence,Conversation,Classifier (UML),Robustness (evolution),Sentence,Natural language processing,Machine learning,Task (project management),Encoder,Labeled data,Generative grammar,Psychology,Biochemistry,Chemistry,Management,Communication,Economics,Gene,Operating system",,,"Devamanyu Hazarika,Soujanya Poria,Roger Zimmermann,Rada Mihalcea",
IF2021,"affective,emotional","On the origin of the methodology for the scalable fusion of affective channels in a continuous emotional space and the ""emotional kinematics"" filtering technique - A correction.",4,"Emotion and Mood Recognition,Speech and dialogue systems,Cognitive Science and Education Research","Kinematics,Space (punctuation),Computer science,Scalability,Sensor fusion,Artificial intelligence,Database,Physics,Classical mechanics,Operating system",,,"Isabelle Hupont,Eva Cerezo,Sergio Ballano,Sandra Baldassarri",
TAFFC2022,emotion,Spontaneous Speech Emotion Recognition Using Multiscale Deep Convolutional LSTM.,97,"Emotion and Mood Recognition,Speech and Audio Processing,EEG and Brain-Computer Interfaces","Spectrogram,Computer science,Convolutional neural network,Artificial intelligence,Speech recognition,Deep learning,Utterance,Emotion recognition,Affective computing,Pattern recognition (psychology)",,,"Shiqing Zhang,Xiaoming Zhao,Qi Tian",
TAFFC2022,affective,Dynamics of Blink and Non-Blink Cyclicity for Affective Assessment: A Case Study for Stress Identification.,6,"Heart Rate Variability and Autonomic Control,Color perception and design,Sleep and Work-Related Fatigue","Attentional blink,Psychology,Categorical variable,Stroop effect,Identification (biology),Cognitive psychology,Eye tracking,Stress (linguistics),Perception,Computer science,Artificial intelligence,Machine learning,Cognition,Linguistics,Philosophy,Botany,Neuroscience,Biology",,,"Peng Ren,Armando Barreto,Xiaole Ma,Shengnan Liu,Min Zhang,Ying Wang,Yeyun Dong,Dezhong Yao",
TAFFC2024,emotion,Adversarial Domain Generalized Transformer for Cross-Corpus Speech Emotion Recognition.,7,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Discriminative model,Artificial intelligence,Generalizability theory,Feature learning,Machine learning,Transfer of learning,Transformer,Feature extraction,Adversarial system,Softmax function,Pattern recognition (psychology),Deep learning,Speech recognition,Engineering,Mathematics,Statistics,Voltage,Electrical engineering",,,"Yuan Gao,Longbiao Wang,Jiaxing Liu,Jianwu Dang,Shogo Okada",
TAFFC2021,emotion,Using Circular Models to Improve Music Emotion Recognition.,11,"Music and Audio Processing,Neuroscience and Music Perception,Animal Vocal Communication and Behavior","Categorical variable,Computer science,Valence (chemistry),Affect (linguistics),Artificial intelligence,Affective computing,Arousal,Regression,Natural language processing,Pattern recognition (psychology),Machine learning,Mathematics,Psychology,Statistics,Communication,Physics,Quantum mechanics,Neuroscience",,,"Isabelle Dufour,George Tzanetakis",
TAFFC2023,emotive,When is a Haptic Message Like an Inside Joke? Digitally Mediated Emotive Communication Builds on Shared History.,2,"Tactile and Sensory Interactions,Virtual Reality Applications and Impacts,Multisensory perception and integration","Haptic technology,Emotive,Context (archaeology),Set (abstract data type),Wearable computer,Human–computer interaction,Interpersonal communication,Computer science,Joke,Matching (statistics),Interpretation (philosophy),Psychology,Social psychology,Artificial intelligence,Paleontology,Philosophy,Linguistics,Statistics,Mathematics,Epistemology,Embedded system,Biology,Programming language",,,"Xi Laura Cang,Ali Israr,Karon E. MacLean",
TAFFC2021,emotion,Autoencoder for Semisupervised Multiple Emotion Detection of Conversation Transcripts.,19,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Conversation,Word2vec,Autoencoder,Natural language processing,Computer science,Artificial intelligence,Lexicon,Context (archaeology),Word embedding,Set (abstract data type),Word (group theory),Computational linguistics,Embedding,Deep learning,Speech recognition,Linguistics,Psychology,Communication,Paleontology,Philosophy,Biology,Programming language",,,"Duc Anh Phan,Yuji Matsumoto,Hiroyuki Shindo",
TAFFC2023,"emotion,emotions",A Spontaneous Driver Emotion Facial Expression (DEFE) Dataset for Intelligent Vehicles: Emotions Triggered by Video-Audio Clips in Driving Scenarios.,55,"Emotion and Mood Recognition,Sleep and Work-Related Fatigue,EEG and Brain-Computer Interfaces","Facial expression,Valence (chemistry),Driving simulator,Arousal,Emotion recognition,Computer science,Facial electromyography,Emotion classification,Expression (computer science),Negative emotion,Speech recognition,Cognitive psychology,Psychology,Artificial intelligence,Social psychology,Physics,Quantum mechanics,Programming language",,,"Wenbo Li,Yaodong Cui,Yintao Ma,Xingxin Chen,Guofa Li,Guanzhong Zeng,Gang Guo,Dongpu Cao",
TAFFC2022,emotion,Analyzing Group-Level Emotion with Global Alignment Kernel based Approach.,8,"Emotion and Mood Recognition,Face and Expression Recognition,Advanced Computing and Algorithms","Kernel (algebra),Computer science,Artificial intelligence,Support vector machine,Pattern recognition (psychology),sort,Robustness (evolution),Convolutional neural network,Kernel principal component analysis,Kernel method,Machine learning,Mathematics,Information retrieval,Biochemistry,Chemistry,Combinatorics,Gene",,,"Xiaohua Huang,Abhinav Dhall,Roland Goecke,Matti Pietikäinen,Guoying Zhao",
TAFFC2021,"affective,emotional",A Mathematical Description of Emotional Processes and Its Potential Applications to Affective Computing.,12,"Emotion and Mood Recognition,Neural dynamics and brain function,Cognitive Science and Education Research","Affective computing,Computer science,Artificial intelligence,Process (computing),Valence (chemistry),Neurophysiology,Arousal,Space (punctuation),Cognitive science,Machine learning,Cognitive psychology,Psychology,Social psychology,Physics,Quantum mechanics,Neuroscience,Operating system",,,"Luca Puviani,Sidita Rama,Giorgio Matteo Vitetta",
TAFFC2023,"affective,emotions",A Multidimensional Culturally Adapted Representation of Emotions for Affective Computational Simulation and Recognition.,8,"Color perception and design,Emotion and Mood Recognition,Face Recognition and Perception","Pleasure,Arousal,Representation (politics),Fuzzy logic,Space (punctuation),Affective computing,Computer science,Expression (computer science),Process (computing),Cognitive psychology,Psychology,Fuzzy set,Artificial intelligence,Social psychology,Neuroscience,Politics,Political science,Law,Programming language,Operating system",,,"Joaquín Taverner,Emilio Vivancos,Vicent J. Botti",
TAFFC2022,emotional,ACSEE: Antagonistic Crowd Simulation Model With Emotional Contagion and Evolutionary Game Theory.,42,"Evacuation and Crowd Dynamics,Crime Patterns and Interventions,Fire effects on ecosystems","Crowd psychology,Emotional contagion,Crowd simulation,Psychology,Social psychology,Game theory,Computer science,Video game,Cognitive psychology,Computer security,Crowds,Microeconomics,Economics,Multimedia",,,"Chaochao Li,Pei Lv,Dinesh Manocha,Hua Wang,Yafei Li,Bing Zhou,Mingliang Xu",
TAFFC2021,emotion,Cross-Cultural and Cultural-Specific Production and Perception of Facial Expressions of Emotion in the Wild.,80,"Face Recognition and Perception,Multisensory perception and integration,Color perception and design","Emotive,Disgust,Facial expression,Psychology,Happiness,Valence (chemistry),Perception,Cognitive psychology,Arousal,Emotional expression,Affect (linguistics),Expression (computer science),Emotion perception,Social psychology,Communication,Anger,Computer science,Sociology,Physics,Quantum mechanics,Neuroscience,Anthropology,Programming language",,,"Ramprakash Srinivasan,Aleix M. Martínez",
INTERSPEECH2022,emotion,Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling,13,"Music and Audio Processing,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Emotion recognition,Benchmark (surveying),Speech recognition,Raw data,Labeled data,Semi-supervised learning,Voice activity detection,Cloud computing,Artificial intelligence,Machine learning,Speech processing,Geodesy,Programming language,Geography,Operating system",https://github.com/usc-sail/fed-ser-semi}{https://github.com/usc-sail/fed-ser-semi}.,https://www.isca-archive.org//interspeech_2022/feng22_interspeech.pdf,"Tiantian Feng, Shrikanth Narayanan","Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate $l=20\%$ using two SER benchmark datasets: IEMOCAP and MSP-Improv. The implementation of this work is at \href{https://github.com/usc-sail/fed-ser-semi}{https://github.com/usc-sail/fed-ser-semi}."
INTERSPEECH2023,emotion,Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning,0,"Advanced Sensor and Control Systems,Music and Audio Processing,Advanced Algorithms and Applications","Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Pattern recognition (psychology),Natural language processing",,https://www.isca-archive.org//interspeech_2023/sun23f_interspeech.pdf,"Yifu Sun, Xulong Zhang, Jianzong Wang, Ning Cheng, Kaiyu Hu, Jing Xiao","The production and annotation of music datasets requires very specialized background knowledge, which is difficult for most people to complete. Therefore, the number of annotated music samples is at a premium for Music Information Retrieval (MIR) tasks. Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources. However, when training at the segment level, segment labels are the major problem. The most commonly used method is that segment inherits the label of the clip containing it, but as we all know, music emotion is not constant during the whole clip. Doing so will introduce label noise and make the training overfit easily. To handle the noisy label issue, we propose a semi-supervised self-learning method and achieve better results than previous methods."
TAFFC2024,emotion,Multi-Party Conversation Modeling for Emotion Recognition.,2,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Conversation,Computer science,Dialog box,ENCODE,Directed acyclic graph,Directed graph,Natural language processing,Artificial intelligence,Graph,Speech recognition,Human–computer interaction,Theoretical computer science,Psychology,Algorithm,Communication,Biochemistry,Chemistry,World Wide Web,Gene",,,"Xiaojun Quan,Siyue Wu,Junqing Chen,Weizhou Shen,Jianxing Yu",
TAFFC2022,emotion,An Active Learning Paradigm for Online Audio-Visual Emotion Recognition.,92,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Modalities,Convolutional neural network,Modality (human–computer interaction),Artificial intelligence,Deep learning,Feature extraction,Emotion recognition,Affective computing,Feature (linguistics),Artificial neural network,Reinforcement learning,Speech recognition,Social science,Linguistics,Philosophy,Sociology",,,"Ioannis Kansizoglou,Loukas Bampis,Antonios Gasteratos",
TAFFC2022,affect,Holistic Affect Recognition Using PaNDA: Paralinguistic Non-Metric Dimensional Analysis.,24,"Emotion and Mood Recognition,Face and Expression Recognition,Face Recognition and Perception","Paralanguage,Task (project management),Multi-task learning,Computer science,Metric (unit),Construct (python library),Transfer of learning,Artificial intelligence,Task analysis,Affect (linguistics),Perspective (graphical),Machine learning,Artificial neural network,Speech recognition,Psychology,Communication,Operations management,Management,Programming language,Economics",,,"Yue Zhang,Felix Weninger,Björn W. Schuller,Rosalind W. Picard",
TAFFC2022,emotions,Recognition of Advertisement Emotions With Application to Computational Advertising.,27,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Color perception and design","Convolutional neural network,Computer science,Recall,ENCODE,Affect (linguistics),Task (project management),Affective computing,Electroencephalography,Emotion recognition,Speech recognition,Artificial intelligence,Psychology,Cognitive psychology,Communication,Biochemistry,Chemistry,Management,Psychiatry,Economics,Gene",,,"Abhinav Shukla,Shruti Shriya Gullapuram,Harish Katti,Mohan S. Kankanhalli,Stefan Winkler,Ramanathan Subramanian",
TAFFC2023,"affect,affecton",AffectON: Incorporating Affect Into Dialog Generation.,2,"Topic Modeling,Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Dialog box,Affect (linguistics),Natural language processing,Computer science,Conversation,Context (archaeology),Artificial intelligence,Natural language,Inference,Linguistics,Psychology,Communication,World Wide Web,Paleontology,Biology,Philosophy",,,"Zana Buçinca,Yücel Yemez,Engin Erzin,Tevfik Metin Sezgin",
TAFFC2024,"emotion,emotions","Fake News, Real Emotions: Emotion Analysis of COVID-19 Infodemic in Weibo.",3,"Misinformation and Its Impacts,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Sentiment analysis,Coronavirus disease 2019 (COVID-19),Emotion detection,Negative emotion,Computer science,Emotion recognition,Psychology,Emotional contagion,Fake news,Cognitive psychology,Internet privacy,Artificial intelligence,Social psychology,Medicine,Disease,Pathology,Infectious disease (medical specialty)",,,"Mingyu Wan,Yin Zhong,Xuefeng Gao,Sophia Yat Mei Lee,Chu-Ren Huang",
TAFFC2022,emotion,Two-Stage Fuzzy Fusion Based-Convolution Neural Network for Dynamic Emotion Recognition.,34,"Emotion and Mood Recognition,Face and Expression Recognition,Advanced Computing and Algorithms","Computer science,Discriminative model,Artificial intelligence,Facial expression,Pattern recognition (psychology),Convolutional neural network,Fuzzy logic,Benchmark (surveying),Convolution (computer science),Speech recognition,Artificial neural network,Geodesy,Geography",,,"Min Wu,Wanjuan Su,Luefeng Chen,Witold Pedrycz,Kaoru Hirota",
TAFFC2021,emotional,First Impressions Count! The Role of the Human's Emotional State on Rapport Established with an Empathic versus Neutral Virtual Therapist.,26,"Social Robot Interaction and HRI,Action Observation and Synchronization,AI in Service Interactions","Feeling,Psychology,Empathy,Empathic concern,Psychotherapist,Social psychology,Perspective-taking",,,"Hedieh Ranjbartabar,Deborah Richards,Ayse Aysin Bilgin,Cat Kutay",
TAFFC2023,"emotion,emotional",Emotion-Regularized Conditional Variational Autoencoder for Emotional Response Generation.,13,"Topic Modeling,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Autoencoder,Latent variable,Sequence (biology),Computer science,Artificial intelligence,Space (punctuation),Cognitive psychology,Psychology,Artificial neural network,Biology,Genetics,Operating system",,,"Yu-Ping Ruan,Zhen-Hua Ling",
TAFFC2021,emotion,A Review on Nonlinear Methods Using Electroencephalographic Recordings for Emotion Recognition.,112,"EEG and Brain-Computer Interfaces,Blind Source Separation Techniques,Neural dynamics and brain function","Electroencephalography,Nonlinear system,Emotion recognition,Computer science,Pattern recognition (psychology),Stimulus (psychology),Artificial intelligence,Speech recognition,Signal processing,Brain activity and meditation,Field (mathematics),Psychology,Cognitive psychology,Neuroscience,Mathematics,Physics,Quantum mechanics,Telecommunications,Radar,Pure mathematics",,,"Beatriz García-Martínez,Arturo Martínez-Rodrigo,Raúl Alcaraz,Antonio Fernández-Caballero",
TAFFC2023,affective,Estimating Affective Taste Experience Using Combined Implicit Behavioral and Neurophysiological Measures.,7,"Biochemical Analysis and Sensing Techniques,Olfactory and Sensory Function Studies,Sensory Analysis and Statistical Methods","Wine tasting,Arousal,Psychology,Neurophysiology,Taste,Low arousal theory,Cognitive psychology,Social psychology,Neuroscience,Physics,Wine,Optics",,,"Anne-Marie Brouwer,Tim J. van den Broek,Maarten A. Hogervorst,Daisuke Kaneko,Alexander Toet,Victor L. Kallen,Jan B. F. van Erp",
TAFFC2021,emotion,Capturing Emotion Distribution for Multimedia Emotion Tagging.,24,"Sentiment Analysis and Opinion Mining,Video Analysis and Summarization,Music and Audio Processing","Discriminator,Computer science,Classifier (UML),Emotion classification,Ground truth,Emotion recognition,Artificial intelligence,Labeled data,Multimedia,Natural language processing,Detector,Telecommunications",,,"Shangfei Wang,Guozhu Peng,Zhuangqiang Zheng,Zhiwei Xu",
TAFFC2024,affective,Multi-Rater Consensus Learning for Modeling Multiple Sparse Ratings of Affective Behaviour.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Emotions and Moral Behavior","Affect (linguistics),Artificial intelligence,Computer science,Machine learning,Annotation,Affective computing,Process (computing),Natural language processing,Psychology,Communication,Operating system",,,"Luca Romeo,Temitayo A. Olugbade,Massimiliano Pontil,Nadia Bianchi-Berthouze",
TAFFC2021,emotion,An EEG-Based Brain Computer Interface for Emotion Recognition and Its Application in Patients with Disorder of Consciousness.,130,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Brain–computer interface,Electroencephalography,Minimally conscious state,Persistent vegetative state,Consciousness,Psychology,Interface (matter),Emotion recognition,CLIPS,Coma (optics),Motor imagery,Cognitive psychology,Speech recognition,Computer science,Artificial intelligence,Audiology,Neuroscience,Medicine,Physics,Bubble,Maximum bubble pressure method,Parallel computing,Optics",,,"Haiyun Huang,Qiuyou Xie,Jiahui Pan,Yanbin He,Zhenfu Wen,Ronghao Yu,Yuanqing Li",
TAFFC2022,"affective,emotions",Affective Dynamics: Principal Motion Analysis of Temporal Dominance of Sensations and Emotions Data.,14,"Sensory Analysis and Statistical Methods,Biochemical Analysis and Sensing Techniques,Color perception and design","Principal component analysis,Extant taxon,Dominance (genetics),Computer science,Motion (physics),Artificial intelligence,Temporal database,Pattern recognition (psychology),Dynamics (music),Mathematics,Psychology,Cognitive psychology,Data mining,Pedagogy,Biochemistry,Chemistry,Evolutionary biology,Biology,Gene",,,"Shogo Okamoto,Yuki Ehara,Takumu Okada,Yoji Yamada",
TAFFC2021,emotion,Feature Extraction and Selection for Emotion Recognition from Electrodermal Activity.,186,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Neural dynamics and brain function","Feature selection,Feature extraction,Pattern recognition (psychology),Mutual information,Valence (chemistry),Computer science,Artificial intelligence,Arousal,Conditional mutual information,Emotion classification,Emotion recognition,Speech recognition,Feature (linguistics),Mel-frequency cepstrum,Machine learning,Psychology,Linguistics,Philosophy,Physics,Quantum mechanics,Neuroscience",,,"Jainendra Shukla,Miguel Barreda-Ángeles,Joan Oliver,Gora Chand Nandi,Domenec Puig",
TAFFC2024,emotion,Emotion Recognition From Full-Body Motion Using Multiscale Spatio-Temporal Network.,6,"Human Pose and Action Recognition,Hand Gesture Recognition Systems,Emotion and Mood Recognition","Fuse (electrical),Computer science,Artificial intelligence,Motion (physics),Generalization,Convergence (economics),Focus (optics),Pattern recognition (psychology),Scale (ratio),Computer vision,Machine learning,Algorithm,Mathematics,Mathematical analysis,Physics,Economic growth,Electrical engineering,Economics,Engineering,Optics,Quantum mechanics",,,"Tao Wang,Shuang Liu,Feng He,Weina Dai,Minghao Du,Yufeng Ke,Dong Ming",
TAFFC2021,emotional,Over-Sampling Emotional Speech Data Based on Subjective Evaluations Provided by Multiple Individuals.,10,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Sentence,Classifier (UML),Natural language processing,Categorical variable,Artificial intelligence,Perception,Ground truth,Emotion perception,Sampling (signal processing),Sentiment analysis,Speech recognition,Machine learning,Psychology,Facial expression,Filter (signal processing),Neuroscience,Computer vision",,,"Reza Lotfian,Carlos Busso",
TAFFC2023,emotion,Investigating Multisensory Integration in Emotion Recognition Through Bio-Inspired Computational Models.,11,"Multisensory perception and integration,Olfactory and Sensory Function Studies,Neural dynamics and brain function","Computer science,Modalities,Artificial intelligence,Multisensory integration,Modality (human–computer interaction),Artificial neural network,Robustness (evolution),Machine learning,Deep learning,Affective computing,Psychology,Perception,Social science,Biochemistry,Neuroscience,Chemistry,Sociology,Gene",,,"Esma Mansouri-Benssassi,Juan Ye",
TAFFC2021,affective,Adapting Software with Affective Computing: A Systematic Review.,61,"Innovative Human-Technology Interaction,Emotion and Mood Recognition,Impact of Technology on Adolescents","Computer science,Affective computing,Adaptation (eye),Premise,Human–computer interaction,State (computer science),Software,Computer Applications,Data science,Psychology,Linguistics,Philosophy,Public relations,Algorithm,Neuroscience,Political science,Programming language",,,"Renan Vinicius Aranha,Cléber Gimenez Corrêa,Fátima L. S. Nunes",
INTERSPEECH2022,emotion,Discriminative Adversarial Learning for Speaker Independent Emotion Recognition,0,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Discriminative model,Adversarial system,Computer science,Speech recognition,Speaker recognition,Speaker diarisation,Emotion recognition,Artificial intelligence,Natural language processing,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2022/kasun22_interspeech.pdf,"Chamara Kasun, Chung Soo Ahn, Jagath Rajapakse, Zhiping Lin, Guang-Bin Huang","Traditional adversarial learning (AL) algorithms learns a speaker independent embedding from low level audio features. This paper introduces discriminative adversarial learning (DAL) which learn a discriminative speaker independent embedding from low level audio features such as mel frequency cepstral coefficients (MFCC) and high level audio features such as Interspeech Para-linguistics Challenge 2010. To this end, DAL jointly minimize triplet and cross-entropy losses with gradient reversal strategy for speaker independent emotion recognition (SIER). Triplet loss reduce intra-class and increase the inter-class embedding distance to improve the discriminativeness of the embedding while the cross-entropy loss determine the emotion or speaker class of the embedding and gradient reversal learn speaker independent embedding for SIER. Experiments on Emo-DB and RAVDESS datasets show that DAL outperform other traditional adversarial learning (AL) algorithms."
INTERSPEECH2023,emotion,Transfer Learning for Personality Perception via Speech Emotion Recognition,1,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Personality,Perception,Perspective (graphical),Emotion recognition,Affect (linguistics),Transfer of learning,Cognitive psychology,Psychology,Affective computing,Big Five personality traits,Computer science,Speech recognition,Artificial intelligence,Social psychology,Communication,Developmental psychology,Neuroscience",,https://www.isca-archive.org//interspeech_2023/li23da_interspeech.pdf,"Yuanchao Li, Peter Bell, Catherine Lai","Holistic perception of affective attributes is an important human perceptual ability. However, this ability is far from being realized in current affective computing, as not all of the attributes are well studied and their interrelationships are poorly understood. In this work, we investigate the relationship between two affective attributes: personality and emotion, from a transfer learning perspective. Specifically, we transfer Transformer-based and wav2vec2-based emotion recognition models to perceive personality from speech across corpora. Compared with previous studies, our results show that transferring emotion recognition is effective for personality perception. Moreoever, this allows for better use and exploration of small personality corpora. We also provide novel findings on the relationship between personality and emotion that will aid future research on holistic affect recognition."
TAFFC2024,affective,An Affective Brain-Computer Interface Based on a Transfer Learning Method.,2,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Brain–computer interface,Affective computing,Affect (linguistics),Transfer of learning,Emotion classification,Electroencephalography,Artificial intelligence,Support vector machine,Computer science,Psychology,Cognitive psychology,Communication,Psychiatry",,,"Weichen Huang,Zijing Guan,Kendi Li,Yajun Zhou,Yuanqing Li",
TAFFC2021,emotion,Designing an Experience Sampling Method for Smartphone Based Emotion Detection.,17,"Personal Information Management and User Behavior,Mental Health Research Topics,Green IT and Sustainability","Computer science,Experience sampling method,Sampling (signal processing),Quality (philosophy),Psychology,Computer vision,Social psychology,Philosophy,Filter (signal processing),Epistemology",,,"Surjya Ghosh,Niloy Ganguly,Bivas Mitra,Pradipta De",
TAFFC2023,emotion,EEG-Based Emotion Recognition via Neural Architecture Search.,40,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Artificial intelligence,Convolutional neural network,Electroencephalography,Artificial neural network,Brain–computer interface,Pattern recognition (psychology),Arousal,Reinforcement learning,Recurrent neural network,Machine learning,Psychology,Psychiatry,Neuroscience,Biology",,,"Chang Li,Zhongzhen Zhang,Rencheng Song,Juan Cheng,Yu Liu,Xun Chen",
TAFFC2022,emotion,Unsupervised Learning in Reservoir Computing for EEG-Based Emotion Recognition.,52,"Neural Networks and Reservoir Computing,Neural dynamics and brain function,EEG and Brain-Computer Interfaces","Computer science,Artificial intelligence,Electroencephalography,Echo state network,Feature extraction,Emotion recognition,Reservoir computing,Pattern recognition (psychology),Artificial neural network,Feature (linguistics),Representation (politics),Machine learning,Recurrent neural network,Affective computing,Psychology,Linguistics,Philosophy,Psychiatry,Politics,Political science,Law",,,"Rahma Fourati,Boudour Ammar,Javier J. Sánchez Medina,Adel M. Alimi",
TAFFC2021,"emotion,emotional",Towards Disorder-Independent Automatic Assessment of Emotional Competence in Neurological Patients with a Classical Emotion Recognition System: Application in Foreign Accent Syndrome.,5,"Voice and Speech Disorders,EEG and Brain-Computer Interfaces,Emotion and Mood Recognition","Emotive,Paralanguage,Speech recognition,Competence (human resources),Computer science,Automation,Psychology,Natural language processing,Cognitive psychology,Artificial intelligence,Engineering,Communication,Mechanical engineering,Social psychology,Philosophy,Epistemology",,,"Julia Sidorova,Simon Karlsson,Oliver Rosander,Marcelo L. Berthier,Ignacio Moreno-Torres",
TAFFC2022,emotion,A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion From Heartbeat.,62,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Heart Rate Variability and Autonomic Control","Heartbeat,Artificial intelligence,Computer science,Machine learning,Probabilistic logic,Modalities,Bayesian probability,Deep learning,Bayesian network,Valence (chemistry),Emotional valence,End-to-end principle,Data mining,Social science,Physics,Cognition,Computer security,Quantum mechanics,Neuroscience,Sociology,Biology",,,"Ross Harper,Joshua Southern",
TAFFC2024,emotion,MPEG: A Multi-Perspective Enhanced Graph Attention Network for Causal Emotion Entailment in Conversations.,1,"Sentiment Analysis and Opinion Mining,Topic Modeling,Mental Health via Writing","Utterance,Conversation,Computer science,Perspective (graphical),Natural language processing,Comprehension,Graph,Artificial intelligence,Sentiment analysis,Cognitive psychology,Psychology,Theoretical computer science,Communication,Programming language",,,"Tiantian Chen,Ying Shen,Xuri Chen,Lin Zhang,Shengjie Zhao",
IF2021,affective,Information fusion for affective computing and sentiment analysis.,22,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Computer science,Information fusion,Sentiment analysis,Affective computing,Fusion,Artificial intelligence,Information retrieval,Linguistics,Philosophy",,,"Amir Hussain,Erik Cambria,Soujanya Poria,Ahmad Y. A. Hawalah,Francisco Herrera",
IF2022,emotion,Emotion recognition based on convolutional neural networks and heterogeneous bio-signal data sources.,60,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Modality (human–computer interaction),Computer science,Convolutional neural network,Modalities,Facial expression,Valence (chemistry),Electroencephalography,Artificial intelligence,Arousal,Pattern recognition (psychology),Emotion recognition,Facial recognition system,Speech recognition,Emotion classification,Psychology,Neuroscience,Social science,Physics,Quantum mechanics,Sociology",,,"Wang Kay Ngai,Haoran Xie,Di Zou,Kee-Lee Chou",
IF2021,emotion,DEAR-MULSEMEDIA: Dataset for emotion analysis and recognition in response to multiple sensorial media.,33,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Arousal,Valence (chemistry),Electroencephalography,Speech recognition,Artificial intelligence,Emotion recognition,Psychology,Physics,Quantum mechanics,Neuroscience,Psychiatry",,,"Aasim Raheel,Muhammad Majid,Syed Muhammad Anwar",
IF2021,affect,End-to-end multimodal affect recognition in real-world environments.,79,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Color perception and design","Computer science,Modality (human–computer interaction),Convolutional neural network,Modalities,Artificial intelligence,Audio visual,Multimodal learning,End-to-end principle,Process (computing),Deep learning,Speech recognition,Social science,Multimedia,Sociology,Operating system",,,"Panagiotis Tzirakis,Jiaxin Chen,Stefanos Zafeiriou,Björn W. Schuller",
IF2022,"emotion,emotional",Deep Emotional Arousal Network for Multimodal Sentiment Analysis and Emotion Recognition.,46,"Emotion and Mood Recognition,Multisensory perception and integration,Sentiment Analysis and Opinion Mining","Arousal,Emotion recognition,Sentiment analysis,Computer science,Cognitive psychology,Functional connectivity,Artificial intelligence,Psychology,Neuroscience",,,"Feng Zhang,Xi-Cheng Li,Chee Peng Lim,Qiang Hua,Chun-Ru Dong,Jun-Hai Zhai",
IF2021,emotion,"Multi-label, multi-task CNN approach for context-based emotion recognition.",43,"Emotion and Mood Recognition,Face and Expression Recognition,Advanced Chemical Sensor Technologies","Computer science,Task (project management),Context (archaeology),Categorical variable,Artificial intelligence,Pattern recognition (psychology),Feature extraction,Point (geometry),Function (biology),Task analysis,Machine learning,Engineering,Mathematics,Paleontology,Geometry,Evolutionary biology,Biology,Systems engineering",,,"Ilyes Bendjoudi,Frédéric Vanderhaegen,Denis Hamad,Fadi Dornaika",
TAFFC2023,emotion,Neurofeedback Training With an Electroencephalogram-Based Brain-Computer Interface Enhances Emotion Regulation.,21,"EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies,Neural and Behavioral Psychology Studies","Neurofeedback,Electroencephalography,Brain–computer interface,Psychology,Interface (matter),Sensorimotor rhythm,Emotion recognition,Cognitive psychology,Brain activity and meditation,Computer science,Neuroscience,Parallel computing,Bubble,Maximum bubble pressure method",,,"Weichen Huang,Wei Wu,Molly V. Lucas,Haiyun Huang,Zhenfu Wen,Yuanqing Li",
TAFFC2022,emotion,Multi-Task Semi-Supervised Adversarial Autoencoding for Speech Emotion Recognition.,103,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Discriminative model,Computer science,Artificial intelligence,Machine learning,Task (project management),Autoencoder,Multi-task learning,Categorical variable,Key (lock),Unsupervised learning,Adversarial system,Pipeline (software),Deep learning,Natural language processing,Computer security,Management,Programming language,Economics",,,"Siddique Latif,Rajib Rana,Sara Khalifa,Raja Jurdak,Julien Epps,Björn W. Schuller",
TAFFC2024,affective,Bodily Electrodermal Representations for Affective Computing.,3,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies","Psychology,Skin conductance,Arousal,Valence (chemistry),Psychophysiology,Embodied cognition,Cognitive psychology,Sensation,Emotion classification,Affective computing,Artificial intelligence,Social psychology,Computer science,Medicine,Neuroscience,Biomedical engineering,Physics,Quantum mechanics",,,"Xinyu Shui,Rongzan Lin,Ziyang Luo,Bingxin Lin,Xinxin Mao,Haojie Li,Ran Liu,Dan Zhang",
TAFFC2024,emotion,Improved Video Emotion Recognition With Alignment of CNN and Human Brain Representations.,2,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Human Pose and Action Recognition","Computer science,Convolutional neural network,Similarity (geometry),Artificial intelligence,Representation (politics),Emotion classification,Emotion recognition,Pattern recognition (psychology),Brain activity and meditation,Perception,Speech recognition,Electroencephalography,Image (mathematics),Psychology,Psychiatry,Neuroscience,Politics,Political science,Law",,,"Kaicheng Fu,Changde Du,Shengpei Wang,Huiguang He",
TAFFC2021,affective,Video Affective Content Analysis by Exploring Domain Knowledge.,8,"Video Analysis and Summarization,Emotion and Mood Recognition,Music and Audio Processing","Computer science,Grammar,Leverage (statistics),Domain (mathematical analysis),Natural language processing,Domain knowledge,Probabilistic logic,Content (measure theory),Artificial intelligence,Regression analysis,Multimedia,Machine learning,Linguistics,Mathematics,Philosophy,Mathematical analysis",,,"Shangfei Wang,Can Wang,Tanfang Chen,Yaxin Wang,Yangyang Shu,Qiang Ji",
TAFFC2022,affect,BReG-NeXt: Facial Affect Computing Using Adaptive Residual Networks With Bounded Gradient.,35,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Face and Expression Recognition","Categorical variable,Residual,FLOPS,Computer science,Artificial intelligence,Bounded function,Residual neural network,Function (biology),Mathematics,Machine learning,Pattern recognition (psychology),Algorithm,Parallel computing,Mathematical analysis,Evolutionary biology,Biology",,,"Behzad Hassani,Pooran Singh Negi,Mohammad H. Mahoor",
TAFFC2024,emotional,Managing Emotional Dialogue for a Virtual Cancer Patient: A Schema-Guided Approach.,0,"Speech and dialogue systems,Topic Modeling,Multimodal Machine Learning Applications","Schema (genetic algorithms),Computer science,Empathy,Conversation,Artificial intelligence,Human–computer interaction,Psychology,Information retrieval,Social psychology,Communication",,,"Benjamin Kane,Catherine Giugno,Lenhart K. Schubert,Kurtis Haut,Caleb Wohn,Mohammed E. Hoque",
INTERSPEECH2023,"affects,emotion",Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task,0,"Color perception and design,Aging and Gerontology Research,Technology and Human Factors in Education and Health","Stroop effect,Prosody,Perception,Task (project management),Hearing loss,Psychology,Semantics (computer science),Cognitive psychology,Audiology,Computer science,Speech recognition,Cognition,Medicine,Engineering,Neuroscience,Programming language,Systems engineering",,https://www.isca-archive.org//interspeech_2023/wang23h_interspeech.pdf,"Yingyang Wang, Min Xu, Jing Shao, Lan Wang, Nan Yan","Semantics and prosody are two cues for the perception of spoken emotion. In situations where cues conflict, older adults (OA) have difficulty inhibiting one channel and focusing on the other. OA with hearing loss may face more challenges. In this study, we examined the effects of aging and hearing loss on multi-channel emotion processing through a prosody-semantics Stroop task in three groups of participants, i.e., younger adults (YA) and OA with and without hearing loss. It was found that OA with hearing loss showed the most degraded performance in processing conflicting information. When information was incongruent in two channels, they judged emotions less accurately than the other two groups. Moreover, OA with hearing loss was the only group to show channel dominance, in that they performed lower accuracy in the prosodic channel. These findings suggest that hearing loss affects spoken emotional perception in conflict situations, independent of age-related changes."
INTERSPEECH2022,emotion,Cross-Cultural Comparison of Gradient Emotion Perception: Human vs. Alexa TTS Voices,4,"Emotion and Mood Recognition,Cultural Differences and Values","Perception,Computer science,Speech recognition,Psychology,Neuroscience",,https://www.isca-archive.org//interspeech_2022/gessinger22_interspeech.pdf,"Iona Gessinger, Michelle Cohn, Georgia Zellou, Bernd Möbius","This study compares how American (US) and German (DE) listeners perceive emotional expressiveness from Amazon Alexa text-to-speech (TTS) and human voices. Participants heard identical stimuli, manipulated from an emotionally ‘neutral' production to three levels of increased happiness generated by resynthesis. Results show that, for both groups, ‘happiness' manipulations lead to higher ratings of emotional valence (i.e., more positive) for the human voice. Moreover, there was a difference across the groups in their perception of arousal (i.e., excitement): US listeners show higher ratings for human voices with manipulations, while DE listeners perceive the Alexa voice as sounding less ‘excited' overall. We discuss these findings in terms of theories of cross-cultural emotion perception and human-computer interaction."
TAFFC2023,affect,Audio-Visual Automatic Group Affect Analysis.,23,"Emotion and Mood Recognition,Color perception and design,Human Pose and Action Recognition","Affect (linguistics),Perception,Set (abstract data type),Computer science,Benchmark (surveying),Affective computing,Audio visual,Artificial intelligence,Cognitive psychology,Psychology,Multimedia,Communication,Geodesy,Neuroscience,Programming language,Geography",,,"Garima Sharma,Abhinav Dhall,Jianfei Cai",
TAFFC2022,emotional,A Multimodal Non-Intrusive Stress Monitoring From the Pleasure-Arousal Emotional Dimensions.,21,"Emotion and Mood Recognition,Sleep and Work-Related Fatigue,Infrared Thermography in Medicine","Modality (human–computer interaction),Computer science,Artificial intelligence,Affective computing,Arousal,Machine learning,Valence (chemistry),Operator (biology),Support vector machine,Psychology,Biochemistry,Chemistry,Repressor,Neuroscience,Transcription factor,Gene,Physics,Quantum mechanics",,,"Mohamed Dahmane,Jahangir Alam,Pierre-Luc St-Charles,Marc Lalonde,Kevin Heffner,Samuel Foucher",
TAFFC2024,emotion,AMDET: Attention Based Multiple Dimensions EEG Transformer for Emotion Recognition.,12,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Electroencephalography,Computer science,Artificial intelligence,Pattern recognition (psychology),Speech recognition,Leverage (statistics),Emotion classification,Emotion recognition,Machine learning,Psychology,Psychiatry",,,"Yongling Xu,Yang Du,Ling Li,Honghao Lai,Jing Zou,Tianying Zhou,Lushan Xiao,Li Liu,Pengcheng Ma",
TAFFC2021,emotional,Effects of Computerized Emotional Training on Children with High Functioning Autism.,17,"Autism Spectrum Disorder Research,Genetics and Neurodevelopmental Disorders,Child Development and Digital Technology","Autism,Psychology,High-functioning autism,Facial expression,Asperger syndrome,Emotion recognition,Expression (computer science),Facial expression recognition,Motion (physics),Developmental psychology,Autism spectrum disorder,Cognitive psychology,Computer science,Artificial intelligence,Facial recognition system,Communication,Pattern recognition (psychology),Programming language,Neuroscience",,,"Stefano Piana,Chiara Malagoli,Maria Carmen Usai,Antonio Camurri",
TAFFC2023,emotion,Modeling Multiple Temporal Scales of Full-Body Movements for Emotion Classification.,18,"Human Pose and Action Recognition,Emotion and Mood Recognition,Human Motion and Animation","Computer science,Sadness,Convolutional neural network,Artificial intelligence,Happiness,Duration (music),RGB color model,Motion (physics),Anger,Pattern recognition (psychology),Machine learning,Psychology,Social psychology,Art,Literature,Psychiatry",,,"Cigdem Beyan,Sukumar Karumuri,Gualtiero Volpe,Antonio Camurri,Radoslaw Niewiadomski",
TAFFC2021,emotion,Improving Cross-Corpus Speech Emotion Recognition with Adversarial Discriminative Domain Generalization (ADDoG).,59,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Discriminative model,Computer science,Generalization,Adversarial system,Artificial intelligence,Focus (optics),Context (archaeology),Machine learning,Domain (mathematical analysis),Speech recognition,Baseline (sea),Natural language processing,Pattern recognition (psychology),Mathematics,Mathematical analysis,Paleontology,Oceanography,Physics,Optics,Biology,Geology",,,"John Gideon,Melvin G. McInnis,Emily Mower Provost",
TAFFC2023,emotion,Behavioral and Physiological Signals-Based Deep Multimodal Approach for Mobile Emotion Recognition.,47,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Emotion recognition,Psychology,Computer science,Artificial intelligence,Speech recognition,Cognitive psychology",,,"Kangning Yang,Chaofan Wang,Yue Gu,Zhanna Sarsenbayeva,Benjamin Tag,Tilman Dingler,Greg Wadley,Jorge Gonçalves",
TAFFC2024,emotion,Unsupervised Time-Aware Sampling Network With Deep Reinforcement Learning for EEG-Based Emotion Recognition.,10,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Electroencephalography,Artificial intelligence,Reinforcement learning,Key (lock),Cluster analysis,Sample (material),Unsupervised learning,Feature (linguistics),Pattern recognition (psychology),Machine learning,Psychology,Linguistics,Chemistry,Philosophy,Computer security,Chromatography,Psychiatry",,,"Yongtao Zhang,Yue Pan,Yulin Zhang,Min Zhang,Linling Li,Li Zhang,Gan Huang,Lei Su,Honghai Liu,Zhen Liang,Zhiguo Zhang",
TAFFC2021,emotion,Jointly Aligning and Predicting Continuous Emotion Annotations.,23,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Sinc function,Computer science,Convolutional neural network,Speech recognition,Artificial intelligence,Activation function,Filter (signal processing),Artificial neural network,Pattern recognition (psychology),Computer vision",,,"Soheil Khorram,Melvin G. McInnis,Emily Mower Provost",
TAFFC2023,emotion,Enforcing Semantic Consistency for Cross Corpus Emotion Prediction Using Adversarial Discrepancy Learning in Emotion.,7,"Emotion and Mood Recognition,Anomaly Detection Techniques and Applications,Sentiment Analysis and Opinion Mining","Computer science,Consistency (knowledge bases),Feature (linguistics),Encoder,Artificial intelligence,Semantics (computer science),Artificial neural network,Adversarial system,Representation (politics),Distortion (music),Feature learning,Annotation,Natural language processing,Speech recognition,Pattern recognition (psychology),Machine learning,Amplifier,Computer network,Philosophy,Linguistics,Bandwidth (computing),Politics,Political science,Law,Programming language,Operating system",,,"Chun-Min Chang,Gao-Yi Chao,Chi-Chun Lee",
TAFFC2024,"affective,emotion",Implementing the Affective Mechanism for Group Emotion Recognition With a New Graph Convolutional Network Architecture.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Graph,Group (periodic table),Emotion recognition,Computer science,Convolutional neural network,Psychology,Artificial intelligence,Cognitive psychology,Theoretical computer science,Chemistry,Organic chemistry",,,"Xingzhi Wang,Dong Zhang,Dah-Jye Lee",
TAFFC2022,affective,Modeling Feature Representations for Affective Speech Using Generative Adversarial Networks.,12,"Emotion and Mood Recognition,Generative Adversarial Networks and Image Synthesis,Music and Audio Processing","Generative grammar,Computer science,Classifier (UML),Artificial intelligence,Adversarial system,Feature (linguistics),Generator (circuit theory),Generative adversarial network,Pattern recognition (psychology),Feature vector,Emotion recognition,Class (philosophy),Field (mathematics),Machine learning,Support vector machine,Natural language processing,Deep learning,Mathematics,Linguistics,Power (physics),Physics,Quantum mechanics,Pure mathematics,Philosophy",,,"Saurabh Sahu,Rahul Gupta,Carol Y. Espy-Wilson",
TAFFC2024,emotion,"Threat Perception Captured by Emotion, Motor and Empathetic System Responses: A Systematic Review.",1,"Neural and Behavioral Psychology Studies,Death Anxiety and Social Exclusion,Action Observation and Synchronization","Perception,Psychology,Cognitive psychology,Social psychology,Neuroscience",,,"Elizabeth M. Jacobs,Fani Deligianni,Frank E. Pollick",
TAFFC2024,"emotion,emotional",HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence.,8,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Artificial intelligence,Computer science,Emotion classification,Set (abstract data type),Sentiment analysis,Perception,Cluster analysis,Affective computing,Emotion detection,Natural language processing,Psychology,Cognitive psychology,Emotion recognition,Neuroscience,Programming language",,,"Benjamin Wortman,James Z. Wang",
INTERSPEECH2022,emotional,An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion,6,"Speech Recognition and Synthesis,Speech and Audio Processing","Sequence (biology),Computer science,Speech recognition,Biology,Genetics",,https://www.isca-archive.org//interspeech_2022/yang22t_interspeech.pdf,"Zijiang Yang, Xin Jing, Andreas Triantafyllopoulos, Meishu Song, Ilhan Aslan, Björn W. Schuller","Emotional voice conversion (EVC) focuses on converting a speech utterance from a source to a target emotion; it can thus be a key enabling technology for human-computer interaction applications and beyond. However, EVC remains an unsolved research problem with several challenges. In particular, as speech rate and rhythm are two key factors of emotional conversion, models have to generate output sequences of differing length. Sequence-to-sequence modelling is recently emerging as a competitive paradigm for models that can overcome those challenges. In an attempt to stimulate further research in this promising new direction, recent sequence-to-sequence EVC papers were systematically investigated and reviewed from six perspectives: their motivation, training strategies, model architectures, datasets, model inputs, and evaluation methods. This information is organised to provide the research community with an easily digestible overview of the current state-of-the-art. Finally, we discuss existing challenges of sequence-to-sequence EVC."
INTERSPEECH2022,emotion,Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition,5,"Speech and Audio Processing,Advanced Algorithms and Applications,Speech Recognition and Synthesis","Discriminative model,Adversarial system,Computer science,Speech recognition,Joint (building),Representation (politics),Feature (linguistics),Emotion recognition,Artificial intelligence,Feature learning,Pattern recognition (psychology),Linguistics,Engineering,Architectural engineering,Philosophy,Politics,Law,Political science",,https://www.isca-archive.org//interspeech_2022/liu22aa_interspeech.pdf,"Yang Liu, Haoqin Sun, Wenbo Guan, Yuqi Xia, Zhen Zhao","Accurately recognizing emotion from speech is a necessary yet challenging task due to its complexity. A common problem existing in most of the previous studies is that some of the particular emotions are severely misclassified. In this paper, we propose a novel framework integrating cascaded attention and adversarial joint loss for speech emotion recognition, aiming at discriminating the confusions by emphasizing more on the emotions which are difficult to be correctly classified. Specifically, we propose a cascaded attention network to extract effective emotional features, where spatiotemporal attention selectively locates the targeted emotional regions from the input features. In these targeted regions, the self-attention with head fusion captures the long-distance dependence of temporal features. Furthermore, an adversarial joint loss strategy is proposed to distinguish the emotional embeddings with high similarity by the generated hard triplets in an adversarial fashion. Experimental results on the benchmark dataset IEMOCAP demonstrate that our method gains an absolute improvement of 3.17% and 0.39% over state-of-the-art strategies in terms of weighted accuracy (WA) and unweighted accuracy (UA), respectively."
INTERSPEECH2022,emotion,SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning,3,Emotion and Mood Recognition,"Computer science,Multi-task learning,Speech recognition,Scale (ratio),Emotion recognition,Natural language processing,Artificial intelligence,Machine learning,Task (project management),Physics,Management,Quantum mechanics,Economics",,https://www.isca-archive.org//interspeech_2022/kang22d_interspeech.pdf,"Zuheng Kang, Junqing Peng, Jianzong Wang, Jing Xiao","Speech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%."
INTERSPEECH2022,emotion,Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition,0,Speech and Audio Processing,"Computer science,Modal,Audio visual,Speech recognition,Transformer,Emotion recognition,Human–computer interaction,Artificial intelligence,Multimedia,Engineering,Electrical engineering,Voltage,Chemistry,Polymer chemistry",,https://www.isca-archive.org//interspeech_2022/takashima22b_interspeech.pdf,"Akihiko Takashima, Ryo Masumura, Atsushi Ando, Yoshihiro Yamazaki, Mihiro Uchida, Shota Orihashi","This paper proposes a novel modeling method for audio-visual emotion recognition. Since human emotions are expressed multi-modally, jointly capturing audio and visual cues is a potentially promising approach. In conventional multi-modal modeling methods, a recognition model was trained from an audio-visual paired dataset so as to only enhance audio-visual emotion recognition performance. However, it fails to estimate emotions from single-modal inputs, which indicates they are degraded by overfitting the combinations of the individual modal features. Our supposition is that the ideal form of the emotion recognition is to accurately perform both audio-visual multi-modal processing and single-modal processing with a single model. This is expected to promote utilization of individual modal knowledge for improving audio-visual emotion recognition. Therefore, our proposed method employs a cross-modal transformer model that enables different types of inputs to be handled. In addition, we introduce a novel training method named interactive co-learning; it allows the model to learn knowledge from both and either of the modals. Experiments on a multi-label emotion recognition task demonstrate the effectiveness of the proposed method."
INTERSPEECH2022,emotions,"Complex Paralinguistic Analysis of Speech: Predicting Gender, Emotions and Deception in a Hierarchical Framework",8,Deception detection and forensic psychology,"Paralanguage,Deception,Computer science,Speech recognition,Psychology,Communication,Social psychology",,https://www.isca-archive.org//interspeech_2022/velichko22_interspeech.pdf,"Alena Velichko, Maxim Markitantov, Heysem Kaya, Alexey Karpov","In this paper, we present a hierarchical framework for complex paralinguistic analysis of speech including gender, emotions and deception recognition. The main idea of the framework is built upon the research on interrelation between various paralinguistic phenomena. It uses gender information to predict emotional states, and the outcome of the emotion recognition to predict the truthfulness of the speech. We use multiple datasets (aGender, Ruslana, EmoDB and DSD) to perform within-corpus and cross-corpus experiments using various performance measures. The experimental results reveal that gender-specific models improve the effectiveness of automatic speech emotion recognition in terms of Unweighted Average Recall up to an absolute 5.7%, and the integration of emotion predictions improves the F-score of automatic deception detection compared to our baseline by an absolute 4.7%. The obtained cross-validation results of 88.4 +/- 1.5% for deception detection beat the existing state-of-the-art by an absolute 2.8%."
INTERSPEECH2022,emotion,CTA-RNN: Channel and Temporal-wise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition,6,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Recurrent neural network,Computer science,Speech recognition,Artificial intelligence,Emotion recognition,Artificial neural network",,https://www.isca-archive.org//interspeech_2022/chen22m_interspeech.pdf,"Chengxin Chen, Pengyuan Zhang","Previous research has looked into ways to improve speech emotion recognition (SER) by utilizing both acoustic and linguistic cues of speech. However, the potential association between state-of-the-art ASR models and the SER task has yet to be investigated. In this paper, we propose a novel channel and temporal-wise attention RNN (CTA-RNN) architecture based on the intermediate representations of pre-trained ASR models. Specifically, the embeddings of a large-scale pre-trained end-to-end ASR encoder contain both acoustic and linguistic information, as well as the ability to generalize to different speakers, making them well suited for downstream SER task. To further exploit the embeddings from different layers of the ASR encoder, we propose a novel CTA-RNN architecture to capture the emotional salient parts of embeddings in both the channel and temporal directions. We evaluate our approach on two popular benchmark datasets, IEMOCAP and MSP-IMPROV, using both within-corpus and cross-corpus settings. Experimental results show that our proposed method can achieve excellent performance in terms of accuracy and robustness."
INTERSPEECH2022,emotion,Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,27,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Deep learning,Computer science,Artificial intelligence,Concatenation (mathematics),Field (mathematics),Domain (mathematical analysis),Utterance,Face (sociological concept),Focus (optics),Machine learning,Data science,Mathematical analysis,Social science,Mathematics,Combinatorics,Sociology,Pure mathematics,Physics,Optics",,https://www.isca-archive.org//interspeech_2022/zhao22k_interspeech.pdf,"Zihan Zhao, Yanfeng Wang, Yu Wang","The research and applications of multimodal emotion recognition have become increasingly popular recently. However, multimodal emotion recognition faces the challenge of lack of data. To solve this problem, we propose to use transfer learning which leverages state-of-the-art pre-trained models including wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including coattention-based early fusion and late fusion with the models trained on both embeddings are explored. Also, a multi-granularity framework which extracts not only frame-level speech embeddings but also segment-level embeddings including phone, syllable and word-level speech embeddings is proposed to further boost the performance. By combining our coattention-based early fusion model and late fusion model with the multi-granularity feature extraction framework, we obtain result that outperforms best baseline approaches by 1.3% unweighted accuracy (UA) on the IEMOCAP dataset."
INTERSPEECH2022,emotion,Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition,6,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Emotion recognition,Computer science,Salient,Speech recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2022/hu22e_interspeech.pdf,"Desheng Hu, Xinhui Hu, Xinkang Xu","Emotion-relevant feature extraction is key to the speech emotion recognition (SER) task. Although neural network for extracting features has achieved excellent results, in particular long short-term memory (LSTM) based models, there is still ample space for improvement. In this paper, from the perspective of utilizing advantages of multiple models, we propose an approach of multiple enhancements for learning emotion-salient features in SER, which is based on the combination of LSTM, one-dimensional convolution and transformer networks. Firstly, we introduce residual-BLSTM (Bidirectional LSTM) module to make the network deeper and to increase the learning ability of the model by adding feed-forward network (FFN) to the output of BLSTM and building residual connections at the same time. Secondly, time pooling employed in residual-BLSTM module is proposed to reduce features redundancy and overcome training overfitting. Finally, we propose an E-transformer module by combining transformer and convolution neural network. This approach enables it to learn local information while capturing global dependencies. We conduct evaluations on the IEMOCAP dataset using the proposed methods, and it shows the state-of-the-art performances."
INTERSPEECH2022,emotion,"Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation",6,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Task (project management),Distillation,Speech recognition,Multi-task learning,Natural language processing,Artificial intelligence,Human–computer interaction,Machine learning,Engineering,Chemistry,Systems engineering,Organic chemistry",,https://www.isca-archive.org//interspeech_2022/mitra22_interspeech.pdf,"Vikramjit Mitra, Hsiang-Yun Sherry Chien, Vasudha Kowtha, Joseph Yitan Cheng, Erdrin Azemi","Estimating dimensional emotions, such as activation, valence and dominance, from acoustic speech signals has been widely explored over the past few years. While accurate estimation of activation and dominance from speech seem to be possible, the same for valence remains challenging. Previous research has shown that the use of lexical information can improve valence estimation performance. Lexical information can be obtained from pre-trained acoustic models, where the learned representations can improve valence estimation from speech. We investigate the use of pre-trained model representations to improve valence estimation from acoustic speech signal. We also explore fusion of representations to improve emotion estimation across all three emotion dimensions: activation, valence and dominance. Additionally, we investigate if representations from pre-trained models can be distilled into models trained with low-level features, resulting in models with a less number of parameters. We show that fusion of pre-trained model em- beddings result in a 79% relative improvement in concordance correlation coefficient on valence estimation compared to standard acoustic feature baseline, while distillation from pre-trained model embeddings to lower- dimensional representations yielded a relative 12% improve- ment. Such performance gains were observed over two evaluation sets, indicating that our proposed architecture generalizes across those evaluation sets."
INTERSPEECH2022,emotion,Speech Emotion Recognition via Generation using an Attention-based Variational Recurrent Neural Network,10,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Emotion recognition,Computer science,Speech recognition,Recurrent neural network,Artificial neural network,Time delay neural network,Artificial intelligence",,https://www.isca-archive.org//interspeech_2022/baruah22_interspeech.pdf,"Murchana Baruah, Bonny Banerjee","The last decade has seen an exponential rise in the number of attention-based models for speech emotion recognition (SER). Most of these models use a spectrogram as the input speech representation and the CNN or RNN or convolutional RNN as the key machine learning (ML) component, and learn feature weights to implement attention. We propose an attention-based model for SER that uses MFCC as the input speech representation and a variational RNN (VRNN) as the key ML component. Since the MFCC is of lower dimension than a spectrogram, the model is size- and data-efficient. The VRNN has been used for problems in vision but rarely for SER. Our model is predictive in nature. At each instant, it infers the emotion class and generates the next observation, computes the generation error, and selectively samples (attends to) the locations of high error. Thus, attention emerges in our model, and does not require learning feature weights. This simple model provides interesting insights when evaluated for SER on benchmark datasets. The model can operate on variable length and infinite duration audio files. This work is the first to explore simultaneous generation and recognition for SER, where the generation capability is necessary for efficient recognition."
INTERSPEECH2022,emotion,A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition,9,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Advanced Computing and Algorithms","Computer science,Confusion,Graph,Artificial neural network,Artificial intelligence,Data mining,Graph isomorphism,Pattern recognition (psychology),Theoretical computer science,Machine learning,Algorithm,Psychology,Line graph,Psychoanalysis",,https://www.isca-archive.org//interspeech_2022/hu22c_interspeech.pdf,"Ying Hu, Yuwu Tang, Hao Huang, Liang He","Speech emotion recognition (SER) is an essential part of human-computer interaction. In this paper, we propose an SER network based on a Graph Isomorphism Network with Weighted Multiple Aggregators (WMA-GIN), which can effectively handle the problem of information confusion when neighbour nodes' features are aggregated together in GIN structure. Moreover, a Full-Adjacent (FA) layer is adopted for alleviating the over-squashing problem, which is existed in all Graph Neural Network (GNN) structures, including GIN. Furthermore, a multi-phase attention mechanism and multi-loss training strategy are employed to avoid missing the useful emotional information in the stacked WMA-GIN layers. We evaluated the performance of our proposed WMA-GIN on the popular IEMOCAP dataset. The experimental results show that WMA-GIN outperforms other GNN-based methods and is comparable to some advanced non-graph-based methods by achieving 72.48% of weighted accuracy (WA) and 67.72% of unweighted accuracy (UA)."
INTERSPEECH2022,emotion,Performance Improvement of Speech Emotion Recognition by Neutral Speech Detection Using Autoencoder and Intermediate Representation,4,,"Autoencoder,Speech recognition,Computer science,Representation (politics),Emotion recognition,Voice activity detection,Artificial intelligence,Pattern recognition (psychology),Speech processing,Artificial neural network,Politics,Political science,Law",,https://www.isca-archive.org//interspeech_2022/santoso22_interspeech.pdf,"Jennifer Santoso, Takeshi Yamada, Kenkichi Ishizuka, Taiichi Hashimoto, Shoji Makino","In recent years, classification-based speech emotion recognition (SER) methods have achieved high overall performance. However, these methods tend to have lower performance for neutral speeches, which account for a large proportion in most practical situations. To solve the problem and improve the SER performance, we propose a neutral speech detector (NSD) based on the anomaly detection approach, which uses an autoencoder, the intermediate layer output of a pretrained SER classifier and only neutral data for training. The intermediate layer output of a pretrained SER classifier enables the reconstruction of both acoustic and text features, which are optimized for SER tasks. We then propose the combination of the SER classifier and the NSD used as a screening mechanism for correcting the class probability of the incorrectly recognized neutral speeches. Results of our experiment using the IEMOCAP dataset indicate that the NSD can reconstruct both the acoustic and textual features, achieving a satisfactory performance for use as a reliable screening method. Furthermore, we evaluated the performance of our proposed screening mechanism, and our experiments show significant improvement of 12.9% in the F-score of the neutral class to 80.3%, and 8.4% in the class-average weighted accuracy to 84.5% compared with state-of-the-art SER classifiers."
TAFFC2022,emotion,A Multi-Componential Approach to Emotion Recognition and the Effect of Personality.,33,"Mental Health Research Topics,Emotion and Mood Recognition,Neural dynamics and brain function","Psychology,Cognitive psychology,Component (thermodynamics),Feeling,Personality,Expression (computer science),Social psychology,Empirical research,Big Five personality traits,Computer science,Epistemology,Philosophy,Physics,Thermodynamics,Programming language",,,"Gelareh Mohammadi,Patrik Vuilleumier",
TAFFC2024,emotion,NEMO: A Database for Emotion Analysis Using Functional Near-Infrared Spectroscopy.,2,"EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies,Optical Imaging and Spectroscopy Techniques","Valence (chemistry),Arousal,Psychology,Cognitive psychology,Functional near-infrared spectroscopy,Perception,Artificial intelligence,Computer science,Cognition,Social psychology,Prefrontal cortex,Physics,Quantum mechanics,Neuroscience",,,"Michiel M. A. Spapé,Kalle Mäkelä,Tuukka Ruotsalo",
TAFFC2022,emotion,Unraveling ML Models of Emotion With NOVA: Multi-Level Explainable AI for Non-Experts.,38,"Explainable Artificial Intelligence (XAI),Data Visualization and Analytics,Anomaly Detection Techniques and Applications","Workflow,Computer science,Annotation,Task (project management),Process (computing),Artificial intelligence,Nova (rocket),Machine learning,Human–computer interaction,Management,Aeronautics,Database,Engineering,Economics,Operating system",,,"Alexander Heimerl,Katharina Weitz,Tobias Baur,Elisabeth André",
TAFFC2023,emotional,Automatic Estimation of Action Unit Intensities and Inference of Emotional Appraisals.,5,"Emotion and Mood Recognition,Face and Expression Recognition,Face recognition and analysis","Interpretability,Inference,Novelty,Valence (chemistry),Artificial intelligence,Computer science,Flexibility (engineering),Emotional valence,Action (physics),Face (sociological concept),Pattern recognition (psychology),Machine learning,Psychology,Mathematics,Statistics,Cognition,Social psychology,Physics,Quantum mechanics,Neuroscience,Social science,Sociology",,,"Dominik Seuss,Teena Hassan,Anja Dieckmann,Matthias Unfried,Klaus R. Scherer,Marcello Mortillaro,Jens-Uwe Garbas",
TAFFC2022,affective,Multi-Label and Multimodal Classifier for Affective States Recognition in Virtual Rehabilitation.,11,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Machine Learning in Healthcare","Artificial intelligence,Classifier (UML),Notation,Machine learning,Pattern recognition (psychology),Anxiety,Computer science,Mathematics,Psychology,Arithmetic,Psychiatry",,,"Jesús Joel Rivas,Maria del Carmen Lara,Luis Castrejón,Jorge Hernández-Franco,Felipe Orihuela-Espina,Lorena Palafox,Amanda C. de C. Williams,Nadia Bianchi-Berthouze,Luis Enrique Sucar",
TAFFC2024,"emostim,emotional",EmoStim: A Database of Emotional Film Clips With Discrete and Componential Assessment.,1,"Emotion and Mood Recognition,Color perception and design,Music and Audio Processing","CLIPS,Psychology,Cognitive psychology,Computer science,Artificial intelligence,Database,Human–computer interaction,Natural language processing",,,"Rukshani Somarathna,Patrik Vuilleumier,Gelareh Mohammadi",
TAFFC2023,affect,Werewolf-XL: A Database for Identifying Spontaneous Affect in Large Competitive Group Interactions.,7,"Emotion and Mood Recognition,Music and Audio Processing,Humor Studies and Applications","Utterance,Computer science,Facial expression,Affect (linguistics),Annotation,Affective computing,Arousal,Cognitive psychology,Psychology,Natural language processing,Database,Human–computer interaction,Artificial intelligence,Social psychology,Communication",,,"Kejun Zhang,Xinda Wu,Xinhang Xie,Xiaoran Zhang,Hui Zhang,Xiaoyu Chen,Lingyun Sun",
TAFFC2024,emotional,"Annotate Smarter, not Harder: Using Active Learning to Reduce Emotional Annotation Effort.",0,"Machine Learning and Algorithms,Domain Adaptation and Few-Shot Learning,Advanced Bandit Algorithms Research","Annotation,Computer science,Artificial intelligence,Benchmark (surveying),Machine learning,Baseline (sea),Active learning (machine learning),Set (abstract data type),Transfer of learning,Overhead (engineering),Noise (video),Pattern recognition (psychology),Image (mathematics),Oceanography,Geodesy,Geology,Programming language,Geography,Operating system",,,"Soraia M. Alarcão,Vânia Mendonça,Cláudia Sevivas,Carolina Maruta,Manuel J. Fonseca",
TAFFC2023,emotion,Chunk-Level Speech Emotion Recognition: A General Framework of Sequence-to-One Dynamic Temporal Modeling.,32,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Chunking (psychology),Computer science,Sentence,Speech recognition,Robustness (evolution),Artificial intelligence,Feature engineering,Pattern recognition (psychology),Feature (linguistics),Representation (politics),Natural language processing,Deep learning,Biochemistry,Chemistry,Linguistics,Philosophy,Politics,Political science,Law,Gene",,,"Wei-Cheng Lin,Carlos Busso",
TAFFC2024,emotion,Enhancing EEG-Based Decision-Making Performance Prediction by Maximizing Mutual Information Between Emotion and Decision-Relevant Features.,2,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Computer science,Artificial intelligence,Mutual information,Feature (linguistics),Electroencephalography,Emotion classification,Machine learning,Construct (python library),Pattern recognition (psychology),Correlation,Brain–computer interface,Affective computing,Data mining,Psychology,Mathematics,Philosophy,Linguistics,Geometry,Psychiatry,Programming language",,,"Xinyuan Wang,Danli Wang,Xuange Gao,Yanyan Zhao,Steve C. Chiu",
TAFFC2023,emotions,Data Augmentation via Face Morphing for Recognizing Intensities of Facial Emotions.,7,"Face recognition and analysis,Emotion and Mood Recognition,Face and Expression Recognition","Morphing,Face (sociological concept),Facial expression,Feature (linguistics),Emotion recognition,Artificial intelligence,Computer science,Facial recognition system,Pattern recognition (psychology),Computer vision,Social science,Linguistics,Philosophy,Sociology",,,"Tsung-Ren Huang,Shin-Min Hsu,Li-Chen Fu",
TAFFC2024,"emotion,emotional",Dual Learning for Conversational Emotion Recognition and Emotional Response Generation.,0,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Utterance,Computer science,Dual (grammatical number),Conversation,Task (project management),Benchmark (surveying),Natural language processing,Artificial intelligence,Duality (order theory),Domain (mathematical analysis),Emotion classification,Multi-task learning,Speech recognition,Psychology,Communication,Mathematics,Linguistics,Mathematical analysis,Philosophy,Management,Geodesy,Discrete mathematics,Economics,Geography",,,"Shuhe Zhang,Haifeng Hu,Songlong Xing",
TAFFC2022,emotional,Normative Emotional Agents: A Viewpoint Paper.,8,"Emotions and Moral Behavior,Psychology of Moral and Emotional Judgment,Psychiatry, Mental Health, Neuroscience","Normative,Norm (philosophy),Psychology,Multi-agent system,Interpretation (philosophy),Social psychology,Normative social influence,Socialization,Epistemology,Computer science,Artificial intelligence,Philosophy,Programming language",,,"Estefania Argente,Elena del Val Noguera,D. Pérez-García,Vicent J. Botti",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ACL2023,emotion,Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression,1,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Face and Expression Recognition","Utterance,Computer science,Artificial intelligence,Regression,Bayesian probability,Prior probability,Perception,Voting,Pattern recognition (psychology),Natural language processing,Machine learning,Speech recognition,Mathematics,Statistics,Psychology,Neuroscience,Politics,Political science,Law",,https://aclanthology.org/2023.acl-long.873.pdf,"Wen Wu,Chao Zhang,Philip Woodland","In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground truth, it ignores the intrinsic uncertainty revealed by the inconsistent labels. This paper proposes a Bayesian approach, deep evidential emotion regression (DEER), to estimate the uncertainty in emotion attributes. Treating the emotion attribute labels of an utterance as samples drawn from an unknown Gaussian distribution, DEER places an utterance-specific normal-inverse gamma prior over the Gaussian likelihood and predicts its hyper-parameters using a deep neural network model. It enables a joint estimation of emotion attributes along with the aleatoric and epistemic uncertainties. AER experiments on the widely used MSP-Podcast and IEMOCAP datasets showed DEER produced state-of-the-art results for both the mean values and the distribution of emotion attributes."
ACL2023,emotion,A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations,9,Emotion and Mood Recognition,"Computer science,Utterance,Facial expression,Speech recognition,Task (project management),Artificial intelligence,Modalities,Face (sociological concept),Benchmark (surveying),Pipeline (software),Cluster analysis,Facial recognition system,Focus (optics),Expression (computer science),Pattern recognition (psychology),Social science,Physics,Management,Geodesy,Sociology,Optics,Economics,Programming language,Geography",,https://aclanthology.org/2023.acl-long.861.pdf,"Wenjie Zheng,Jianfei Yu,Rui Xia,Shijin Wang","Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people’s faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expressionaware Multimodal Multi-Task learning (FacialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at "
TAFFC2023,affect,Collecting Mementos: A Multimodal Dataset for Context-Sensitive Modeling of Affect and Memory Processing in Responses to Videos.,2,"Emotion and Mood Recognition,Mental Health Research Topics,Neuroscience and Music Perception","Context (archaeology),Affect (linguistics),Computer science,Crowdsourcing,Artificial intelligence,Natural language processing,Information retrieval,Psychology,World Wide Web,Communication,Paleontology,Biology",,,"Bernd Dudzik,Hayley Hung,Mark A. Neerincx,Joost Broekens",
TAFFC2022,emotion,EEG-Based Emotion Recognition Using Regularized Graph Neural Networks.,471,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,ECG Monitoring and Analysis","Electroencephalography,Adjacency matrix,Computer science,Artificial intelligence,Graph,Pattern recognition (psychology),Artificial neural network,Adjacency list,Psychology,Neuroscience,Algorithm,Theoretical computer science",,,"Peixiang Zhong,Di Wang,Chunyan Miao",
TAFFC2023,emotion,Learning Users Inner Thoughts and Emotion Changes for Social Media Based Suicide Risk Detection.,6,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Mental Health Research Topics","Microblogging,Social media,Emotion detection,Psychology,Computer science,Internet privacy,Social psychology,World Wide Web,Artificial intelligence,Emotion recognition",,,"Lei Cao,Huijun Zhang,Xin Wang,Ling Feng",
TAFFC2024,emotion,Emotion Dictionary Learning With Modality Attentions for Mixed Emotion Exploration.,1,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Emotion classification,Modalities,Task (project management),Computer science,Set (abstract data type),Modality (human–computer interaction),Emotion recognition,Cognitive psychology,Focus (optics),Modal,Artificial intelligence,Speech recognition,Psychology,Social science,Chemistry,Physics,Management,Sociology,Polymer chemistry,Optics,Economics,Programming language",,,"Fang Liu,Pei Yang,Yezhi Shu,Fei Yan,Guanhua Zhang,Yong-Jin Liu",
TAFFC2023,emotion,Improving Textual Emotion Recognition Based on Intra- and Inter-Class Variations.,7,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Discriminative model,Class (philosophy),Computer science,Emotion recognition,Artificial intelligence,Task (project management),Natural language processing,Emotion classification,Entropy (arrow of time),Pattern recognition (psychology),Cross entropy,Affective computing,Machine learning,Speech recognition,Engineering,Physics,Systems engineering,Quantum mechanics",,,"Hassan Alhuzali,Sophia Ananiadou",
TAFFC2023,affective,Learning Enhanced Acoustic Latent Representation for Small Scale Affective Corpus with Adversarial Cross Corpora Integration.,1,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Context (archaeology),Computer science,Artificial intelligence,Representation (politics),Natural language processing,Autoencoder,Adversarial system,Scale (ratio),Transfer of learning,Deep learning,Paleontology,Physics,Quantum mechanics,Politics,Political science,Law,Biology",,,"Chun-Min Chang,Chi-Chun Lee",
TAFFC2024,emotion,Emotion Recognition From Few-Channel EEG Signals by Integrating Deep Feature Aggregation and Transfer Learning.,3,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Electroencephalography,Computer science,Transfer of learning,Channel (broadcasting),Artificial intelligence,Pattern recognition (psychology),Speech recognition,Emotion classification,Feature extraction,Feature (linguistics),Psychology,Neuroscience,Telecommunications,Linguistics,Philosophy",,,"Fang Liu,Pei Yang,Yezhi Shu,Niqi Liu,Jenny Sheng,Junwen Luo,Xiaoan Wang,Yong-Jin Liu",
TAFFC2022,affect,Adapting the Interplay Between Personalized and Generalized Affect Recognition Based on an Unsupervised Neural Framework.,13,"Emotion and Mood Recognition,Mental Health Research Topics,Anomaly Detection Techniques and Applications","Affect (linguistics),Affective computing,Emotion recognition,Artificial intelligence,Computer science,Artificial neural network,Psychology,Machine learning,Pattern recognition (psychology),Cognitive psychology,Communication",,,"Pablo V. A. Barros,Emilia I. Barakova,Stefan Wermter",
TAFFC2024,emotion,Olfactory-Enhanced VR: What's the Difference in Brain Activation Compared to Traditional VR for Emotion Induction?,86,"Anxiety, Depression, Psychometrics, Treatment, Cognitive Processes,Psychosomatic Disorders and Their Treatments,Digital Mental Health Interventions","Panic disorder,Specific phobia,Psychology,Agoraphobia,Psychopathology,Clinical psychology,Autism spectrum disorder,Phobias,Anxiety,Eating disorders,Psychological intervention,Psychiatry,Exposure therapy,Addiction,Autism",,,"Xinyue Zhong,Wanqing Liu,Jialan Xie,Yun Gu,Guangyuan Liu",
TAFFC2022,affect,Deep Temporal Analysis for Non-Acted Body Affect Recognition.,37,"Emotion and Mood Recognition,Video Surveillance and Tracking Methods,Human Pose and Action Recognition","Affect (linguistics),Emotion recognition,Affective computing,Computer science,Psychology,Artificial intelligence,Communication",,,"Danilo Avola,Luigi Cinque,Alessio Fagioli,Gian Luca Foresti,Cristiano Massaroni",
TAFFC2023,emotion,You're Not You When You're Angry: Robust Emotion Features Emerge by Recognizing Speakers.,486,"Speech Recognition and Synthesis,Speech and Audio Processing,Phonetics and Phonology Research","Computer science,Speech recognition,Perplexity,Intelligibility (philosophy),Robustness (evolution),Stress (linguistics),Vocabulary,Natural language processing,Speech technology,Speech processing,Artificial intelligence,Linguistics,Language model,Philosophy,Biochemistry,Chemistry,Epistemology,Gene",,,"Zakaria Aldeneh,Emily Mower Provost",
TAFFC2022,emotions,"""Emotions are the Great Captains of Our Lives"": Measuring Moods Through the Power of Physiological and Environmental Sensing.",2,"Emotion and Mood Recognition,Innovative Human-Technology Interaction,Context-Aware Activity Recognition Systems","Smartwatch,Mood,Computer science,Experience sampling method,Task (project management),Human–computer interaction,Artificial intelligence,Machine learning,Applied psychology,Psychology,Wearable computer,Engineering,Social psychology,Systems engineering,Embedded system",,,"Keith April Araño,Peter A. Gloor,Carlotta Orsenigo,Carlo Vercellis",
TAFFC2024,emotion,Dynamic Confidence-Aware Multi-Modal Emotion Recognition.,5,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Face and Expression Recognition","Computer science,Robustness (evolution),Artificial intelligence,Emotion recognition,Modal,Modalities,Machine learning,Process (computing),Pattern recognition (psychology),Modality (human–computer interaction),Social science,Biochemistry,Chemistry,Sociology,Polymer chemistry,Gene,Operating system",,,"Qi Zhu,Chuhang Zheng,Zheng Zhang,Wei Shao,Daoqiang Zhang",
TAFFC2022,emotional,Leaders and Followers Identified by Emotional Mimicry During Collaborative Learning: A Facial Expression Recognition Study on Emotional Valence.,21,"Innovative Teaching and Learning Methods,Creativity in Education and Neuroscience,Educational Environments and Student Outcomes","Mimicry,Facial expression,Psychology,Valence (chemistry),Emotional expression,Emotion recognition,Cognitive psychology,Social psychology,Nonverbal communication,Developmental psychology,Communication,Ecology,Physics,Quantum mechanics,Neuroscience,Biology",,,"Muhterem Dindar,Sanna Järvelä,Sara Ahola,Xiaohua Huang,Guoying Zhao",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
NAACL2021,emotion,Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter,28,"Sentiment Analysis and Opinion Mining,Hate Speech and Cyberbullying Detection,Humor Studies and Applications","Bhattacharyya distance,Computer science,Modal,Natural language processing,Computational linguistics,Sentiment analysis,Artificial intelligence,Linguistics,Speech recognition,Chemistry,Polymer chemistry,Philosophy",,https://aclanthology.org/2021.naacl-main.456.pdf,"Tulika Saha,Apoorva Upadhyaya,Sriparna Saha,Pushpak Bhattacharyya","Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA (‘TA’ means tweet act, i.e., speech act in Twitter) dataset called "
NAACL2021,emotion,Multimodal End-to-End Sparse Model for Emotion Recognition,56,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Computer science,End-to-end principle,Speech recognition,Artificial intelligence,Computer vision",,https://aclanthology.org/2021.naacl-main.417.pdf,"Wenliang Dai,Samuel Cahyawijaya,Zihan Liu,Pascale Fung","Existing works in multimodal affective computing tasks, such as emotion recognition and personality recognition, generally adopt a two-phase pipeline by first extracting feature representations for each single modality with hand crafted algorithms, and then performing end-to-end learning with extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extracting algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain the performance with around half less computation in the feature extraction part of the model."
MM2022,emotion,Feeling Without Sharing: A Federated Video Emotion Recognition Framework Via Privacy-Agnostic Hybrid Aggregation.,1,"3D Surveying and Cultural Heritage,Conservation Techniques and Studies,Image Processing and 3D Reconstruction","Cultural heritage,Computer science,History,Archaeology",,,"Fan Qi,Zixin Zhang,Xianshan Yang,Huaiwen Zhang,Changsheng Xu",
MM2022,emotion,EASE: Robust Facial Expression Recognition via Emotion Ambiguity-SEnsitive Cooperative Networks.,19,"Emotion and Mood Recognition,Face and Expression Recognition,EEG and Brain-Computer Interfaces","Ambiguity,Computer science,Expression (computer science),Facial expression,Divergence (linguistics),Artificial intelligence,Construct (python library),Pattern recognition (psychology),Programming language,Philosophy,Linguistics",,,"Lijuan Wang,Guoli Jia,Ning Jiang,Haiying Wu,Jufeng Yang",
MM2023,"emotion,emotional",Emotion-Prior Awareness Network for Emotional Video Captioning.,7,"Multimodal Machine Learning Applications,Video Analysis and Summarization,Human Pose and Action Recognition","Closed captioning,Computer science,Task (project management),Sentence,Emotion recognition,Task analysis,Multimedia,Human–computer interaction,Natural language processing,Artificial intelligence,Image (mathematics),Engineering,Systems engineering",,,"Peipei Song,Dan Guo,Xun Yang,Shengeng Tang,Erkun Yang,Meng Wang",
MM2023,affective,Freq-HD: An Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos.,6,"Emotion and Mood Recognition,Speech and Audio Processing,Human Pose and Action Recognition","Expression (computer science),Computer science,Artificial intelligence,Pattern recognition (psychology),Noise (video),Computation,Dynamics (music),Facial expression,Selection (genetic algorithm),Speech recognition,Image (mathematics),Algorithm,Programming language,Physics,Acoustics",,,"Zeng Tao,Yan Wang,Zhaoyu Chen,Boyang Wang,Shaoqi Yan,Kaixun Jiang,Shuyong Gao,Wenqiang Zhang",
MM2023,emotion,StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning.,1,"Multimodal Machine Learning Applications,Image Retrieval and Classification Techniques,Generative Adversarial Networks and Image Synthesis","Computer science,Exploit,Ambiguity,Artificial intelligence,Feature learning,Convolutional neural network,Graph,Benchmark (surveying),Natural language processing,Machine learning,Theoretical computer science,Computer security,Geodesy,Programming language,Geography",,,"Peiguang Jing,Xianyi Liu,Ji Wang,Yinwei Wei,Liqiang Nie,Yuting Su",
MM2023,"affective,affectfal",AffectFAL: Federated Active Affective Computing with Non-IID Data.,3,"Emotion and Mood Recognition,Advanced Steganography and Watermarking Techniques,Face and Expression Recognition","Computer science,Federated learning,Affective computing,Entropy (arrow of time),Annotation,Sampling (signal processing),Experience sampling method,Artificial intelligence,Psychology,Social psychology,Physics,Filter (signal processing),Quantum mechanics,Computer vision",,,"Zixin Zhang,Fan Qi,Shuai Li,Changsheng Xu",
MM2023,emotion,ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition.,13,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,ECG Monitoring and Analysis","Discriminative model,Computer science,Electroencephalography,Artificial intelligence,Pattern recognition (psychology),Complementarity (molecular biology),Subspace topology,Support vector machine,Speech recognition,Machine learning,Psychology,Biology,Genetics,Psychiatry",,,"Peiliang Gong,Ziyu Jia,Pengpai Wang,Yueying Zhou,Daoqiang Zhang",
TAFFC2023,emotional,Quantifying Emotional Similarity in Speech.,6,"Music and Audio Processing,Emotion and Mood Recognition,Speech and Audio Processing","Similarity (geometry),Psychology,Task (project management),Categorical variable,Cognitive psychology,Sample (material),Representation (politics),Embedding,Function (biology),Natural language processing,Computer science,Artificial intelligence,Machine learning,Image (mathematics),Chemistry,Management,Chromatography,Evolutionary biology,Politics,Political science,Law,Economics,Biology",,,"John B. Harvill,Seong-Gyun Leem,Mohammed Abdel-Wahab,Reza Lotfian,Carlos Busso",
TAFFC2022,affective,Affective Video Content Analysis via Multimodal Deep Quality Embedding Network.,7,"Music and Audio Processing,Human Pose and Action Recognition,Video Analysis and Summarization","Computer science,Inference,Artificial intelligence,Stochastic gradient descent,Embedding,Gradient descent,Video quality,Noise (video),Test set,Artificial neural network,Deep learning,Classifier (UML),Mean opinion score,Set (abstract data type),Machine learning,Pattern recognition (psychology),Image (mathematics),Metric (unit),Operations management,Economics,Programming language",,,"Yaochen Zhu,Zhenzhong Chen,Feng Wu",
TAFFC2023,affective,Multimodal Affective States Recognition Based on Multiscale CNNs and Biologically Inspired Decision Fusion Model.,22,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neural and Behavioral Psychology Studies","Artificial intelligence,Modality (human–computer interaction),Pattern recognition (psychology),Computer science,Convolutional neural network,Fusion rules,Activity recognition,Valence (chemistry),Sensor fusion,Machine learning,Speech recognition,Image fusion,Physics,Quantum mechanics,Image (mathematics)",,,"Yuxuan Zhao,Xinyan Cao,Jinlong Lin,Dunshan Yu,Xixin Cao",
TAFFC2022,emotion,Adapted Dynamic Memory Network for Emotion Recognition in Conversation.,55,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Conversation,Computer science,Utterance,Benchmark (surveying),Context (archaeology),Speech recognition,Representation (politics),Artificial intelligence,Convolution (computer science),Dependency (UML),Concatenation (mathematics),Natural language processing,Artificial neural network,Psychology,Communication,Paleontology,Mathematics,Geodesy,Combinatorics,Politics,Political science,Law,Biology,Geography",,,"Songlong Xing,Sijie Mai,Haifeng Hu",
TAFFC2023,emotion,CPED: A Chinese Positive Emotion Database for Emotion Elicitation and Analysis.,8,"Emotion and Mood Recognition,Humor Studies and Applications,Heart Rate Variability and Autonomic Control","CLIPS,Feeling,Psychology,Empathy,Negative emotion,Support vector machine,Kansei,Emotion classification,Database,Artificial intelligence,Cognitive psychology,Computer science,Social psychology",,,"Yulin Zhang,Guozhen Zhao,Yezhi Shu,Yan Ge,Dan Zhang,Yong-Jin Liu,Xianghong Sun",
TAFFC2022,affective,Affective Words and the Company They Keep: Studying the Accuracy of Affective Word Lists in Determining Sentence and Word Valence in a Domain-Specific Corpus.,3,"Sentiment Analysis and Opinion Mining,Humor Studies and Applications,Misinformation and Its Impacts","Valence (chemistry),Sentence,Natural language processing,Computer science,Artificial intelligence,Psychology,Word (group theory),Context (archaeology),Affect (linguistics),Linguistics,Communication,Paleontology,Philosophy,Physics,Quantum mechanics,Biology",,,"Nadine Braun,Martijn Goudbeek,Emiel Krahmer",
TAFFC2024,affection,Interaction Between Dynamic Affection and Arithmetic Cognitive Ability: A Practical Investigation With EEG Measurement.,2,"EEG and Brain-Computer Interfaces,Neural dynamics and brain function,Emotion and Mood Recognition","Cognition,Psychology,Cognitive psychology,Affection,Electroencephalography,Social psychology,Neuroscience",,,"Xiaonan Yang,Yilu Peng,Yuyang Han,Fangyi Li,Qin Zhang,Shuo Wu,Xia Wu",
INTERSPEECH2022,emotion,Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion Recognition,4,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Subspace topology,Computer science,Speech recognition,Discriminant,Linear discriminant analysis,Artificial intelligence,Pattern recognition (psychology),Emotion recognition,Natural language processing",,https://www.isca-archive.org//interspeech_2022/li22_interspeech.pdf,"Shaokai Li, Peng Song, Keke Zhao, Wenjing Zhang, Wenming Zheng","Speech emotion recognition (SER) is a long-standing important research problem in speech signal processing. In practice, the training and test data are often collected in different scenarios, e.g., different languages, different collecting devices, which would severely degrade the recognition performance. To tackle this problem, in this letter, we propose a novel transfer learning algorithm, named coupled discriminant subspace alignment (CDSA), for cross-database SER. In CDSA, we first conduct linear discriminant analysis (LDA) in source and target databases, respectively. Meanwhile, we learn a latent common subspace, where the target samples are represented by the combination of source samples. Furthermore, we align the projection subspace of source and target databases to make the model more robust. Extensive experiments are carried out on four benchmark databases, and the results demonstrate the effectiveness of the proposed method."
INTERSPEECH2023,emotional,Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Intensity (physics),Natural language processing,Artificial intelligence,Physics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2023/oh23_interspeech.pdf,"Yoori Oh, Juheon Lee, Yoseob Han, Kyogu Lee","Recent text-to-speech models have reached the level of generating natural speech similar to what humans say. But there still have limitations in terms of expressiveness. The existing emotional speech synthesis models have shown controllability using interpolated features with scaling parameters in emotional latent space. However, the emotional latent space generated from the existing models is difficult to control the continuous emotional intensity because of the entanglement of features like emotions, speakers, etc. In this paper, we propose a novel method to control the continuous intensity of emotions using semi-supervised learning. The model learns emotions of intermediate intensity using pseudo-labels generated from phoneme-level sequences of speech information. An embedding space built from the proposed model satisfies the uniform grid geometry with an emotional basis. The experimental results showed that the proposed method was superior in controllability and naturalness."
TAFFC2023,emotion,Examining Emotion Perception Agreement in Live Music Performance.,13,"Neuroscience and Music Perception,Music and Audio Processing,Music Technology and Sound Studies","Perception,Psychology,Cognitive psychology,Neuroscience",,,"Simin Yang,Courtney N. Reed,Elaine Chew,Mathieu Barthet",
TAFFC2024,emotion,Gusa: Graph-Based Unsupervised Subdomain Adaptation for Cross-Subject EEG Emotion Recognition.,7,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neonatal and fetal brain pathology","Discriminative model,Electroencephalography,Computer science,Domain adaptation,Graph,Artificial intelligence,Pattern recognition (psychology),Emotion recognition,Domain (mathematical analysis),Adaptation (eye),Representation (politics),Emotion classification,Speech recognition,Theoretical computer science,Psychology,Mathematics,Mathematical analysis,Psychiatry,Neuroscience,Politics,Classifier (UML),Political science,Law",,,"Xiaojun Li,C. L. Philip Chen,Bianna Chen,Tong Zhang",
TAFFC2023,emotions,Improving Humanness of Virtual Agents and Users' Cooperation Through Emotions.,4,"Social Robot Interaction and HRI,Evolutionary Game Theory and Cooperation,Psychology of Moral and Emotional Judgment","Dilemma,Perception,Context (archaeology),Social psychology,Psychology,Appraisal theory,Multi-agent system,Uniqueness,Social dilemma,Virtual actor,Baseline (sea),Computer science,Cognitive psychology,Artificial intelligence,Epistemology,Virtual reality,Political science,Paleontology,Philosophy,Neuroscience,Law,Biology",,,"Moojan Ghafurian,Neil Budnarain,Jesse Hoey",
TAFFC2022,emotions,Conveying Emotions Through Device-Initiated Touch.,40,"Tactile and Sensory Interactions,Multisensory perception and integration,Action Observation and Synchronization","Gesture,Facial expression,Human–computer interaction,Perception,Computer science,Context (archaeology),Modality (human–computer interaction),Valence (chemistry),Affordance,Psychology,Computer vision,Artificial intelligence,Biology,Paleontology,Physics,Quantum mechanics,Neuroscience",,,"Marc Teyssier,Gilles Bailly,Catherine Pelachaud,Eric Lecolinet",
TAFFC2024,affective,"Anthropomorphism and Affective Perception: Dimensions, Measurements, and Interdependencies in Aerial Robotics.",6,"Social Robot Interaction and HRI,AI in Service Interactions,Evolutionary Psychology and Human Behavior","Drone,Perception,Interdependence,Psychology,Artificial intelligence,Human–computer interaction,Robot,Robotics,Social psychology,Cognitive psychology,Computer science,Sociology,Social science,Genetics,Neuroscience,Biology",,,"Viviane Herdel,Anastasia Kuzminykh,Yisrael Parmet,Jessica R. Cauchard",
TAFFC2023,"emonet,emotion",EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition.,37,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Computer science,Adapter (computing),Speech recognition,Residual,Natural language processing,Artificial intelligence,Transfer of learning,Notation,Domain (mathematical analysis),Mathematics,Algorithm,Mathematical analysis,Arithmetic,Operating system",,,"Maurice Gerczuk,Shahin Amiriparian,Sandra Ottl,Björn W. Schuller",
TAFFC2022,emotion,Identifying Cortical Brain Directed Connectivity Networks From High-Density EEG for Emotion Recognition.,32,"EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies,Emotion and Mood Recognition","Electroencephalography,Pattern recognition (psychology),Emotion recognition,Autoregressive model,Computer science,Artificial intelligence,Speech recognition,SIGNAL (programming language),Brain activity and meditation,Psychology,Neuroscience,Mathematics,Econometrics,Programming language",,,"Hailing Wang,Xia Wu,Li Yao",
TAFFC2024,affects,How Virtual Reality Therapy Affects Refugees From Ukraine - Acute Stress Reduction Pilot Study.,1,"Technology and Human Factors in Education and Health,Virtual Reality Applications and Impacts,Infrared Thermography in Medicine","Session (web analytics),Stress reduction,Mood,Virtual reality,Relaxation (psychology),Stress (linguistics),Psychology,Refugee,Applied psychology,Computer science,Clinical psychology,Human–computer interaction,Social psychology,Political science,Linguistics,Philosophy,World Wide Web,Law",,,"Dorota Kaminska,Grzegorz Zwolinski,Dorota Merecz-Kot",
TAFFC2023,emotions,Finding Needles in a Haystack: Recognizing Emotions Just From Your Heart.,4,"EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis,Heart Rate Variability and Autonomic Control","Haystack,Perspective (graphical),Artificial intelligence,Computer science,Set (abstract data type),Representation (politics),Emotionality,Heartbeat,Cognition,Cognitive psychology,Psychology,Pattern recognition (psychology),Social psychology,Neuroscience,Computer security,Politics,Political science,Law,Programming language",,,Wei Li,
TAFFC2022,"affective,emotion",Emotion Dependent Domain Adaptation for Speech Driven Affective Facial Feature Synthesis.,4,"Speech and Audio Processing,Face recognition and analysis,Emotion and Mood Recognition","Disgust,Computer science,Affective computing,Surprise,Facial expression,Speech recognition,Feature (linguistics),Domain (mathematical analysis),Adaptation (eye),Artificial intelligence,Animation,Natural language processing,Cognitive psychology,Psychology,Anger,Communication,Linguistics,Mathematical analysis,Philosophy,Computer graphics (images),Mathematics,Psychiatry,Neuroscience",,,"Rizwan Sadiq,Engin Erzin",
TAFFC2024,affect,Show me How You Use Your Mouse and I Tell You How You Feel? Sensing Affect With the Computer Mouse.,1,"Emotion and Mood Recognition,Color perception and design,Mental Health Research Topics","Affect (linguistics),Longitudinal field,Computer mouse,USable,Task (project management),Computer science,Longitudinal study,Cognitive psychology,Field (mathematics),Tracking (education),Human–computer interaction,Psychology,Artificial intelligence,Communication,Engineering,Multimedia,Statistics,Mathematics,Pedagogy,Physics,Systems engineering,Quantum mechanics,Magnetic field,Pure mathematics",,,"Paul Freihaut,Anja S. Göritz",
TAFFC2024,emotion,CFDA-CSF: A Multi-Modal Domain Adaptation Method for Cross-Subject Emotion Recognition.,4,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,EEG and Brain-Computer Interfaces","Feature (linguistics),Discriminative model,Modal,Feature vector,Artificial intelligence,Computer science,Pattern recognition (psychology),Space (punctuation),Domain (mathematical analysis),Adaptation (eye),Modalities,Modality (human–computer interaction),Session (web analytics),Emotion recognition,Speech recognition,Machine learning,Mathematics,Psychology,Mathematical analysis,Chemistry,Neuroscience,Polymer chemistry,Social science,Philosophy,Linguistics,Sociology,World Wide Web,Operating system",,,"Magdiel Jiménez-Guarneros,Gibran Fuentes Pineda",
TAFFC2023,emotion,AT2GRU: A Human Emotion Recognition Model With Mitigated Device Heterogeneity.,8,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Computer science,Electroencephalography,Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Feature extraction,Speech recognition,Machine learning,Psychology,Psychiatry",,,"Pritam Khan,Priyesh Ranjan,Sudhir Kumar",
TAFFC2022,emotion,An Efficient LSTM Network for Emotion Recognition From Multichannel EEG Signals.,188,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Artificial intelligence,Electroencephalography,Computer science,Pattern recognition (psychology),Emotion recognition,Feature (linguistics),Speech recognition,Representation (politics),Psychology,Neuroscience,Linguistics,Philosophy,Politics,Political science,Law",,,"Xiaobing Du,Cuixia Ma,Guanhua Zhang,Jinyao Li,Yu-Kun Lai,Guozhen Zhao,Xiaoming Deng,Yong-Jin Liu,Hongan Wang",
TAFFC2022,emotion,Self-Supervised ECG Representation Learning for Emotion Recognition.,92,"Emotion and Mood Recognition,ECG Monitoring and Analysis,EEG and Brain-Computer Interfaces","Computer science,Artificial intelligence,Pattern recognition (psychology),Transfer of learning,Convolutional neural network,Emotion classification,Multi-task learning,Supervised learning,Machine learning,Transformation (genetics),Feature learning,Task (project management),Deep learning,Artificial neural network,Speech recognition,Engineering,Biochemistry,Chemistry,Gene,Systems engineering",,,"Pritam Sarkar,Ali Etemad",
TAFFC2024,affective,Modeling the Interplay Between Cohesion Dimensions: A Challenge for Group Affective Emergent States.,0,"Opinion Dynamics and Social Influence,Mental Health Research Topics,Action Observation and Synchronization","Cohesion (chemistry),Psychology,Social psychology,Cognitive psychology,Group cohesiveness,Cognition,Group dynamic,Social group,Computer science,Chemistry,Organic chemistry,Neuroscience",,,"Lucien Maman,Nale Lehmann-Willenbrock,Mohamed Chetouani,Laurence Likforman-Sulem,Giovanna Varni",
TAFFC2023,"emotion,affecting",Exploring the Contextual Factors Affecting Multimodal Emotion Recognition in Videos.,33,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Emotions and Moral Behavior","Sadness,Anger,Happiness,Emotion recognition,Facial expression,Context (archaeology),Emotion classification,Psychology,Cognitive psychology,Key (lock),Emotion perception,Computer science,Speech recognition,Social psychology,Communication,Paleontology,Computer security,Biology",,,"Prasanta Bhattacharya,Raj Kumar Gupta,Yinping Yang",
TAFFC2022,"emotion,emosen",EmoSen: Generating Sentiment and Emotion Controlled Responses in a Multimodal Dialogue System.,36,"Topic Modeling,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Benchmark (surveying),Autoencoder,Conversation,Sentiment analysis,Multimodality,Artificial intelligence,Emotion recognition,Natural language processing,Emotion detection,Task (project management),Machine learning,Deep learning,Psychology,Communication,World Wide Web,Geodesy,Management,Economics,Geography",,,"Mauajama Firdaus,Hardik Chauhan,Asif Ekbal,Pushpak Bhattacharyya",
TAFFC2024,emotion,Learning With Rater-Expanded Label Space to Improve Speech Emotion Recognition.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Perception,Emotion recognition,Consistency (knowledge bases),Artificial intelligence,Natural language processing,Space (punctuation),Affective computing,Speech recognition,Subjectivity,Task (project management),Machine learning,Human–computer interaction,Psychology,Engineering,Operating system,Philosophy,Systems engineering,Epistemology,Neuroscience",,,"Shreya G. Upadhyay,Woan-Shiuan Chien,Bo-Hao Su,Chi-Chun Lee",
TAFFC2024,emotion,A Multi-Level Alignment and Cross-Modal Unified Semantic Graph Refinement Network for Conversational Emotion Recognition.,3,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Multimodal Machine Learning Applications","Computer science,Semantic gap,Artificial intelligence,Graph,Natural language processing,Encoder,Modal,Bridging (networking),Theoretical computer science,Chemistry,Polymer chemistry,Computer network,Image (mathematics),Operating system,Image retrieval",,,"Xiaoheng Zhang,Weigang Cui,Bin Hu,Yang Li",
INTERSPEECH2023,emotion,CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation,0,Emotion and Mood Recognition,"Naturalness,Controllability,Computer science,Representation (politics),Control (management),Intensity (physics),Speech recognition,Artificial intelligence,Natural language processing,Mathematics,Applied mathematics,Physics,Quantum mechanics,Politics,Political science,Law",,https://www.isca-archive.org//interspeech_2023/cui23b_interspeech.pdf,"Yuhao Cui, Xiongwei Wang, Zhongzhou Zhao, Wei Zhou, Haiqing Chen","Existing fine-grained intensity regulation methods rely on explicit control through predicted emotion probabilities. However, these high-level semantic probabilities are often inaccurate and unsmooth at the phoneme level, leading to bias in learning. Especially when we attempt to mix multiple emotion intensities for specific phonemes, resulting in markedly reduced controllability and naturalness of the synthesis. To address this issue, we propose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which leverages accurate disentanglement of emotion manifolds from the reference speech to learn the implicit representation at a lower semantic level. This representation bridges the semantical gap between explicit probabilities and the synthesis model, reducing bias in learning. In experiments, our CASEIN surpasses existing methods in both controllability and naturalness. Notably, we are the first to achieve fine-grained control over the mixed intensity of multiple emotions."
INTERSPEECH2022,emotional,Text aware Emotional Text-to-speech with BERT,3,Speech and dialogue systems,"Computer science,Speech recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2022/mukherjee22_interspeech.pdf,"Arijit Mukherjee, Shubham Bansal, Sandeepkumar Satpal, Rupesh Mehta","Emotional text to speech is the idea of synthesizing emotional audio via a text-to-speech model. With neural text-to-speech, sentence-level naturalness has improved a lot and is almost at par with human speech, but the current approach to emotional text-to-speech models heavily relies on the user to input the expected emotion along with the text to synthesize the desired speech. In this work, we propose a novel text-aware emotional text-to-speech system that leverages a pre-trained BERT model to get a deep representation of the emotional context from the text both during training and inference. We show that our proposed method synthesizes emotional audio with emotion depending on the emotional context of the input text. We also show that our method outperforms baseline systems in varying the emotional intensity depending on the text."
TAFFC2023,emotions,The Mediating Effect of Emotions on Trust in the Context of Automated System Usage.,3,"Human-Automation Interaction and Safety,Occupational Health and Safety Research,Safety Warnings and Signage","Correctness,Context (archaeology),Reliability (semiconductor),Affect (linguistics),Loneliness,Computer science,Psychology,Hostility,Computer security,Social psychology,Human–computer interaction,Internet privacy,Applied psychology,Paleontology,Power (physics),Physics,Communication,Quantum mechanics,Biology,Programming language",,,"Md Abdullah Al Fahim,Mohammad Maifi Hasan Khan,Theodore Jensen,Yusuf Albayram,Emil Coman,Ross Buck",
TAFFC2024,emotion,Emotion Recognition in Conversation Based on a Dynamic Complementary Graph Convolutional Network.,3,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Utterance,Computer science,Conversation,Sentence,Natural language processing,Artificial intelligence,Redundancy (engineering),Interaction information,Convolutional neural network,Graph,Speech recognition,Theoretical computer science,Linguistics,Philosophy,Statistics,Mathematics,Operating system",,,"Zhenyu Yang,Xiaoyang Li,Yuhu Cheng,Tong Zhang,Xuesong Wang",
TAFFC2023,emotional,EEG-Based Emotional Video Classification via Learning Connectivity Structure.,13,"EEG and Brain-Computer Interfaces,Neural dynamics and brain function,Functional Brain Connectivity Studies","Electroencephalography,Computer science,Artificial intelligence,Graph,Pattern recognition (psychology),Perception,Property (philosophy),Consistency (knowledge bases),Feature extraction,Machine learning,Psychology,Theoretical computer science,Philosophy,Epistemology,Psychiatry,Neuroscience",,,"Soobeom Jang,Seong-Eun Moon,Jong-Seok Lee",
TAFFC2023,affect,Discerning Affect From Touch and Gaze During Interaction With a Robot Pet.,5,"Social Robot Interaction and HRI,Gaze Tracking and Assistive Technology,Face Recognition and Perception","Gaze,Affect (linguistics),Robot,Human–computer interaction,Psychology,Human–robot interaction,Computer science,Cognitive psychology,Communication,Computer vision,Artificial intelligence,Cognitive science",,,"Xi Laura Cang,Paul Bucci,Jussi Rantala,Karon E. MacLean",
TAFFC2023,emotion,Few-Shot Learning in Emotion Recognition of Spontaneous Speech Using a Siamese Neural Network With Adaptive Sample Pair Formation.,21,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Metric (unit),Artificial intelligence,Speech recognition,Artificial neural network,Machine learning,Sample (material),Engineering,Operations management,Chemistry,Chromatography",,,"Kexin Feng,Theodora Chaspari",
TAFFC2022,emotional,Enhancement of Movement Intention Detection Using EEG Signals Responsive to Emotional Music Stimulus.,6,"EEG and Brain-Computer Interfaces,Muscle activation and electromyography studies,Neuroscience and Neural Engineering","Electroencephalography,Stimulus (psychology),Brain–computer interface,Psychology,Cognitive psychology,Computer science,Neuroscience",,,"S. M. Shafiul Hasan,Masudur R. Siddiquee,J. Sebastian Marquez,Ou Bai",
TAFFC2024,affective,Multi-Modal Hierarchical Empathetic Framework for Social Robots With Affective Body Control.,1,"Social Robot Interaction and HRI,Emotion and Mood Recognition,Humor Studies and Applications","Robot,Human–robot interaction,Expression (computer science),Artificial intelligence,Computer science,Social robot,Modalities,Human–computer interaction,Mobile robot,Robot control,Social science,Sociology,Programming language",,,"Yue Gao,Yangqing Fu,Ming Sun,Feng Gao",
TAFFC2023,emotion,Survey of Deep Representation Learning for Speech Emotion Recognition.,57,"Music and Audio Processing,Speech and Audio Processing,Speech Recognition and Synthesis","Feature engineering,Representation (politics),Feature learning,Deep learning,Computer science,Artificial intelligence,Pace,Feature (linguistics),Focus (optics),Natural language processing,Machine learning,Linguistics,Philosophy,Physics,Geodesy,Optics,Politics,Political science,Law,Geography",,,"Siddique Latif,Rajib Rana,Sara Khalifa,Raja Jurdak,Junaid Qadir,Björn W. Schuller",
TAFFC2024,emotions,Looking Into Gait for Perceiving Emotions via Bilateral Posture and Movement Graph Convolutional Networks.,6,"Gait Recognition and Analysis,Human Pose and Action Recognition","Gait,Graph,Computer science,Movement (music),Cognitive psychology,Convolutional neural network,Artificial intelligence,Psychology,Computer vision,Physical medicine and rehabilitation,Theoretical computer science,Aesthetics,Medicine,Philosophy",,,"Yingjie Zhai,Guoli Jia,Yu-Kun Lai,Jing Zhang,Jufeng Yang,Dacheng Tao",
TAFFC2023,affective,A Review of Affective Computing Research Based on Function-Component-Representation Framework.,11,"Emotion and Mood Recognition,Mental Health Research Topics,Neural and Behavioral Psychology Studies","Affect (linguistics),Representation (politics),Component (thermodynamics),Categorical variable,Blueprint,Affective computing,Function (biology),Field (mathematics),Computer science,Cognitive psychology,Psychology,Cognitive science,Artificial intelligence,Mathematics,Machine learning,Engineering,Communication,Mechanical engineering,Physics,Evolutionary biology,Politics,Biology,Political science,Pure mathematics,Law,Thermodynamics",,,"Haiwei Ma,Svetlana Yarosh",
TAFFC2024,emotional,Bodily Sensation Map vs. Bodily Motion Map: Visualizing and Analyzing Emotional Body Motions.,1,"Emotion and Mood Recognition,Color perception and design,Aesthetic Perception and Analysis","Sensation,Motion (physics),Psychology,Body posture,Computer vision,Artificial intelligence,Motion capture,Motion sickness,Cognitive psychology,Computer science,Physical medicine and rehabilitation,Medicine,Psychiatry",,,"Myeongul Jung,Youngwug Cho,Jejoong Kim,Hyung Sook Kim,Kwanguk (Kenny) Kim",
TAFFC2023,emotion,Automatic Emotion Recognition in Clinical Scenario: A Systematic Review of Methods.,19,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies","Scopus,Observational study,Systematic review,Inclusion (mineral),Population,Artificial intelligence,Computer science,Psychology,Protocol (science),MEDLINE,Machine learning,Clinical psychology,Medicine,Alternative medicine,Pathology,Social psychology,Political science,Law,Environmental health",,,"Lucia Pepa,Luca Spalazzi,Marianna Capecci,Maria Gabriella Ceravolo",
TAFFC2024,emotion,Cross-Task Inconsistency Based Active Learning (CTIAL) for Emotion Recognition.,0,"Emotion and Mood Recognition,Machine Learning and Algorithms,Sentiment Analysis and Opinion Mining","Task (project management),Computer science,Artificial intelligence,Categorical variable,Multi-task learning,Machine learning,Transfer of learning,Sample (material),Metric (unit),Active learning (machine learning),Natural language processing,Engineering,Systems engineering,Chemistry,Operations management,Chromatography",,,"Yifan Xu,Xue Jiang,Dongrui Wu",
TAFFC2023,affect,A Re-Analysis and Synthesis of Data on Affect Dynamics in Learning.,14,"Intelligent Tutoring Systems and Adaptive Learning,Innovative Teaching and Learning Methods,Online Learning and Analytics","Affect (linguistics),Dynamics (music),Metric (unit),Computer science,Cognitive psychology,Psychology,Data science,Artificial intelligence,Engineering,Pedagogy,Operations management,Communication",,,"Shamya Karumbaiah,Ryan S. Baker,Jaclyn Ocumpaugh,Juliana Ma. Alexandra L. Andres",
TAFFC2022,affective,Affective Dynamics and Cognition During Game-Based Learning.,13,"Educational Games and Gamification,Intelligent Tutoring Systems and Adaptive Learning,Innovative Teaching and Learning Methods","Cognition,Dynamics (music),Psychology,Cognitive psychology,Affective computing,Cognitive science,Computer science,Artificial intelligence,Neuroscience,Pedagogy",,,"Elizabeth B. Cloude,Daryn A. Dever,Debbie L. Hahs-Vaughn,Andrew Emerson,Roger Azevedo,James C. Lester",
TAFFC2024,emotion,Continuous Emotion Ambiguity Prediction: Modeling With Beta Distributions.,0,"Emotion and Mood Recognition,Advanced Text Analysis Techniques,Sentiment Analysis and Opinion Mining","Ambiguity,BETA (programming language),Beta distribution,Psychology,Computer science,Artificial intelligence,Mathematics,Statistics,Programming language",,,"Deboshree Bose,Vidhyasaharan Sethu,Eliathamby Ambikairajah",
TAFFC2024,emotion,Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition.,10,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Emotion recognition,Speech recognition,Computer science,Affective computing,Natural language processing,Artificial intelligence",,,"Weidong Chen,Xiaofen Xing,Peihao Chen,Xiangmin Xu",
TAFFC2023,emotion,The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection.,136,"Topic Modeling,Sentiment Analysis and Opinion Mining,Natural Language Processing Techniques","Computer science,Sentiment analysis,Task (project management),Natural language processing,Word (group theory),Artificial intelligence,Emotion detection,Emotion classification,Language model,Empirical research,Machine learning,Emotion recognition,Linguistics,Philosophy,Management,Epistemology,Economics",,,"Rui Mao,Qian Liu,Kai He,Wei Li,Erik Cambria",
TAFFC2022,emotion,Causal Narrative Comprehension: A New Perspective for Emotion Cause Extraction.,4,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Narrative,Narrative network,Comprehension,Computer science,Cognitive psychology,Narrative structure,Sentence,Causal structure,Semantics (computer science),Perspective (graphical),Causation,Psychology,Natural language processing,Artificial intelligence,Narrative inquiry,Linguistics,Narrative criticism,Epistemology,Philosophy,Physics,Quantum mechanics,Programming language",,,"Wei Cao,Kun Zhang,Shulan Ruan,Hanqing Tao,Sirui Zhao,Hao Wang,Qi Liu,Enhong Chen",
TAFFC2023,emotion,ECPEC: Emotion-Cause Pair Extraction in Conversations.,27,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Utterance,Conversation,Task (project management),Computer science,Sentiment analysis,Natural language processing,Artificial intelligence,Speech recognition,Cognitive psychology,Psychology,Communication,Management,Economics",,,"Wei Li,Yang Li,Vlad Pandelea,Mengshi Ge,Luyao Zhu,Erik Cambria",
TAFFC2024,emotion,GDDN: Graph Domain Disentanglement Network for Generalizable EEG Emotion Recognition.,3,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neonatal and fetal brain pathology","Emotion recognition,Electroencephalography,Computer science,Speech recognition,Psychology,Graph,Artificial intelligence,Cognitive psychology,Pattern recognition (psychology),Theoretical computer science,Neuroscience",,,"Bianna Chen,C. L. Philip Chen,Tong Zhang",
TAFFC2024,emotion,Analyzing Continuous-Time and Sentence-Level Annotations for Speech Emotion Recognition.,1,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Speech recognition,Emotion recognition,Sentence,Natural language processing,Computer science,Artificial intelligence,Psychology",,,"Luz Martinez-Lucas,Wei-Cheng Lin,Carlos Busso",
TAFFC2023,emotion,A Reinforcement Learning Based Two-Stage Model for Emotion Cause Pair Extraction.,7,"Sentiment Analysis and Opinion Mining,Topic Modeling,Spam and Phishing Detection","Computer science,Reinforcement learning,Task (project management),Benchmark (surveying),Artificial intelligence,Pipeline (software),Machine learning,Engineering,Systems engineering,Geodesy,Programming language,Geography",,,"Xinhong Chen,Qing Li,Zongxi Li,Haoran Xie,Fu Lee Wang,Jianping Wang",
TAFFC2022,emotional,Modulation of Driver's Emotional States by Manipulating In-Vehicle Environment: Validation With Biosignals Recorded in An Actual Car Environment.,-1,,,,,"Hodam Kim,Suhye Kim,Hongmin Kim,Youngsoo Ji,Chang-Hwan Im",
TAFFC2023,emotions,Classifying Suicide-Related Content and Emotions on Twitter Using Graph Convolutional Neural Networks.,12,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Suicide and Self-Harm Studies","Artificial intelligence,Computer science,Machine learning,Natural language processing,Context (archaeology),Information retrieval,Psychology,Biology,Paleontology",,,"Annika Marie Schoene,Lana Bojanic,Minh-Quoc Nghiem,Isabelle M. Hunt,Sophia Ananiadou",
TAFFC2022,"affective,emotion",What Lies Beneath - A Survey of Affective Theory Use in Computational Models of Emotion.,1,"Social Robot Interaction and HRI,Emotions and Moral Behavior,Emotion and Mood Recognition","Affective computing,Task (project management),Computer science,Emotion classification,Psychology,Affect (linguistics),Cognitive psychology,Human–computer interaction,Communication,Engineering,Systems engineering",,,"Geneva Smith,Jacques Carette",
TAFFC2024,emotion,A Multi-Stage Visual Perception Approach for Image Emotion Analysis.,2,"Visual Attention and Saliency Detection,Video Surveillance and Tracking Methods,Face and Expression Recognition","Perception,Psychology,Image (mathematics),Stage (stratigraphy),Artificial intelligence,Cognitive psychology,Computer science,Computer vision,Emotion recognition,Paleontology,Neuroscience,Biology",,,"Jicai Pan,Jinqiao Lu,Shangfei Wang",
TAFFC2023,emotion,Sentiment- Emotion- and Context-Guided Knowledge Selection Framework for Emotion Recognition in Conversations.,27,"Sentiment Analysis and Opinion Mining,Topic Modeling,Text and Document Classification Technologies","Utterance,Context (archaeology),Selection (genetic algorithm),Computer science,Representation (politics),Natural language processing,Commonsense knowledge,Artificial intelligence,Knowledge representation and reasoning,Psychology,Paleontology,Politics,Political science,Law,Biology",,,"Geng Tu,Bin Liang,Dazhi Jiang,Ruifeng Xu",
TAFFC2024,emotion,Emotion-Aware Multimodal Fusion for Meme Emotion Detection.,0,Emotion and Mood Recognition,"Emotion detection,Fusion,Computer science,Emotion recognition,Psychology,Cognitive psychology,Human–computer interaction,Natural language processing,Artificial intelligence,Linguistics,Philosophy",,,"Shivam Sharma,Ramaneswaran S.,Md. Shad Akhtar,Tanmoy Chakraborty",
TAFFC2022,emotion,EEG-Video Emotion-Based Summarization: Learning With EEG Auxiliary Signals.,10,"Video Analysis and Summarization,Music and Audio Processing,Multimedia Communication and Technology","Automatic summarization,Computer science,Artificial intelligence,Visualization,Electroencephalography,Modalities,Deep learning,Modality (human–computer interaction),Reinforcement learning,Feature extraction,Synchronization (alternating current),Pattern recognition (psychology),Machine learning,Speech recognition,Psychology,Computer network,Social science,Channel (broadcasting),Psychiatry,Sociology",,,"Wai-Cheong Lincoln Lew,Di Wang,Kai Keng Ang,Joo-Hwee Lim,Chai Quek,Ah-Hwee Tan",
TAFFC2023,emotion,Multimodal Emotion-Cause Pair Extraction in Conversations.,22,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Conversation,Task (project management),Computer science,Construct (python library),Modalities,Emotion classification,Emotion recognition,Natural language processing,Heuristic,Artificial intelligence,Natural (archaeology),Cognitive psychology,Psychology,Communication,Programming language,Social science,Management,Sociology,Economics,Archaeology,History",,,"Fanfan Wang,Zixiang Ding,Rui Xia,Zhaoyu Li,Jianfei Yu",
TAFFC2023,"emotion,emo",AutoML-Emo: Automatic Knowledge Selection Using Congruent Effect for Emotion Identification in Conversations.,15,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Topic Modeling","Computer science,Commonsense knowledge,Transformer,Selection (genetic algorithm),Artificial intelligence,Knowledge base,Identification (biology),Architecture,Machine learning,Cognitive science,Psychology,Engineering,Botany,Voltage,Electrical engineering,Biology,Art,Visual arts",,,"Dazhi Jiang,Runguo Wei,Jintao Wen,Geng Tu,Erik Cambria",
TAFFC2024,emotion,Contrastive Learning Based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition With Missing Modalities.,9,"Face and Expression Recognition,Emotion and Mood Recognition,Text and Document Classification Technologies","Modality (human–computer interaction),Artificial intelligence,Modalities,Emotion recognition,Pattern recognition (psychology),Feature (linguistics),Computer science,Invariant (physics),Multimodal therapy,Psychology,Speech recognition,Mathematics,Psychotherapist,Linguistics,Social science,Philosophy,Sociology,Mathematical physics",,,"Rui Liu,Haolin Zuo,Zheng Lian,Björn W. Schuller,Haizhou Li",
TAFFC2023,emotion,Emotion Recognition for Everyday Life Using Physiological Signals From Wearables: A Systematic Literature Review.,66,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Color perception and design","Wearable computer,Emotion recognition,Everyday life,Wearable technology,Human–computer interaction,Computer science,Artificial intelligence,Machine learning,Political science,Law,Embedded system",,,"Stanislaw Saganowski,Bartosz Perz,Adam G. Polak,Przemyslaw Kazienko",
TAFFC2022,emotion,Towards Contrastive Context-Aware Conversational Emotion Recognition.,8,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Utterance,Computer science,Conversation,Context (archaeology),Natural language processing,Artificial intelligence,Context model,Semantics (computer science),Speech recognition,Semantic interpretation,Psychology,Communication,Object (grammar),Paleontology,Biology,Programming language",,,"Hanqing Zhang,Dawei Song",
TAFFC2022,emotion,Quality-Aware Bag of Modulation Spectrum Features for Robust Speech Emotion Recognition.,13,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","Speech recognition,Emotion recognition,Quality (philosophy),Modulation (music),Computer science,Spectrum (functional analysis),Artificial intelligence,Pattern recognition (psychology),Physics,Acoustics,Quantum mechanics",,,"Shruti Rajendra Kshirsagar,Tiago H. Falk",
TAFFC2024,affective,VAD: A Video Affective Dataset With Danmu.,2,Video Analysis and Summarization,"Computer science,Affective computing,Emotion recognition,Psychology,Artificial intelligence",,,"Shangfei Wang,Xin Li,Feiyi Zheng,Jicai Pan,Xuewei Li,Yanan Chang,Zhouan Zhu,Qiong Li,Jiahe Wang,Yufei Xiao",
INTERSPEECH2022,"emotion,emotional",Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems,6,"Speech Recognition and Synthesis,Music and Audio Processing,Speech and Audio Processing","Computer science,Speech recognition,Speech synthesis,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2022/yoon22b_interspeech.pdf,"Hyun-Wook Yoon, Ohsung Kwon, Hoyeon Lee, Ryuichi Yamamoto, Eunwoo Song, Jae-Min Kim, Min-Jae Hwang","This paper proposes an effective emotional text-to-speech (TTS) system with a pre-trained language model (LM)-based emotion prediction method. Unlike conventional systems that require auxiliary inputs such as manually defined emotion classes, our system directly estimates emotion-related attributes from the input text. Specifically, we utilize generative pre-trained transformer (GPT)-3 to jointly predict both an emotion class and its strength in representing emotions' coarse and fine properties, respectively. Then, these attributes are combined in the emotional embedding space and used as conditional features of the TTS model for generating output speech signals. Consequently, the proposed system can produce emotional speech only from text without any auxiliary inputs. Furthermore, because the GPT-3 enables to capture emotional context among the consecutive sentences, the proposed method can effectively handle the paragraph-level generation of emotional speech."
INTERSPEECH2022,emotion,Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings,2,Speech Recognition and Synthesis,"Speech recognition,Computer science,Emotion recognition,Natural language processing,Artificial intelligence,Speech processing",,https://www.isca-archive.org//interspeech_2022/girish22_interspeech.pdf,"K V Vijay Girish, Srikanth Konjeti, Jithendra Vepa","Speech emotion recognition (SER) is useful in many applications and is approached using signal processing techniques in the past and deep learning techniques recently. Human emotions are complex in nature and can vary widely within an utterance. The SER accuracy has improved using various multi- modal techniques but there is still some gap in understanding the model behaviour and expressing these complex emotions in a human interpretable form. In this work, we propose and define interpretability measures represented as a Human Level Indicator Matrix for an utterance and showcase it's effectiveness in both qualitative and quantitative terms. A word level interpretability is presented using an attention based sequence modelling of self-supervised speech and text pre-trained embeddings. Prosody features are also combined with the proposed model to see the efficacy at the word and utterance level. We provide insights into sub-utterance level emotion predictions for complex utterances where the emotion classes change within the utterance. We evaluate the model and provide the interpretations on the publicly available IEMOCAP dataset."
TAFFC2023,emotion,Self Supervised Adversarial Domain Adaptation for Cross-Corpus and Cross-Language Speech Emotion Recognition.,33,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Discriminative model,Artificial intelligence,Adversarial system,Discriminator,Focus (optics),Natural language processing,Speech recognition,Domain (mathematical analysis),Language model,Key (lock),Machine learning,Telecommunications,Mathematical analysis,Physics,Mathematics,Computer security,Detector,Optics",,,"Siddique Latif,Rajib Rana,Sara Khalifa,Raja Jurdak,Björn W. Schuller",
TAFFC2022,emotional,Semantic-Rich Facial Emotional Expression Recognition.,18,"Emotion and Mood Recognition,Face Recognition and Perception,Face and Expression Recognition","Facial expression,Happiness,Surprise,Disgust,Anger,Emotion classification,Feeling,Psychology,Valence (chemistry),Vocabulary,Expression (computer science),Emotional expression,Expressed emotion,Affective computing,Salience (neuroscience),Computer science,Cognitive psychology,Artificial intelligence,Linguistics,Social psychology,Philosophy,Physics,Quantum mechanics,Psychiatry,Programming language",,,"Keyu Chen,Xu Yang,Changjie Fan,Wei Zhang,Yu Ding",
TAFFC2024,emotion,FBSTCNet: A Spatio-Temporal Convolutional Network Integrating Power and Connectivity Features for EEG-Based Emotion Decoding.,3,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Neural Networks and Applications","Decoding methods,Electroencephalography,Computer science,Convolutional neural network,Power (physics),Emotion recognition,Artificial intelligence,Psychology,Speech recognition,Pattern recognition (psychology),Neuroscience,Telecommunications,Physics,Quantum mechanics",,,"Weichen Huang,Wenlong Wang,Yuanqing Li,Wei Wu",
TAFFC2024,emotion,CFN-ESA: A Cross-Modal Fusion Network With Emotion-Shift Awareness for Dialogue Emotion Recognition.,10,"Emotion and Mood Recognition,Speech and dialogue systems,Context-Aware Activity Recognition Systems","Emotion recognition,Modal,Psychology,Speech recognition,Fusion,Computer science,Linguistics,Chemistry,Philosophy,Polymer chemistry",,,"Jiang Li,Xiaoping Wang,Yingjian Liu,Zhigang Zeng",
TAFFC2022,emotion,An Efficient Framework for Constructing Speech Emotion Corpus Based on Integrated Active Learning Strategies.,2,"Music and Audio Processing,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Artificial intelligence,Process (computing),Class (philosophy),Natural language processing,Emotion recognition,Preference,Sampling (signal processing),Speech recognition,Machine learning,Emotion classification,Mathematics,Statistics,Filter (signal processing),Computer vision,Operating system",,,"Fuji Ren,Zheng Liu,Xin Kang",
TAFFC2024,emotion,Hierarchical Shared Encoder With Task-Specific Transformer Layer Selection for Emotion-Cause Pair Extraction.,0,"Video Analysis and Summarization,Anomaly Detection Techniques and Applications,Time Series Analysis and Forecasting","Encoder,Computer science,Transformer,Selection (genetic algorithm),Task (project management),Artificial intelligence,Psychology,Natural language processing,Engineering,Electrical engineering,Voltage,Systems engineering,Operating system",,,"Xinxin Su,Zhen Huang,Yixin Su,Bayu Distiawan Trisedya,Yong Dou,Yunxiang Zhao",
TAFFC2022,emotion,Joint Feature Adaptation and Graph Adaptive Label Propagation for Cross-Subject Emotion Recognition From EEG Signals.,20,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neonatal and fetal brain pathology","Computer science,Pattern recognition (psychology),Emotion classification,Artificial intelligence,Electroencephalography,Graph,Speech recognition,Machine learning,Theoretical computer science,Psychology,Psychiatry",,,"Yong Peng,Wenjuan Wang,Wanzeng Kong,Feiping Nie,Bao-Liang Lu,Andrzej Cichocki",
TAFFC2022,emotion,Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech.,17,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Valence (chemistry),Computer science,Speech recognition,Weighting,Personalization,Externalization,Artificial intelligence,Keyword spotting,Artificial neural network,Test set,Psychology,Medicine,Physics,Quantum mechanics,World Wide Web,Radiology,Psychoanalysis",,,"Kusha Sridhar,Carlos Busso",
TAFFC2024,emotion,CiABL: Completeness-Induced Adaptative Broad Learning for Cross-Subject Emotion Recognition With EEG and Eye Movement Signals.,4,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition","Electroencephalography,Eye movement,Completeness (order theory),Speech recognition,Emotion recognition,Artificial intelligence,Psychology,Computer science,Pattern recognition (psychology),Cognitive psychology,Neuroscience,Mathematics,Mathematical analysis",,,"Xinrong Gong,C. L. Philip Chen,Bin Hu,Tong Zhang",
TAFFC2023,emotion,Unsupervised Cross-Corpus Speech Emotion Recognition Using a Multi-Source Cycle-GAN.,13,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Artificial intelligence,Emotion recognition,Speech recognition,Set (abstract data type),Valence (chemistry),Annotation,Natural language processing,Physics,Quantum mechanics,Programming language",,,"Bo-Hao Su,Chi-Chun Lee",
TAFFC2024,emotion,Improving Representation With Hierarchical Contrastive Learning for Emotion-Cause Pair Extraction.,0,Sentiment Analysis and Opinion Mining,"Representation (politics),Artificial intelligence,Computer science,Feature extraction,Natural language processing,Contrastive analysis,Psychology,Pattern recognition (psychology),Linguistics,Philosophy,Politics,Political science,Law",,,"Guimin Hu,Yi Zhao,Guangming Lu",
TAFFC2023,emotional,Artificial Emotional Intelligence in Socially Assistive Robots for Older Adults: A Pilot Study.,61,"Social Robot Interaction and HRI,Emotion and Mood Recognition,AI in Service Interactions","Psychology,Facial expression,Emotional intelligence,Conversation,Emotional expression,Mood,Empathy,Social robot,Expression (computer science),Cognitive psychology,Robot,Social psychology,Artificial intelligence,Computer science,Communication,Mobile robot,Robot control,Programming language",,,"Hojjat Abdollahi,Mohammad H. Mahoor,Rohola Zandie,Jarid Siewierski,Sara H. Qualls",
TAFFC2022,emotion,Inconsistency-Based Multi-Task Cooperative Learning for Emotion Recognition.,6,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Face and Expression Recognition","Task (project management),Computer science,Artificial intelligence,Emotion recognition,Machine learning,Multi-task learning,Feature (linguistics),Sample (material),Pattern recognition (psychology),Engineering,Linguistics,Philosophy,Chemistry,Systems engineering,Chromatography",,,"Yifan Xu,Yuqi Cui,Xue Jiang,Yingjie Yin,Jingting Ding,Liang Li,Dongrui Wu",
TAFFC2024,emotion,Spectral-Spatial Attention Alignment for Multi-Source Domain Adaptation in EEG-Based Emotion Recognition.,4,"EEG and Brain-Computer Interfaces,IoT-based Smart Home Systems,Machine Learning and ELM","Electroencephalography,Emotion recognition,Adaptation (eye),Computer science,Domain adaptation,Speech recognition,Pattern recognition (psychology),Artificial intelligence,Domain (mathematical analysis),Psychology,Cognitive psychology,Neuroscience,Mathematics,Mathematical analysis,Classifier (UML)",,,"Yi Yang,Ze Wang,Wei Tao,Xucheng Liu,Ziyu Jia,Boyu Wang,Feng Wan",
INTERSPEECH2024,affect,How Does Alignment Error Affect Automated Pronunciation Scoring in Children's Speech?,143,"Artificial Intelligence in Healthcare and Education,COVID-19 diagnosis using AI,Ethics and Social Impacts of AI","Bridging (networking),Computer science,Internet of Things,Artificial intelligence,The Internet,Robotics,Data science,Engineering ethics,Engineering,Computer security,Robot,World Wide Web",,https://www.isca-archive.org//interspeech_2024/kadambi24_interspeech.pdf,"Prad Kadambi, Tristan Mahr, Lucas Annear, Henry Nomeland, Julie Liss, Katherine Hustad, Visar Berisha","Automated goodness of pronunciation scores measure deviation from typical adult speech by first phonetically segmenting speech using forced alignment and then computing phoneme likelihoods. Care must be taken to distinguish between the impact of alignment error (a spurious signal) and true acoustic deviation on the automated score. Using mixed effects modeling, we predict ∆PLLR, the difference between pronunciation scores computed using manual alignment (PLLRm) versus computed using automatic forced alignments (PLLRa). Pronunciation deviations and alignment error are both magnified in children’s speech and may be influenced by factors such as phoneme position and phoneme type. Our methodology shows that alignment error has a moderate effect on ∆PLLR, and other variables have small to no effect. Manual PLLR closely matches automatically calculated PLLR following cross utterance averaging. Thus, practical comparisons between child speakers should be very comparable across the two methods."
INTERSPEECH2024,affect,Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism,1,"Autism Spectrum Disorder Research,Child Development and Digital Technology","Dialog box,Affect (linguistics),Autism,Task (project management),Computer science,Human–computer interaction,Task analysis,Psychology,Cognitive psychology,Developmental psychology,Engineering,Communication,World Wide Web,Systems engineering",,https://www.isca-archive.org//interspeech_2024/demopoulos24_interspeech.pdf,"Carly Demopoulos, Linnea Lampinen, Cristian Preciado, Hardik Kothare, Vikram Ramanarayanan","Impairments in nonverbal communication are a defining feature of autism spectrum disorder (ASD) and can manifest as difficulty with, or even complete lack of, communication of emotional states via production of facial affect or vocal affect. The purpose of this study was to evaluate psychometric properties of a novel multimodal dialog based Affect Production Task (APT) in children and adolescents (ages 8-17) with a diagnosis of autism (N=72) or neurotypical controls (N=37). Participants completed activities designed to quantify objective facial and vocal affect production ability using audiovisual capture. Criterion, ecological, and discriminant validity were assessed. Psychometric performance across task conditions, age, sex, and race-ethnicity also was examined. Results of this initial psychometric evaluation suggest that the APT is a valid measure of affect production abilities in children and adolescents, and that psychometric performance is invariant to age, sex, or race/ethnicity."
TAFFC2023,emotion,GANSER: A Self-Supervised Data Augmentation Framework for EEG-Based Emotion Recognition.,62,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Computer science,Artificial intelligence,Electroencephalography,Overfitting,Machine learning,Transformation (genetics),Classifier (UML),Pattern recognition (psychology),Deep learning,Artificial neural network,Psychology,Biochemistry,Chemistry,Psychiatry,Gene",,,"Zhi Zhang,Yan Liu,Sheng-hua Zhong",
TAFFC2024,emotion,Bridge Graph Attention Based Graph Convolution Network With Multi-Scale Transformer for EEG Emotion Recognition.,4,EEG and Brain-Computer Interfaces,"Computer science,Transformer,Graph,Electroencephalography,Speech recognition,Theoretical computer science,Artificial intelligence,Pattern recognition (psychology),Psychology,Engineering,Electrical engineering,Voltage,Psychiatry",,,"Huachao Yan,Kailing Guo,Xiaofen Xing,Xiangmin Xu",
TAFFC2024,emotion,A Weighted Co-Training Framework for Emotion Recognition Based on EEG Data Generation Using Frequency-Spatial Diffusion Transformer.,3,EEG and Brain-Computer Interfaces,"Electroencephalography,Emotion recognition,Pattern recognition (psychology),Computer science,Speech recognition,Artificial intelligence,Transformer,Psychology,Engineering,Electrical engineering,Voltage,Psychiatry",,,"Yufan Yi,Yiping Xu,Bo Yang,Yan Tian",
TAFFC2023,emotion,Personal-Zscore: Eliminating Individual Difference for EEG-Based Cross-Subject Emotion Recognition.,24,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Neural dynamics and brain function","Electroencephalography,Computer science,Emotion recognition,Representation (politics),Pattern recognition (psychology),Artificial intelligence,Feature (linguistics),Set (abstract data type),Field (mathematics),Visualization,Significant difference,Speech recognition,Psychology,Mathematics,Statistics,Linguistics,Philosophy,Psychiatry,Politics,Political science,Pure mathematics,Law,Programming language",,,"Huayu Chen,Shuting Sun,Jianxiu Li,Ruilan Yu,Nan Li,Xiaowei Li,Bin Hu",
TAFFC2023,emotion,A Novel Markovian Framework for Integrating Absolute and Relative Ordinal Emotion Information.,1,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Ordinal data,Representation (politics),Range (aeronautics),Ordinal regression,Artificial intelligence,Natural language processing,Machine learning,Materials science,Politics,Political science,Law,Composite material",,,"Jingyao Wu,Ting Dang,Vidhyasaharan Sethu,Eliathamby Ambikairajah",
TAFFC2024,"emotake,emotion",EmoTake: Exploring Drivers' Emotion for Takeover Behavior Prediction.,2,"Energy, Environment, and Transportation Policies,Traffic Prediction and Management Techniques,Human-Automation Interaction and Safety","Psychology,Emotion classification,Cognitive psychology,Social psychology,Computer science",,,"Yu Gu,Yibing Weng,Yantong Wang,Meng Wang,Guohang Zhuang,Jinyang Huang,Xiaolan Peng,Liang Luo,Fuji Ren",
TAFFC2023,emotion,"I Enjoy Writing and Playing, Do You?: A Personalized and Emotion Grounded Dialogue Agent Using Generative Adversarial Network.",10,"Topic Modeling,AI in Service Interactions,Sentiment Analysis and Opinion Mining","Persona,Adversarial system,Computer science,Generative grammar,Chatbot,Metric (unit),Human–computer interaction,Popularity,Empathy,Artificial intelligence,Baseline (sea),Natural language processing,Psychology,Social psychology,Operations management,Oceanography,Economics,Geology",,,"Mauajama Firdaus,Naveen Thangavelu,Asif Ekbal,Pushpak Bhattacharyya",
INTERSPEECH2021,emotional,Emotional Prosody Control for Speech Generation,6,"Speech Recognition and Synthesis,Topic Modeling,Speech and dialogue systems","Prosody,Speech recognition,Computer science,Valence (chemistry),Speech synthesis,Arousal,Natural language processing,Speech corpus,Artificial intelligence,Psychology,Physics,Quantum mechanics,Neuroscience",,https://www.isca-archive.org//interspeech_2021/sivaprasad21_interspeech.pdf,"Sarath Sivaprasad, Saiteja Kosgi, Vineet Gandhi","Machine-generated speech is characterized by its limited or unnatural emotional variation. Current text to speech systems generates speech with either a flat emotion, emotion selected from a predefined set, average variation learned from prosody sequences in training data or transferred from a source style. We propose a text to speech (TTS) system, where a user can choose the emotion of generated speech from a continuous and meaningful emotion space (Arousal-Valence space). The proposed TTS system can generate speech from the text in any speaker’s style, with fine control of emotion. We show that the system works on emotion unseen during training and can scale to previously unseen speakers given his/her speech sample. Our work expands the horizon of the state-of-the-art FastSpeech2 backbone to a multi-speaker setting and gives it much-coveted continuous (and interpretable) affective control, without any observable degradation in the quality of the synthesized speech. Audio samples are publicly available."
INTERSPEECH2021,"emotion,emotional",Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability,28,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Reinforcement learning,Computer science,Reinforcement,Emotion recognition,Rendering (computer graphics),Speech recognition,Artificial intelligence,Psychology,Social psychology",,https://www.isca-archive.org//interspeech_2021/liu21p_interspeech.pdf,"Rui Liu, Berrak Sisman, Haizhou Li","Emotional text-to-speech synthesis (ETTS) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for ETTS, denoted as"
INTERSPEECH2021,emotion,Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition,9,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Speech recognition,Emotion recognition,Graph,Convolutional neural network,Feature learning,Representation (politics),Natural language processing,Artificial intelligence,Theoretical computer science,Politics,Political science,Law",,https://www.isca-archive.org//interspeech_2021/liu21o_interspeech.pdf,"Jiaxing Liu, Yaodong Song, Longbiao Wang, Jianwu Dang, Ruiguo Yu","With the development of speech emotion recognition (SER), dialogue-level SER (DSER) is more aligned with actual scenarios. In this paper, we propose a DSER approach that includes two stages of representation learning: intra-utterance representation learning and inter-utterance representation learning. In the intra-utterance representation learning stage, traditional convolutional neural network (CNN) has demonstrated great success. However, the basic design of a CNN restricts its ability to model the local and global information in the spectrogram. Therefore, we propose a novel local-global representation learning method for the intra-utterance stage. The local information is learned by a time-frequency convolutional neural network (TFCNN), which we published previously. Here, we propose a time-frequency capsule neural network (TFCap) to model global information that can extract more stable global time-frequency information directly from spectrograms. In the inter-utterance stage, a graph convolutional network (GCN) is introduced to explore the relations between utterances in a dialog. Our proposed methods were evaluated on the IEMOCAP database. The proposed time-frequency based method in the intra-utterance stage achieves an absolute increase of 9.35% compared to CNN. By integrating GCN in the inter-utterance stage, the proposed approach achieves an absolute increase of 4.05% compared to the model in the previous stage."
INTERSPEECH2021,emotion,Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition,15,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Speech recognition,Computer science,Correlation,Emotion recognition,Transformer,Artificial intelligence,Natural language processing,Engineering,Mathematics,Electrical engineering,Geometry,Voltage",,https://www.isca-archive.org//interspeech_2021/wang21ga_interspeech.pdf,"Yuhua Wang, Guang Shen, Yuezhu Xu, Jiahang Li, Zhengdao Zhao","Various studies have confirmed the necessity and benefits of leveraging multimodal features for SER, and the latest research results show that the temporal information captured by the transformer is very useful for improving multimodal speech emotion recognition. However, the dependency between different modalities and high-level temporal-feature learning using a deeper transformer is yet to be investigated. Thus, we propose a multimodal transformer with sharing weights for speech emotion recognition. The proposed network shares the weights across the modalities in each transformer layer to learn the correlation among multiple modalities. In addition, since the emotion contained in a speech generally include audio and text features, both of which have not only internal dependence but also mutual dependence, we design a deep multimodal attention mechanism to capture these two kinds of emotional dependence. We evaluated our model on the publicly available IEMOCAP dataset. The experimental results demonstrate that the proposed model yielded a promising result."
INTERSPEECH2021,emotion,Speech Emotion Recognition with Multi-Task Learning,66,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Task (project management),Emotion recognition,Natural language processing,Artificial intelligence,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2021/cai21b_interspeech.pdf,"Xingyu Cai, Jiahong Yuan, Renjie Zheng, Liang Huang, Kenneth Church",Speech emotion recognition (SER) classifies speech into emotion categories such as: 
INTERSPEECH2021,emotion,Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition,5,"Speech and Audio Processing,Emotion and Mood Recognition,Face and Expression Recognition","Computer science,Metric (unit),Speech recognition,Representation (politics),Feature (linguistics),Emotion recognition,Artificial intelligence,Fusion,Feature learning,Pattern recognition (psychology),Natural language processing,Engineering,Linguistics,Operations management,Philosophy,Politics,Law,Political science",,https://www.isca-archive.org//interspeech_2021/gao21e_interspeech.pdf,"Yuan Gao, Jiaxing Liu, Longbiao Wang, Jianwu Dang","Due to the lack of sufficient speech emotional data, the recognition performance of existing speech emotion recognition (SER) approaches is relatively low and requires further improvement to meet the needs of real-life applications. For the problem of data scarcity, an increasingly popular solution is to transfer emotional information through pre-training models and extract additional features. However, the feature representation needs further compression because the training object of unsupervised learning is to reconstruct input, making the latent representation contain non-affective information. In this paper, we introduce deep metric learning to constrain the feature distribution of the pre-training model. Specifically, we propose a triplet loss to modify the representation extraction model as a pseudo-siamese network and achieve more efficient knowledge transfer for emotion recognition. Furthermore, we propose a gated fusion method to learn the connection of features extracted from the pre-training model and supervised feature extraction model. We conduct experiments on the common benchmarking dataset IEMOCAP to verify the performance of the proposed model. The experimental results demonstrate the advantages of our model, outperforming the unsupervised transfer learning system by 3.7% and 3.88% in weighted accuracy and unweighted accuracy, respectively."
INTERSPEECH2021,emotion,Parametric Distributions to Model Numerical Emotion Labels,3,"Music and Audio Processing,Emotion and Mood Recognition,Sensory Analysis and Statistical Methods","Parametric statistics,Computer science,Parametric model,Numerical models,Computer simulation,Mathematics,Statistics,Simulation",,https://www.isca-archive.org//interspeech_2021/bose21_interspeech.pdf,"Deboshree Bose, Vidhyasaharan Sethu, Eliathamby Ambikairajah","It is common to represent emotional states as values on a set of numerical scales corresponding to attributes such as arousal and valence. Often these labels are obtained from multiple annotators who record their perception of emotion in terms of these attributes. Combining these multiple annotations by taking the mean, as is typical in affective computing systems ignores the inherent ambiguity in the labels. Recently it has been recognised that this ambiguity carries useful information and systems that employ distributions over the numerical scales to represent emotional states have been proposed. In this paper we show that the common and widespread assumption that this distribution is Gaussian may not be suitable since the underlying numerical scales are bounded. We then compare a range of well-known distributions defined on bounded domains to ascertain which of them would be the most suitable alternative. Statistical measures are proposed to enable quantifiable comparisons and the results are reported. All comparisons reported in the paper were carried out on the RECOLA dataset."
INTERSPEECH2021,emotion,Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes,2,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Emotion recognition,Speech recognition,Computer science,Audio visual,Identity (music),Psychology,Cognitive psychology,Multimedia,Physics,Acoustics",,https://www.isca-archive.org//interspeech_2021/ito21_interspeech.pdf,"Koichiro Ito, Takuya Fujioka, Qinghua Sun, Kenji Nagamatsu","In this paper, we propose an audio-visual speech emotion recognition (AV-SER) that can suppress the disturbance from an identity attribute by disentangling an emotion attribute and an identity one. We developed a model that first disentangles both attributes for each modality. In order to achieve the disentanglement, we introduce a co-attention module to our model. Our model disentangles the emotion attribute by giving the identity attribute as conditional features to the module. Conversely, the identity attribute is also obtained with the emotion attribute as a condition. Our model then makes a prediction for each attribute from these disentangled features by considering both modalities. In addition, to ensure the disentanglement capacity of our model, we train the model with an identification task as the auxiliary task and an SER task as the primary task alternately, and we update only the part of parameters responsible for each task. The experimental result shows the effectiveness of our method with the wild CMU-MOSEI dataset."
TAFFC2022,emotions,Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network.,163,"Emotion and Mood Recognition,Gaze Tracking and Assistive Technology,Face recognition and analysis","Computer science,Facial expression,Artificial neural network,Artificial intelligence,Pipeline (software),Emotion recognition,Cluster analysis,Facial recognition system,Speech recognition,Face (sociological concept),Frame (networking),Pattern recognition (psychology),Telecommunications,Social science,Sociology,Programming language",,,"Andrey V. Savchenko,Lyudmila V. Savchenko,Ilya Makarov",
TAFFC2024,emotions,EEG Microstates and fNIRS Metrics Reveal the Spatiotemporal Joint Neural Processing Features of Human Emotions.,1,"Functional Brain Connectivity Studies,EEG and Brain-Computer Interfaces,Complex Systems and Time Series Analysis","Electroencephalography,Joint (building),Artificial intelligence,Computer science,Pattern recognition (psychology),Psychology,Cognitive psychology,Neuroscience,Engineering,Architectural engineering",,,"Xiaopeng Si,He Huang,Jiayue Yu,Dong Ming",
TAFFC2022,affect,Dimensional Affect Uncertainty Modelling for Apparent Personality Recognition.,9,"Emotion and Mood Recognition,Face recognition and analysis,Face Recognition and Perception","Artificial intelligence,Context (archaeology),Task (project management),Machine learning,Affect (linguistics),Personality,Computer science,Emotion recognition,Uncertainty quantification,Latent variable,Psychology,Social psychology,Engineering,Paleontology,Systems engineering,Communication,Biology",,,"Mani Kumar Tellamekala,Timo Giesbrecht,Michel F. Valstar",
TAFFC2024,emotional,VyaktitvaNirdharan: Multimodal Assessment of Personality and Trait Emotional Intelligence.,0,Emotional Intelligence and Performance,"Trait,Emotional intelligence,Psychology,Personality,Affective computing,Big Five personality traits,Cognitive psychology,Social psychology,Computer science,Neuroscience,Programming language",,,"Maitree Leekha,Shahid Nawaz Khan,Harshita Srinivas,Rajiv Ratn Shah,Jainendra Shukla",
TAFFC2022,emotion,"Robust Audiovisual Emotion Recognition: Aligning Modalities, Capturing Temporal Information, and Handling Missing Features.",18,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Computer science,Modalities,Robustness (evolution),Artificial intelligence,Speech recognition,Machine learning,Pattern recognition (psychology),Social science,Biochemistry,Chemistry,Sociology,Gene",,,"Lucas Goncalves,Carlos Busso",
TAFFC2024,affect,Affect-Conditioned Image Generation.,0,Generative Adversarial Networks and Image Synthesis,"Affect (linguistics),Image (mathematics),Psychology,Computer vision,Artificial intelligence,Computer science,Cognitive psychology,Social psychology,Communication",,,"Francisco Ibarrola,Rohan Lulham,Kazjon Grace",
TAFFC2022,emotion,PARSE: Pairwise Alignment of Representations in Semi-Supervised EEG Learning for Emotion Recognition.,34,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Pairwise comparison,Artificial intelligence,Computer science,Parsing,Pattern recognition (psychology),Labeled data,Valence (chemistry),Representation (politics),Natural language processing,Electroencephalography,Speech recognition,Machine learning,Psychology,Psychiatry,Politics,Political science,Law,Physics,Quantum mechanics",,,"Guangyi Zhang,Vandad Davoodnia,Ali Etemad",
TAFFC2024,emotion,U-Shaped Distribution Guided Sign Language Emotion Recognition With Semantic and Movement Features.,3,"Hand Gesture Recognition Systems,Gaze Tracking and Assistive Technology,Gait Recognition and Analysis","Sign language,Movement (music),Computer science,Sign (mathematics),Natural language processing,Artificial intelligence,Emotion recognition,Speech recognition,Psychology,Linguistics,Mathematics,Physics,Mathematical analysis,Philosophy,Acoustics",,,"Jiangtao Zhang,Qingshan Wang,Qi Wang",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
IJCV2023,emotions,Going Deeper than Tracking: A Survey of Computer-Vision Based Recognition of Animal Pain and Emotions.,32,"Human-Animal Interaction Studies,Animal Behavior and Welfare Studies,Veterinary Pharmacology and Anesthesia","Field (mathematics),Tracking (education),Computer science,Facial recognition system,Artificial intelligence,Motion (physics),Animal welfare,Human–computer interaction,Computer vision,Psychology,Data science,Pattern recognition (psychology),Ecology,Pedagogy,Mathematics,Pure mathematics,Biology",,,"Sofia Broomé,Marcelo Feighelstein,Anna Zamansky,Gabriel Carreira Lencioni,Pia Haubro Andersen,Francisca Pessanha,Marwa Mahmoud,Hedvig Kjellström,Albert Ali Salah",
IJCV2023,emotional,SMG: A Micro-gesture Dataset Towards Spontaneous Body Gestures for Emotional Stress State Analysis.,21,"Hand Gesture Recognition Systems,Emotion and Mood Recognition,Human Pose and Action Recognition","Gesture,Facial expression,Body language,Nonverbal communication,Feeling,Computer science,Focus (optics),Emotional expression,Gesture recognition,Cognitive psychology,Psychology,Communication,Artificial intelligence,Social psychology,Physics,Optics",,,"Haoyu Chen,Henglin Shi,Xin Liu,Xiaobai Li,Guoying Zhao",
INTERSPEECH2021,emotion,Speech Emotion Recognition via Multi-Level Cross-Modal Distillation,6,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Modal,Computer science,Distillation,Speech recognition,Emotion recognition,Artificial intelligence,Natural language processing,Pattern recognition (psychology),Chemistry,Organic chemistry,Polymer chemistry",,https://www.isca-archive.org//interspeech_2021/li21p_interspeech.pdf,"Ruichen Li, Jinming Zhao, Qin Jin","Speech emotion recognition faces the problem that most of the existing speech corpora are limited in scale and diversity due to the high annotation cost and label ambiguity. In this work, we explore the task of learning robust speech emotion representations based on large unlabeled speech data. Under a simple assumption that the internal emotional states across different modalities are similar, we propose a method called Multi-level Cross-modal Emotion Distillation (MCED), which trains the speech emotion model without any labeled speech emotion data by transferring emotion knowledge from a pretrained text emotion model. Extensive experiments on two benchmark datasets, IEMOCAP and MELD, show that our proposed MCED can help learn effective speech emotion representations which generalize well on downstream speech emotion recognition tasks."
INTERSPEECH2021,emotion,A Speech Emotion Recognition Framework for Better Discrimination of Confusions,13,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Speech recognition,Emotion recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2021/liu21n_interspeech.pdf,"Jiawang Liu, Haoxiang Wang","Speech emotion recognition (SER) plays an important role in human-machine interaction (HMI). Various methods have been proposed for the SER task. However, a common problem in most of the previous studies is some specific emotions are grossly misclassified. In this paper, we propose a novel SER framework aiming at discriminating the confusions by utilizing triplet loss and data augmentation to enforce a CNN-LSTM model to emphasize more on these emotions which are hard to be correctly classified. Ablation experiments demonstrate the effectiveness of the proposed framework. On Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset, our framework can achieve 79.52% of Weighted Accuracy (WA) and 78.30% of Unweighted Accuracy (UA). Compared to the other state-of-the-art models, our framework obtains more than 3.34% and 1.94% improvement on WA and UA respectively."
INTERSPEECH2021,affect,Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features,2,"Emotion and Mood Recognition,Color perception and design,Categorization, perception, and language","Computer science,Artificial intelligence,Pattern recognition (psychology),Resolution (logic),Computer vision,Speech recognition",,https://www.isca-archive.org//interspeech_2021/haider21_interspeech.pdf,"Fasih Haider, Saturnino Luz","An approach to the categorization of voice samples according to emotions expressed by the speaker is proposed which uses Multi-Resolution Cochleagram (MRCG) and scalogram features in a novel way. Audio recordings from the EmoDB, EMOVO and Savee Data-sets are employed in training and testing of predictive models consisting of different sets of speech features. This study systematically evaluates the performance of the feature sets most commonly used in computational paralinguistic tasks (i.e."
INTERSPEECH2023,emotion,Speech Emotion Recognition using Decomposed Speech via Multi-task Learning,0,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Speech recognition,Computer science,Emotion recognition,Task (project management),Voice activity detection,Speech processing,Speech technology,Artificial intelligence,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2023/hsu23_interspeech.pdf,"Jia-Hao Hsu, Chung-Hsien Wu, Yu-Hung Wei","In speech emotion recognition, most recent studies used powerful models to obtain robust features without considering the disentangled components, which contain diverse emotion-rich information helpful for speech emotion recognition. In this study, an autoencoder is used as the speech decomposition model to obtain the disentangled components, including content, timbre, pitch, and rhythm features, which are regarded as emotion-rich features, for speech emotion recognition. The mechanism of multi-task training is then used to train the tasks of speech emotion recognition, speaker recognition, speech recognition, and spectral reconstruction at the same time, while exploiting commonalities and differences across tasks. The model proposed in this study achieved an accuracy of 77.50% on the four-classes emotion recognition task of IEMOCAP. Experiments showed that the proposed methods can effectively improve speech emotion recognition performance, outperforming the SOTA approach."
INTERSPEECH2023,emotion,Hybrid Dataset for Speech Emotion Recognition in Russian Language,3,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Speech recognition,Emotion recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/kondratenko23_interspeech.pdf,"Vladimir Kondratenko, Nikolay Karpov, Artem Sokolov, Nikita Savushkin, Oleg Kutuzov, Fyodor Minkin","We present a new data set for speech emotion recognition (SER) tasks called Dusha. The corpus contains approximately 350 hours of data, more than 300 000 audio recordings of Russian speech, and their transcripts. Therefore it is the biggest open bi-modal data collection with an open license for SER tasks nowadays. This data set is the first speech emotion corpus in Russian, including both crowd-sourced acted and real-life emotions from podcasts, with multiple speakers and scalable data set size. Acted subset has a more balanced class distribution than the unbalanced real-life part consisting of audio podcasts. So the first one is suitable for model pre-training, and the second is elaborated for fine-tuning purposes, model approbation, and validation. This paper describes in detail our collecting procedure, pre-processing routine, annotation, and experiment with a baseline model to demonstrate some actual metrics which could be obtained with the Dusha data set."
TAFFC2023,affect,Audio-Visual Gated-Sequenced Neural Networks for Affect Recognition.,5,"Emotion and Mood Recognition,Human Pose and Action Recognition,Music and Audio Processing","Computer science,Artificial intelligence,Artificial neural network,Affective computing,Robustness (evolution),Machine learning,Modalities,Speech recognition,Pattern recognition (psychology),Social science,Biochemistry,Chemistry,Sociology,Gene",,,"Decky Aspandi,Federico Sukno,Björn W. Schuller,Xavier Binefa",
TAFFC2022,emotion,A Dual-Branch Dynamic Graph Convolution Based Adaptive TransFormer Feature Fusion Network for EEG Emotion Recognition.,56,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Artificial intelligence,Pattern recognition (psychology),Graph,Feature extraction,Transfer of learning,Adapter (computing),Electroencephalography,Speech recognition,Theoretical computer science,Psychology,Psychiatry,Operating system",,,"Mingyi Sun,Wei-Gang Cui,Shuyue Yu,Hongbin Han,Bin Hu,Yang Li",
TAFFC2024,affective,A Wide Evaluation of ChatGPT on Affective Computing Tasks.,14,"Advanced Bandit Algorithms Research,Digital Mental Health Interventions","Affective computing,Computer science,Human–computer interaction,Psychology,Cognitive psychology,Artificial intelligence",,,"Mostafa M. Amin,Rui Mao,Erik Cambria,Björn W. Schuller",
TAFFC2022,emotion,ER-Chat: A Text-to-Text Open-Domain Dialogue Framework for Emotion Regulation.,12,"Topic Modeling,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Mirroring,Context (archaeology),Emotion classification,Psychology,Fluency,Cognitive psychology,Computer science,Social psychology,Paleontology,Mathematics education,Biology",,,"Shin Katayama,Shunsuke Aoki,Takuro Yonezawa,Tadashi Okoshi,Jin Nakazawa,Nobuo Kawaguchi",
TAFFC2023,emotion,TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition.,137,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Discriminative model,Electroencephalography,Computer science,Pattern recognition (psychology),Artificial intelligence,Convolutional neural network,Speech recognition,Psychology,Psychiatry",,,"Yi Ding,Neethu Robinson,Su Zhang,Qiuhao Zeng,Cuntai Guan",
TAFFC2022,affective,Modeling Real-World Affective and Communicative Nonverbal Vocalizations From Minimally Speaking Individuals.,2,"Infant Health and Development,Speech and dialogue systems,Digital Communication and Language","Nonverbal communication,Psychology,Context (archaeology),Cognitive psychology,Communication,Biology,Paleontology",,,"Jaya Narain,Kristina T. Johnson,Thomas F. Quatieri,Rosalind W. Picard,Pattie Maes",
TAFFC2023,emotions,"Dynamic State-Space Modeling With Factorial Memories in Temporal Dominance of Sensations, Emotions and Temporal Liking.",4,"Sensory Analysis and Statistical Methods,Color perception and design,Biochemical Analysis and Sensing Techniques","Perception,Cognitive psychology,Dominance (genetics),Psychology,Factorial,State space,Dynamics (music),Artificial intelligence,Mathematics,Computer science,Statistics,Biochemistry,Chemistry,Neuroscience,Gene,Mathematical analysis,Pedagogy",,,"Kyoichi Tachi,Shogo Okamoto",
TAFFC2023,affective,Modelling Stochastic Context of Audio-Visual Expressive Behaviour With Affective Processes.,4,"Music and Audio Processing,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Context (archaeology),Variety (cybernetics),Feature (linguistics),Abstraction,Perception,Affective computing,Artificial intelligence,Domain (mathematical analysis),Probabilistic logic,Affect (linguistics),Machine learning,Human–computer interaction,Psychology,Communication,Paleontology,Mathematical analysis,Linguistics,Philosophy,Mathematics,Epistemology,Neuroscience,Biology",,,"Mani Kumar Tellamekala,Timo Giesbrecht,Michel F. Valstar",
INTERSPEECH2023,emotion,Dual Memory Fusion for Multimodal Speech Emotion Recognition,5,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Dual (grammatical number),Speech recognition,Fusion,Emotion recognition,Artificial intelligence,Natural language processing,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2023/prisayad23_interspeech.pdf,"Darshana Prisayad, Tharindu Fernando, Sridha Sridharan, Simon Denman, Clinton Fookes","Deep learning has been widely used in multi-modal Speech Emotion Recognition (SER) to learn sentiment-related features by aggregating representations from multiple modes. However, most SOTA methods use attentive fusion or late fusion of data which ignores the possibility of long-term dependencies among data. In this study, we propose a transformer-based SER architecture that fuses modality representations through explicit memory modules, where the information from current inputs is integrated with historical information allowing the model to understand the relative importance of modes over time. We have used Wav2Vec2 and BERT models to extract audio and text features which are then fused together by aggregating features from individual modes with information stored in memory, followed by downstream classification. Following state-of-the-art methods, we evaluate our proposed method on the IEMOCAP dataset and results indicate that memory-based fusion can achieve substantial improvements."
INTERSPEECH2023,emotion,Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition,1,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Transfer of learning,Speech recognition,Emotion recognition,Domain (mathematical analysis),Unsupervised learning,Artificial intelligence,Natural language processing,Mathematical analysis,Mathematics",,https://www.isca-archive.org//interspeech_2023/jiang23_interspeech.pdf,"Shenjie Jiang, Peng Song, Shaokai Li, Keke Zhao, Wenming Zheng","Cross-domain speech emotion recognition (SER), which utilizes the source domain to recognize the emotions in the target domain, has received significant attention in recent years. In this paper, we propose a novel unsupervised transfer learning method named unsupervised transfer components learning (UTCL) for cross-domain SER. Specifically, we first learn a common projection for the cross-domain data, in which a PCA-like strategy is conducted for the source and target domains separately. Meanwhile, we design a simple strategy to ensure all cross-domain samples share similar manifold structures so that the learned common projection can preserve more transfer components. Furthermore, a novel adaptive structured graph strategy is designed to further narrow the gap between the cross-domain samples.  Comprehensive experimental results on several benchmark datasets demonstrate that our method can achieve better performance in comparison with several state-of-the-art methods."
NAACL2022,emotion,CoMPM: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation,21,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Conversation,Dual (grammatical number),Context (archaeology),Graph,Natural language processing,Emotion recognition,Artificial intelligence,Human–computer interaction,Theoretical computer science,Linguistics,Paleontology,Philosophy,Biology",,https://aclanthology.org/2022.naacl-main.416.pdf,"Joosung Lee,Wooin Lee","As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker’s pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods. Our code is available on github ."
NAACL2021,emotionally,Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures,2,"Sentiment Analysis and Opinion Mining,Topic Modeling,Mental Health via Writing","Sexual abuse,Psychology,Computer science,Association (psychology),Natural language processing,Poison control,Human factors and ergonomics,Psychotherapist,Medicine,Environmental health",,https://aclanthology.org/2021.naacl-main.387.pdf,"Ramit Sawhney,Puneet Mathur,Taru Jain,Akash Kumar Gautam,Rajiv Ratn Shah","The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better."
INTERSPEECH2023,"emotion,emotional",Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute,0,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Speech recognition,Emotion recognition,Class (philosophy),Computer science,Natural language processing,Artificial intelligence,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2023/nagase23_interspeech.pdf,"Ryotaro Nagase, Takahiro Fukumori, Yoichi Yamashita","In recent years, much research has been into speech emotion recognition (SER) using deep learning to predict emotions conveyed by speech. We studied the method that detected the emotion for the whole utterance using the frame-based SER, which estimates emotions in each frame rather than in a whole utterance. One of the problems with this method is that the emotional label sequence, which is used in training the frame-based SER, does not sufficiently consider phonemic characteristics. To solve this problem, we propose new methods of recognizing the emotion for the whole utterance using frame-based SER that considers the phoneme class attribute such as vowels, voiced consonants, unvoiced consonants, and other symbols in training. As a result, we found that the proposed methods significantly improve the performance of the result for the whole utterance compared to conventional methods."
INTERSPEECH2023,emotion,Improving Joint Speech and Emotion Recognition Using Global Style Tokens,0,Speech Recognition and Synthesis,"Computer science,Joint (building),Speech recognition,Style (visual arts),Emotion recognition,Natural language processing,Artificial intelligence,Engineering,Architectural engineering,Archaeology,History",,https://www.isca-archive.org//interspeech_2023/kyung23_interspeech.pdf,"Jehyun Kyung, Ju-Seok Seong, Jeong-Hwan Choi, Ye-Rin Jeoung, Joon-Hyuk Chang","Automatic speech recognition (ASR) and speech emotion recognition (SER) are closely related in that the acoustic features of speech, such as pitch, tone, and intensity, can vary according to the speaker's emotional state. Our study focuses on a joint ASR and SER task, in which an emotion token is tagged and recognized along with the text. To further improve the joint recognition performance, we propose a novel training method that adopts the global style tokens (GSTs). The style embedding is extracted from the GSTs module to enhance the joint ASR and SER model to capture emotional information from speech. Specifically, a conformer-based joint ASR and SER model pre-trained on a large-scale dataset is jointly fine-tuned with style embedding to improve both ASR and SER. The experimental results on the IEMOCAP dataset showed that the proposed model achieves a word error rate of 15.8% and four emotion classification weighted and unweighted accuracy of 75.1% and 76.3%, respectively."
TAFFC2023,emotion,Weakly-Supervised Learning for Fine-Grained Emotion Recognition Using Physiological Signals.,16,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Artificial intelligence,Computer science,Emotion recognition,Stimulus (psychology),Valence (chemistry),Speech recognition,Pattern recognition (psychology),Natural language processing,Psychology,Cognitive psychology,Physics,Quantum mechanics",,,"Tianyi Zhang,Abdallah El Ali,Chen Wang,Alan Hanjalic,Pablo César",
TAFFC2023,emotion,An Adversarial Training Based Speech Emotion Classifier With Isolated Gaussian Regularization.,6,"Speech Recognition and Synthesis,Music and Audio Processing,Emotion and Mood Recognition","Discriminative model,Computer science,Mixture model,Classifier (UML),Regularization (linguistics),Artificial intelligence,Gaussian,Speech recognition,Centroid,Adversarial system,Gaussian process,Pattern recognition (psychology),Natural language processing,Physics,Quantum mechanics",,,"Changzeng Fu,Chaoran Liu,Carlos Toshinori Ishi,Hiroshi Ishiguro",
TAFFC2023,emotion,SMIN: Semi-Supervised Multi-Modal Interaction Network for Conversational Emotion Recognition.,44,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Computer science,Artificial intelligence,Modal,Machine learning,Ambiguity,Leverage (statistics),Emotion recognition,Benchmark (surveying),Automatic summarization,Supervised learning,Natural language processing,Artificial neural network,Chemistry,Geodesy,Polymer chemistry,Programming language,Geography",,,"Zheng Lian,Bin Liu,Jianhua Tao",
TAFFC2023,affect,Challenges in Evaluating Technological Interventions for Affect Regulation.,2,"Color perception and design,Emotion and Mood Recognition,Mental Health Research Topics","Affect (linguistics),Breathing,Psychological intervention,Skin conductance,Anxiety,Stressor,Psychology,Cognition,Medicine,Neuroscience,Psychiatry,Communication,Biomedical engineering",,,"Pardis Miri,Horia Margarit,Andero Uusberg,Keith Marzullo,Tali M. Ball,Daniel Yamins,Robert Flory,James J. Gross",
TAFFC2023,emotion,EEG-Based Emotion Recognition With Emotion Localization via Hierarchical Self-Attention.,42,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Face and Expression Recognition","Emotion recognition,Electroencephalography,Psychology,Cognitive psychology,Emotion classification,Computer science,Affective computing,Artificial intelligence,Speech recognition,Neuroscience",,,"Yuzhe Zhang,Huan Liu,Dalin Zhang,Xuxu Chen,Tao Qin,Qinghua Zheng",
TAFFC2023,emotion,Emotion Distribution Learning Based on Peripheral Physiological Signals.,6,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Sadness,Computer science,Emotion classification,Artificial intelligence,Feature (linguistics),Feature selection,Task (project management),Benchmark (surveying),Disgust,Machine learning,Anger,Convolutional neural network,Pattern recognition (psychology),Psychology,Engineering,Linguistics,Philosophy,Systems engineering,Geodesy,Psychiatry,Geography",,,"Yezhi Shu,Pei Yang,Niqi Liu,Shu Zhang,Guozhen Zhao,Yong-Jin Liu",
TAFFC2023,emotion,Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition.,113,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Electroencephalography,Subject (documents),Emotion classification,Emotion recognition,Artificial intelligence,Convolutional neural network,Computer science,Pattern recognition (psychology),Speech recognition,Psychology,Cognitive psychology,Neuroscience,Library science",,,"Xinke Shen,Xianggen Liu,Xin Hu,Dan Zhang,Sen Song",
INTERSPEECH2023,emotion,ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models,3,"Speech Recognition and Synthesis,Speech and dialogue systems,Topic Modeling","Zero (linguistics),Shot (pellet),Computer science,Speech recognition,Diffusion,Style (visual arts),Speech synthesis,Artificial intelligence,Physics,Linguistics,Materials science,Art,Philosophy,Literature,Metallurgy,Thermodynamics",https://ZET-Speech.github.io/ZET-Speech-Demo/.,https://www.isca-archive.org//interspeech_2023/kang23_interspeech.pdf,"Minki Kang, Wooseok Han, Sung Ju Hwang, Eunho Yang","Emotional Text-To-Speech (TTS) is an important task in the development of systems (e.g., human-like dialogue agents) that require natural and emotional speech. Existing approaches, however, only aim to produce emotional TTS for seen speakers during training, without consideration of the generalization to unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive emotion-controllable TTS model that allows users to synthesize any speaker's emotional speech using only a short, neutral speech segment and the target emotion label. Specifically, to enable a zero-shot adaptive TTS model to synthesize emotional speech, we propose domain adversarial learning and guidance methods on the diffusion model. Experimental results demonstrate that ZET-Speech successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers. Samples are at https://ZET-Speech.github.io/ZET-Speech-Demo/."
INTERSPEECH2023,"emotion,emotional",Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech,0,Emotion and Mood Recognition,"Prosody,Electroencephalography,Emotional prosody,Speech recognition,Computer science,Psychology,Audiology,Cognitive psychology,Neuroscience,Medicine",,https://www.isca-archive.org//interspeech_2023/zhang23f_interspeech.pdf,"Zechen Zhang, Xihong Wu, Jing Chen","Emotion classification with EEG responses can be used in human-computer interaction, security, medical treatment, etc. Neural responses recorded via EEG can reflect more direct and objective emotional information than other behavioral signals (i.e., facial expression...). In most previous studies, only features of EEG were used as input for machine learning models. In this work, we assumed that the emotional features included in speech stimuli could assist in emotion recognition with EEG when the emotion is evoked by the emotional prosody of speech. An EEG data corpus was collected with specific speech stimuli, in which emotion was represented with only speech prosody and without semantic context. A novel EEG-Prosody CRNN model was proposed to classify four types of typical emotions. The classification accuracy can achieve at 82.85% when the prosody features of speech were integrated as input, which outperformed most audio-evoked EEG-based emotion classification methods."
TAFFC2023,emotion,GMSS: Graph-Based Multi-Task Self-Supervised Learning for EEG Emotion Recognition.,72,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Overfitting,Jigsaw,Artificial intelligence,Task (project management),Multi-task learning,Computer science,Generalization,Electroencephalography,Feature learning,Machine learning,Pattern recognition (psychology),Emotion recognition,Speech recognition,Graph,Artificial neural network,Psychology,Engineering,Mathematics,Mathematical analysis,Pedagogy,Systems engineering,Theoretical computer science,Psychiatry",,,"Yang Li,Ji Chen,Fu Li,Boxun Fu,Hao Wu,Youshuo Ji,Yijin Zhou,Yi Niu,Guangming Shi,Wenming Zheng",
TAFFC2023,affective,Editorial: Special Issue on Unobtrusive Physiological Measurement Methods for Affective Applications.,0,"Non-Invasive Vital Sign Monitoring,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Photoplethysmogram,Strapping,Computer science,Scalability,Formative assessment,Psychology,Human–computer interaction,Artificial intelligence,Computer vision,Engineering,Mathematics education,Mechanical engineering,Filter (signal processing),Database",,,"Ioannis T. Pavlidis,Theodora Chaspari,Daniel McDuff",
TAFFC2023,emotion,WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition via Gesture and Facial Expression.,5,"Emotion and Mood Recognition,Speech and Audio Processing,Gaze Tracking and Assistive Technology","Modalities,Facial expression,Gesture,Modality (human–computer interaction),Computer science,Emotion recognition,Expression (computer science),Leverage (statistics),Artificial intelligence,Speech recognition,Human–computer interaction,Social science,Sociology,Programming language",,,"Yu Gu,Xiang Zhang,Huan Yan,Jingyang Huang,Zhi Liu,Mianxiong Dong,Fuji Ren",
TAFFC2023,emotion,Emotion Arousal Assessment Based on Multimodal Physiological Signals for Game Users.,3,"Emotion and Mood Recognition,Color perception and design,Heart Rate Variability and Autonomic Control","Arousal,Emotion recognition,Computer science,SIGNAL (programming language),Cognitive psychology,Psychology,Speech recognition,Social psychology,Programming language",,,"Rongyang Li,Jianguo Ding,Huansheng Ning",
TAFFC2023,emotion,Two Birds With One Stone: Knowledge-Embedded Temporal Convolutional Transformer for Depression Detection and Emotion Recognition.,21,"Emotion and Mood Recognition,Mental Health via Writing,Sentiment Analysis and Opinion Mining","Emotion recognition,Transformer,Psychology,Artificial intelligence,Computer science,Cognitive psychology,Pattern recognition (psychology),Speech recognition,Engineering,Electrical engineering,Voltage",,,"Wenbo Zheng,Lan Yan,Fei-Yue Wang",
TAFFC2023,emotion,Group Synchrony for Emotion Recognition Using Physiological Signals.,9,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Valence (chemistry),Arousal,Interpersonal communication,Conversation,Dyad,Emotion recognition,Psychology,Notation,Cognitive psychology,Artificial intelligence,Speech recognition,Computer science,Mathematics,Communication,Social psychology,Arithmetic,Physics,Quantum mechanics",,,"Patrícia J. Bota,Tianyi Zhang,Abdallah El Ali,Ana Fred,Hugo Plácido da Silva,Pablo César",
TAFFC2023,emotion,Virtual Reality for Emotion Elicitation - A Review.,72,"Emotion and Mood Recognition,Color perception and design,Face Recognition and Perception","Virtual reality,Perception,Empathy,Relevance (law),Mechanism (biology),Cognitive psychology,Psychology,Affect (linguistics),Cognition,Human–computer interaction,Computer science,Social psychology,Communication,Philosophy,Epistemology,Neuroscience,Political science,Law",,,"Rukshani Somarathna,Tomasz Bednarz,Gelareh Mohammadi",
TAFFC2023,emotions,A Neural Predictive Model of Negative Emotions for COVID-19.,4,"Functional Brain Connectivity Studies,Mental Health Research Topics,Anxiety, Depression, Psychometrics, Treatment, Cognitive Processes","Interpretability,Artificial neural network,Artificial intelligence,Population,Psychology,Neural correlates of consciousness,Machine learning,Computer science,Medicine,Psychiatry,Cognition,Environmental health",,,"Yu Mao,Dongtao Wei,Wenjing Yang,Qunlin Chen,Jiangzhou Sun,Yaxu Yu,Yu Li,Kaixiang Zhuang,Xiaoqin Wang,Li He,Tingyong Feng,Xu Lei,Qinghua He,Hong Chen,Shaozheng Qin,Yunzhe Liu,Jiang Qiu",
TAFFC2023,affect,Graph-Based Facial Affect Analysis: A Review.,19,"Face recognition and analysis,Color perception and design,Emotion and Mood Recognition","Affect (linguistics),Facial expression,Affective computing,Graph,Computer science,Psychology,Artificial intelligence,Theoretical computer science,Communication",,,"Yang Liu,Xingming Zhang,Yante Li,Jinzhao Zhou,Xin Li,Guoying Zhao",
TAFFC2023,emotion,Survey on Emotion Sensing Using Mobile Devices.,12,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Wearable computer,Field (mathematics),Mobile device,Inference,Digital library,World Wide Web,Human–computer interaction,Artificial intelligence,Information retrieval,Mathematics,Art,Poetry,Literature,Pure mathematics,Embedded system",,,"Kangning Yang,Benjamin Tag,Chaofan Wang,Yue Gu,Zhanna Sarsenbayeva,Tilman Dingler,Greg Wadley,Jorge Gonçalves",
TAFFC2023,emotion,"Emotion Expression in Human Body Posture and Movement: A Survey on Intelligible Motion Factors, Quantification and Validation.",6,"Color perception and design,Human Motion and Animation,Emotion and Mood Recognition","Motion (physics),Movement (music),Expression (computer science),Motion capture,Body posture,Psychology,Computer vision,Artificial intelligence,Computer science,Physical medicine and rehabilitation,Medicine,Physics,Acoustics,Programming language",,,"Mehdi-Antoine Mahfoudi,Alexandre Meyer,Thibaut Gaudin,Axel Buendia,Saïda Bouakaz",
TAFFC2023,emotion,EEG-Based Subject-Independent Emotion Recognition Using Gated Recurrent Unit and Minimum Class Confusion.,29,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Computer science,Discriminative model,Artificial intelligence,Generalization,Confusion matrix,Electroencephalography,Pattern recognition (psychology),Class (philosophy),Feature (linguistics),Speech recognition,Confusion,Machine learning,Subject (documents),Psychology,Mathematics,Mathematical analysis,Linguistics,Philosophy,Psychiatry,Library science,Psychoanalysis",,,"Heng Cui,Aiping Liu,Xu Zhang,Xiang Chen,Jun Liu,Xun Chen",
TAFFC2023,affective,Interaction of Cognitive and Affective Load Within a Virtual City.,3,"Tactile and Sensory Interactions,Color perception and design,Spatial Cognition and Navigation","Cognitive load,Task (project management),Arousal,Cognition,Psychology,Virtual reality,Cognitive psychology,Everyday life,Human–computer interaction,Computer science,Social psychology,Engineering,Neuroscience,Systems engineering,Law,Political science",,,"Thomas D. Parsons,Justin Asbee,Christopher G. Courtney",
TAFFC2023,affective,MIA-Net: Multi-Modal Interactive Attention Network for Multi-Modal Affective Analysis.,14,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Modal,Modalities,Modality (human–computer interaction),Generalization,Computer science,Artificial intelligence,Representation (politics),Task (project management),Modal analysis,Machine learning,Pairwise comparison,Theoretical computer science,Mathematics,Engineering,Mathematical analysis,Social science,Chemistry,Systems engineering,Structural engineering,Finite element method,Sociology,Politics,Political science,Polymer chemistry,Law",,,"Shuzhen Li,Tong Zhang,Bianna Chen,C. L. Philip Chen",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
EMNLP2024,emotion,Linear Layer Extrapolation for Fine-Grained Emotion Classification,0,"Emotion and Mood Recognition,Face and Expression Recognition,Anomaly Detection Techniques and Applications","Extrapolation,Computer science,Layer (electronics),Artificial intelligence,Statistics,Mathematics,Materials science,Composite material",,https://aclanthology.org/2024.emnlp-main.1161.pdf,"Mayukh Sharma,Sean O’Brien,Julian McAuley","Certain abilities of Transformer-based language models consistently emerge in their later layers. Previous research has leveraged this phenomenon to improve factual accuracy through self-contrast, penalizing early-exit predictions based on the premise that later-layer updates are more factually reliable than earlier-layer associations. We observe a similar pattern for fine-grained emotion classification in text, demonstrating that self-contrast can enhance encoder-based text classifiers. Additionally, we reinterpret self-contrast as a form of linear extrapolation, which motivates a refined approach that dynamically adjusts the contrastive strength based on the selected intermediate layer. Experiments across multiple models and emotion classification datasets show that our method outperforms standard classification techniques in fine-grained emotion classification tasks."
EMNLP2024,affective,MASIVE: Open-Ended Affective State Identification in English and Spanish,0,Deception detection and forensic psychology,"Identification (biology),State (computer science),Psychology,Linguistics,Natural language processing,Computer science,Philosophy,Algorithm,Botany,Biology",,https://aclanthology.org/2024.emnlp-main.1139.pdf,"Nicholas Deas,Elsbeth Turcan,Ivan Ernesto Perez Mejia,Kathleen McKeown","In the field of emotion analysis, much NLP research focuses on identifying a limited number of discrete emotion categories, often applied across languages. These basic sets, however, are rarely designed with textual data in mind, and culture, language, and dialect can influence how particular emotions are interpreted. In this work, we broaden our scope to a practically unbounded set of affective states, which includes any terms that humans use to describe their experiences of feeling. We collect and publish MASIVE, a dataset of Reddit posts in English and Spanish containing over 1,000 unique affective states each. We then define the new problem of affective state identification for language generation models framed as a masked span prediction task. On this task, we find that smaller finetuned multilingual models outperform much larger LLMs, even on region-specific Spanish affective states. Additionally, we show that pretraining on MASIVE improves model performance on existing emotion benchmarks. Finally, through machine translation experiments, we find that native speaker-written data is vital to good performance on this task."
ECCV2022,emotion,S2-VER: Semi-Supervised Visual Emotion Recognition,16,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","C4.5 algorithm,Random forest,Decision tree,Artificial intelligence,Computer science,Feature (linguistics),Machine learning,Support vector machine,Perceptron,Multilayer perceptron,Artificial neural network,Speech recognition,Pattern recognition (psychology),Algorithm,Naive Bayes classifier,Linguistics,Philosophy","https://github.com/exped1230/S2-VER.""",https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970483.pdf,"Guoli Jia, Jufeng Yang","""Visual emotion recognition (VER), which plays an important role in various applications, has attracted increasing attention of researchers. Due to the ambiguous characteristic of emotion, it is hard to annotate a reliable large-scale dataset in this field. An alternative solution is semi-supervised learning (SSL), which progressively selects high-confidence samples from unlabeled data to help optimize the model. However, it is challenging to directly employ existing SSL algorithms in VER task. On the one hand, compared with object recognition, in VER task, the accuracy of the produced pseudo labels for unlabeled data drops a large margin. On the other hand, the maximum probability in the prediction is difficult to reach the fixed threshold, which leads to few unlabeled samples can be leveraged. Both of them would induce the suboptimal performance of the learned model. To address these issues, we propose S2-VER, the first SSL algorithm for VER, which consists of two com- ponents. The first component, reliable emotion label learning, aims to improve the accuracy of pseudo-labels. In detail, it generates smoothing labels by computing the similarity between the maintained emotion prototypes and the embedding of the sample. The second one is ambiguity-aware adaptive threshold strategy, which is dedicated to leveraging more unlabeled samples. Specifically, our strategy uses information entropy to measure the ambiguity of the smoothing labels, then adaptively adjusts the threshold, which is adopted to select high-confidence unlabeled samples. Extensive experiments conducted on six public datasets show that our proposed S2-VER performs favorably against the state-of-the-art approaches. The code is available at https://github.com/exped1230/S2-VER."""
ECCV2024,emotional,All You Need is Your Voice: Emotional Face Representation with Audio Perspective for Emotional Talking Face Generation,0,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Speech and Audio Processing","Computer science,Perspective (graphical),Face (sociological concept),Speech recognition,Representation (politics),Multimedia,Human–computer interaction,Artificial intelligence,Linguistics,Philosophy,Politics,Political science,Law","https://github.com/sbde500/EAP""",https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08116.pdf,"Seongho Kim, Byung Cheol Song*","""With the rise of generative models, multi-modal video generation has gained significant attention, particularly in the realm of audio-driven emotional talking face synthesis. This paper addresses two key challenges in this domain: Input bias and intensity saturation. A novel neutralization scheme is first proposed to counter input bias, yielding impressive results in generating neutral talking faces from emotionally expressive ones. Furthermore, 2D continuous emotion label-based regression learning effectively generates varying emotional intensities on a frame basis. Results from a user study quantify subjective interpretations of strong emotions and naturalness, revealing up to 78.09% higher emotion accuracy and up to 3.41 higher naturalness score compared to the lowest-ranked method. https://github.com/sbde500/EAP"""
INTERSPEECH2022,emotion,ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition,8,"Music and Audio Processing,Speech and Audio Processing,Neuroscience and Music Perception","Computer science,Emotion recognition,Artificial intelligence,Feature (linguistics),Fusion,Pattern recognition (psychology),Sensor fusion,Feature extraction,Speech recognition,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2022/huang22d_interspeech.pdf,"Zi Huang, Shulei Ji, Zhilan Hu, Chuangjian Cai, Jing Luo, Xinyu Yang","Music emotion recognition (MER), a sub-task of music information retrieval (MIR), has developed rapidly in recent years. However, the learning of affect-salient features remains a challenge. In this paper, we propose an end-to-end attention-based deep feature fusion (ADFF) approach for MER. Only taking log Mel-spectrogram as input, this method uses adapted VGGNet as spatial feature learning module (SFLM) to obtain spatial features across different levels. Then, these features are fed into squeeze-and-excitation (SE) attention-based temporal feature learning module (TFLM) to get multi-level emotion-related spatial-temporal features (ESTFs), which can discriminate emotions well in the final emotion space. In addition, a novel data processing is devised to cut the single-channel input into multi-channel to improve calculative efficiency while ensuring the quality of MER. Experiments show that our proposed method achieves 10.43% and 4.82% relative improvement of valence and arousal respectively on the R2 score compared to the state-of-the-art model, meanwhile, performs better on datasets with distinct scales and in multi-task learning."
INTERSPEECH2024,emotional,X-E-Speech: Joint Training Framework of Non-Autoregressive Cross-lingual Emotional Text-to-Speech and Voice Conversion,0,Speech Recognition and Synthesis,"Speech recognition,Computer science,Joint (building),Autoregressive model,Natural language processing,Training (meteorology),Speech processing,Artificial intelligence,Mathematics,Statistics,Engineering,Architectural engineering,Physics,Meteorology",,https://www.isca-archive.org//interspeech_2024/guo24b_interspeech.pdf,"Houjian Guo, Chaoran Liu, Carlos Toshinori Ishi, Hiroshi Ishiguro","Large language models (LLMs) have been widely used in cross-lingual and emotional speech synthesis, but they require extensive data and retain the drawbacks of previous autoregressive (AR) speech models, such as slow inference speed and lack of robustness and interpretation. In this paper, we propose a cross-lingual emotional speech generation model, X-E-Speech, which achieves the disentanglement of speaker style and cross-lingual content features by jointly training non-autoregressive (NAR) voice conversion (VC) and text-to-speech (TTS) models. For TTS, we freeze the style-related model components and fine-tune the content-related structures to enable cross-lingual emotional speech synthesis. For VC, we improve the emotion similarity between the generated results and the reference speech by introducing the similarity loss between content features for VC and text for TTS."
INTERSPEECH2024,emotion,Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations,0,Speech and Audio Processing,"Computer science,Emotion recognition,Speech recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2024/khaertdinov24_interspeech.pdf,"Bulat Khaertdinov, Pedro Jeruis, Annanda Sousa, Enrique Hortal","Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in Speech Emotion Recognition (SER) performance, reaching unprecedented levels. However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task. In this paper, we propose a multi-view SSL pre-training technique that can be applied to various representations of speech, including the ones generated by large speech models, to improve SER performance in scenarios where annotations are limited. Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations."
INTERSPEECH2024,emotion,MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge in Speech Emotion Recognition,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Perspective (graphical),Speech recognition,Training (meteorology),Emotion recognition,Artificial neural network,Fusion,Artificial intelligence,Natural language processing,Linguistics,Philosophy,Physics,Meteorology",,https://www.isca-archive.org//interspeech_2024/sun24b_interspeech.pdf,"Haiyang Sun, Fulin Zhang, Yingying Gao, Shilei Zhang, Zheng Lian, Junlan Feng","Speech Emotion Recognition (SER) is an important research topic in human-computer interaction. Many recent works focus on directly extracting emotional cues through pre-trained knowledge, frequently overlooking considerations of appropriateness and comprehensiveness. Therefore, we propose a novel framework for pre-training knowledge in SER, called Multi-perspective Fusion Search Network (MFSN). Considering comprehensiveness, we partition speech knowledge into Textual-related Emotional Content (TEC) and Speech-related Emotional Content (SEC), capturing cues from both semantic and acoustic perspectives, and we design a new architecture search space to fully leverage them. Considering appropriateness, we verifies the efficacy of different modeling approaches in capturing SEC and fills the gap in current research. Experimental results on multiple datasets demonstrate the superiority of MFSN."
INTERSPEECH2024,emotion,Are Paralinguistic Representations all that is needed for Speech Emotion Recognition?,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and dialogue systems","Paralanguage,Computer science,Speech recognition,Natural language processing,Human–computer interaction,Communication,Psychology",,https://www.isca-archive.org//interspeech_2024/phukan24b_interspeech.pdf,"Orchid Chetia Phukan, Gautam Siddharth Kashyap, Arun Balaji Buduru, Rajesh Sharma","Availability of representations from pre-trained models (PTMs) have facilitated substantial progress in speech emotion recognition (SER). Particularly, representations from PTM trained for paralinguistic speech processing have shown state-of-the-art (SOTA) performance for SER. However, such paralinguistic PTM representations haven’t been evaluated for SER in linguistic environments other than English. Also, paralinguistic PTM representations haven’t been investigated in benchmarks such as SUPERB, EMO-SUPERB, ML-SUPERB for SER. This makes it difficult to access the efficacy of paralinguistic PTM representations for SER in multiple languages. To fill this gap, we perform a comprehensive comparative study of five SOTA PTM representations. Our results shows that paralinguistic PTM (TRILLsson) representations performs the best and this performance can be attributed to its effectiveness in capturing pitch, tone and other speech characteristics more effectively than other PTM representations."
INTERSPEECH2024,emotion,A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion Recognition,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Anchoring,Computer science,Layer (electronics),Speech recognition,Emotion recognition,Psychology,Materials science,Cognitive science,Composite material",,https://www.isca-archive.org//interspeech_2024/upadhyay24_interspeech.pdf,"Shreya G. Upadhyay, Carlos Busso, Chi-Chun Lee","Cross-lingual speech emotion recognition (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the task-specific nature and hierarchical architecture of these models, each transformer layer encapsulates different levels of information. Leveraging this hierarchical structure, our study focuses on the information embedded  across different layers. Through an examination of layer feature similarity across different languages, we propose a novel strategy called a layer-anchoring mechanism to facilitate emotion transfer in cross-lingual SER tasks. Our approach is evaluated using two distinct language affective corpora (MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on the BIIC podcast corpus. The analysis uncovers interesting insights into the behavior of popular pretrained models."
TAFFC2023,emotion,Estimating the Uncertainty in Emotion Class Labels With Utterance-Specific Dirichlet Priors.,5,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Prior probability,Utterance,Class (philosophy),Dirichlet distribution,Artificial intelligence,Latent Dirichlet allocation,Computer science,Mathematics,Natural language processing,Speech recognition,Pattern recognition (psychology),Topic model,Bayesian probability,Mathematical analysis,Boundary value problem",,,"Wen Wu,Chao Zhang,Xixin Wu,Philip C. Woodland",
TAFFC2023,"emotion,affect",Smart Affect Monitoring With Wearables in the Wild: An Unobtrusive Mood-Aware Emotion Recognition System.,6,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Heart Rate Variability and Autonomic Control","Mood,Affect (linguistics),Wearable computer,Emotion recognition,Affective computing,Emotion detection,Psychology,Computer science,Wearable technology,Human–computer interaction,Cognitive psychology,Artificial intelligence,Communication,Clinical psychology,Embedded system",,,"Yekta Said Can,Cem Ersoy",
TAFFC2023,emotional,Emotional Expressivity is a Reliable Signal of Surprise.,3,"Personality Traits and Psychology,Emotions and Moral Behavior,Psychology of Moral and Emotional Judgment","Surprise,Expressivity,SIGNAL (programming language),Psychology,Computer science,Emotion classification,Cognitive psychology,Speech recognition,Social psychology,Programming language,Genetics,Biology",,,"Su Lei,Jonathan Gratch",
TAFFC2023,emotion,MMPosE: Movie-Induced Multi-Label Positive Emotion Classification Through EEG Signals.,7,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Discriminative model,Electroencephalography,Emotion classification,Correlation,Computer science,Artificial intelligence,Representation (politics),Class (philosophy),Negative emotion,Emotion detection,Emotion recognition,Pattern recognition (psychology),Psychology,Mathematics,Social psychology,Geometry,Psychiatry,Politics,Political science,Law",,,"Xiaobing Du,Xiaoming Deng,Hangyu Qin,Yezhi Shu,Fang Liu,Guozhen Zhao,Yu-Kun Lai,Cuixia Ma,Yong-Jin Liu,Hongan Wang",
TAFFC2023,emotional,Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic Crowd Simulation.,4,"Evacuation and Crowd Dynamics,Anomaly Detection Techniques and Applications,Traffic control and management","Emotional contagion,Seriousness,Action (physics),Reinforcement learning,Rationality,Psychology,Field (mathematics),Computer science,Reinforcement,Cognitive psychology,Social psychology,Artificial intelligence,Mathematics,Physics,Quantum mechanics,Political science,Pure mathematics,Law",,,"Pei Lv,Qingqing Yu,Boya Xu,Chaochao Li,Bing Zhou,Mingliang Xu",
TAFFC2023,emotion,Audio-Visual Emotion Recognition With Preference Learning Based on Intended and Multi-Modal Perceived Labels.,18,"Emotion and Mood Recognition,Music and Audio Processing,Color perception and design","Emotion recognition,Modal,Preference,Audio visual,Psychology,Speech recognition,Cognitive psychology,Computer science,Human–computer interaction,Artificial intelligence,Multimedia,Mathematics,Chemistry,Statistics,Polymer chemistry",,,"Yuanyuan Lei,Houwei Cao",
TAFFC2023,emotion,Driver Emotion Recognition With a Hybrid Attentional Multimodal Fusion Framework.,25,"Emotion and Mood Recognition,Sleep and Work-Related Fatigue,EEG and Brain-Computer Interfaces","Computer science,Fuse (electrical),Emotion recognition,Feature (linguistics),Affective computing,Artificial intelligence,Sensor fusion,Convolutional neural network,Machine learning,Engineering,Linguistics,Philosophy,Electrical engineering",,,"Luntian Mou,Yiyuan Zhao,Chao Zhou,Bahareh Nakisa,Mohammad Naim Rastgoo,Lei Ma,Tiejun Huang,Baocai Yin,Ramesh C. Jain,Wen Gao",
ACL2023,emotion,MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations,15,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Modalities,Conversation,Complementarity (molecular biology),Artificial intelligence,Natural language processing,Benchmark (surveying),Focus (optics),Multimodality,Correlation,Task (project management),Emotion recognition,Psychology,Communication,Social science,Physics,Geometry,Mathematics,Management,Geodesy,Sociology,Biology,World Wide Web,Optics,Economics,Genetics,Geography",,https://aclanthology.org/2023.acl-long.824.pdf,"Tao Shi,Shao-Lun Huang","Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping relationships across different modalities. Furthermore, existing state-of-the-art ERC models have difficulty classifying minority and semantically similar emotion categories. To address these challenges, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross-attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion categories on both datasets, the improvements in minority and semantically similar emotions are especially significant."
ACL2023,emotion,A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation,11,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Conversation,Natural language processing,Artificial intelligence,Modalities,Transformer,Locality,Modality (human–computer interaction),Linguistics,Social science,Philosophy,Physics,Quantum mechanics,Voltage,Sociology",,https://aclanthology.org/2023.acl-long.732.pdf,"Xiaoheng Zhang,Yang Li","Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these limitations, we propose a cross-modality context fusion and semantic refinement network (CMCF-SRNet). Specifically, we first design a cross-modal locality-constrained transformer to explore the multimodal interaction. Second, we investigate a graph-based semantic refinement transformer, which solves the limitation of insufficient semantic relationship information between utterances. Extensive experiments on two public benchmark datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, indicating its potential application in emotion recognition. Our model will be available at "
EMNLP2023,emotion,Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition,9,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Advanced Computing and Algorithms","Computer science,Modalities,Artificial intelligence,Utterance,Graph,Emotion recognition,Modality (human–computer interaction),Smoothing,Modal,Natural language processing,Machine learning,Theoretical computer science,Computer vision,Social science,Chemistry,Sociology,Polymer chemistry",https://anonymous.4open.science/r/MERC-7F88).,https://aclanthology.org/2023.emnlp-main.996.pdf,"Dongyuan Li,Yusong Wang,Kotaro Funakoshi,Manabu Okumura","Multimodal emotion recognition aims to recognize emotions for each utterance from multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (Joyful), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter- and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that Joyful achieved state-of-the-art (SOTA) performance compared with all baselines. Code is released on Github (https://anonymous.4open.science/r/MERC-7F88)."
EMNLP2023,emotion,Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification,0,"Sentiment Analysis and Opinion Mining,Hate Speech and Cyberbullying Detection,Spam and Phishing Detection","Complaint,Sentiment analysis,Identification (biology),Computer science,Modal,Task (project management),Process (computing),Artificial intelligence,Data science,Natural language processing,Chemistry,Botany,Management,Political science,Polymer chemistry,Law,Economics,Biology,Operating system",,https://aclanthology.org/2023.emnlp-main.999.pdf,"Apoorva Singh,Siddarth Chandrasekar,Sriparna Saha,Tanmay Sen","Automatic detection of consumers’ complaints about items or services they buy can be critical for organizations and online merchants. Previous studies on complaint identification are limited to text. Images along with the reviews can provide cues to identify complaints better, thus emphasizing the importance of incorporating multi-modal inputs into the process. Generally, the customer’s emotional state significantly impacts the complaint expression; thus, the effect of emotion and sentiment on complaint identification must also be investigated. Furthermore, different organizations are usually not allowed to share their privacy-sensitive records due to data security and privacy concerns. Due to these issues, traditional models find it hard to understand and identify complaint patterns, particularly in the financial and healthcare sectors. In this work, we created a new dataset - Multi-modal Complaint Dataset (MCD), a collection of reviews and images of the products posted on the website of the retail giant Amazon. We propose a federated meta-learning-based multi-modal multi-task framework for identifying complaints considering emotion recognition and sentiment analysis as two auxiliary tasks. Experimental results indicate that the proposed approach outperforms the baselines and the state-of-the-art approaches in centralized and federated meta-learning settings."
TAFFC2023,affect,Using Affect as a Communication Modality to Improve Human-Robot Communication in Robot-Assisted Search and Rescue Scenarios.,6,"Social Robot Interaction and HRI,Language, Discourse, Communication Strategies,Emotions and Moral Behavior","Modality (human–computer interaction),Situation awareness,Robot,Context (archaeology),Computer science,Search and rescue,Human–computer interaction,Affect (linguistics),Human–robot interaction,Affective computing,Artificial intelligence,Psychology,Communication,Engineering,Paleontology,Biology,Aerospace engineering",,,"Sami Alperen Akgun,Moojan Ghafurian,Mark Crowley,Kerstin Dautenhahn",
TAFFC2023,emotion,An Enroll-to-Verify Approach for Cross-Task Unseen Emotion Class Recognition.,2,"Emotion and Mood Recognition,Human Pose and Action Recognition,Speech Recognition and Synthesis","Class (philosophy),Task (project management),Emotion recognition,Computer science,Psychology,Artificial intelligence,Speech recognition,Cognitive psychology,Engineering,Systems engineering",,,"Jeng-Lin Li,Chi-Chun Lee",
TAFFC2023,emotions,Speech Synthesis With Mixed Emotions.,21,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Speech recognition,Psychology,Computer science,Cognitive psychology",,,"Kun Zhou,Berrak Sisman,Rajib Rana,Björn W. Schuller,Haizhou Li",
TAFFC2023,affective,Use of Affective Visual Information for Summarization of Human-Centric Videos.,6,"Video Analysis and Summarization,Music and Audio Processing,Advanced Image and Video Retrieval Techniques","Automatic summarization,Computer science,Affective computing,Human–computer interaction,Artificial intelligence,Psychology,Cognitive psychology,Computer vision",,,"Berkay Köprü,Engin Erzin",
TAFFC2023,emotion,Multitask Learning From Augmented Auxiliary Data for Improving Speech Emotion Recognition.,14,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Emotion recognition,Speech recognition,Multi-task learning,Computer science,Artificial intelligence,Natural language processing,Affective computing,Cognitive psychology,Psychology,Machine learning,Task (project management),Engineering,Systems engineering",,,"Siddique Latif,Rajib Rana,Sara Khalifa,Raja Jurdak,Björn W. Schuller",
TAFFC2023,emotion,"Recognizing, Fast and Slow: Complex Emotion Recognition With Facial Expression Detection and Remote Physiological Measurement.",3,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Advanced Memory and Neural Computing","Arousal,Valence (chemistry),Computer science,Facial expression,Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Cognition,Emotion detection,Feature (linguistics),Affective computing,Emotion classification,Feature extraction,Psychology,Physics,Philosophy,Quantum mechanics,Neuroscience,Linguistics",,,"Yi-Chiao Wu,Li-Wen Chiu,Chun-Chih Lai,Bing-Fei Wu,Sunny S. J. Lin",
TAFFC2023,emotion,Applying Segment-Level Attention on Bi-Modal Transformer Encoder for Audio-Visual Emotion Recognition.,10,"Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Speech recognition,Modalities,Encoder,Artificial intelligence,Pattern recognition (psychology),Facial expression,Artificial neural network,Operating system,Social science,Sociology",,,"Jia-Hao Hsu,Chung-Hsien Wu",
TAFFC2023,emotion,Cluster-Level Contrastive Learning for Emotion Recognition in Conversations.,31,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Computer science,Leverage (statistics),Natural language processing,Categorical variable,Artificial intelligence,Utterance,Speech recognition,Feature learning,Valence (chemistry),Space (punctuation),Machine learning,Physics,Quantum mechanics,Operating system",,,"Kailai Yang,Tianlin Zhang,Hassan Alhuzali,Sophia Ananiadou",
INTERSPEECH2024,"emotion,emotions",Bridging Emotions Across Languages: Low Rank Adaptation for Multilingual Speech Emotion Recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis","Bridging (networking),Computer science,Adaptation (eye),Emotion recognition,Rank (graph theory),Speech recognition,Natural language processing,Linguistics,Artificial intelligence,Psychology,Mathematics,Computer network,Philosophy,Combinatorics,Neuroscience",,https://www.isca-archive.org//interspeech_2024/goncalves24_interspeech.pdf,"Lucas Goncalves, Donita Robinson, Elizabeth Richerson, Carlos Busso","The field of speech emotion recognition (SER) is constantly evolving with the surge in voice data and linguistic diversity. This growth highlights the need for SER systems capable of overcoming language barriers in both linguistic structure and cultural expression of emotions. We envision a SER framework that captures general trends in the expression of emotions, while also modeling language-specific information. Our study investigates low rank adaptation (LoRA) for creating multilingual SER models, applying LoRA in a multilingual context to efficiently adapt pre-trained models to new languages with minimal changes. This enhances cross-lingual adaptability and efficiency of SER systems, refining models to recognize emotions across languages without extensive retraining. In this study, we focus on exploring this method to bridge the gap between English and Taiwanese Mandarin in naturalistic settings, demonstrating strong performance in both languages."
INTERSPEECH2024,emotion,Enhancing Multimodal Emotion Recognition through ASR Error Compensation and LLM Fine-Tuning,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Speech recognition,Compensation (psychology),Artificial intelligence,Human–computer interaction,Psychology,Social psychology",,https://www.isca-archive.org//interspeech_2024/kyung24_interspeech.pdf,"Jehyun Kyung, Serin Heo, Joon-Hyuk Chang","Multimodal emotion recognition (MER), particularly using speech and text, is promising for enhancing human-computer interaction. However, the efficacy of such systems is often compromised by inaccuracies introduced during the automatic speech recognition (ASR) process. Addressing this, we present a comprehensive MER system that incorporates ways to make up for errors in ASR-generated text. Our system capitalizes on the strengths of speech signals and ASR-generated text, employing a cross-modal transformer (CMT) to blend these modalities effectively. We introduce a novel error compensation technique to counteract the detrimental effects of ASR inaccuracies and employ preference learning to fine-tune a large language model (LLM), thus improving its ability to distinguish slight emotional nuances in text. Performance of our proposed MER system is evaluated on the IEMOCAP dataset, demonstrating significant advancements in emotion recognition accuracy over conventional methods."
TAFFC2023,emotion,"Simple But Powerful, a Language-Supervised Method for Image Emotion Classification.",11,"Sentiment Analysis and Opinion Mining,Multimodal Machine Learning Applications,Text and Document Classification Technologies","Computer science,Task (project management),Emotion classification,Notation,Artificial intelligence,Image (mathematics),Simple (philosophy),Natural language processing,Margin (machine learning),Pattern recognition (psychology),Machine learning,Linguistics,Philosophy,Management,Epistemology,Economics",,,"Sinuo Deng,Lifang Wu,Ge Shi,Lehao Xing,Wenjin Hu,Heng Zhang,Ye Xiang",
TAFFC2023,affect,Dyadic Affect in Parent-Child Multimodal Interaction: Introducing the DAMI-P2C Dataset and its Preliminary Analysis.,11,"Child Development and Digital Technology,Innovative Human-Technology Interaction,Digital Mental Health Interventions","Dyad,Affect (linguistics),Psychology,Psychological intervention,Affective computing,Reading (process),Social relation,Cognitive psychology,Developmental psychology,Data science,Computer science,Social psychology,Communication,Neuroscience,Psychiatry,Political science,Law",,,"Huili Chen,Sharifa Alghowinem,Soo Jung Jang,Cynthia Breazeal,Hae Won Park",
INTERSPEECH2023,emotion,Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition,0,"Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Emotion recognition,Speech recognition,Feature (linguistics),Multi-task learning,Artificial intelligence,Natural language processing,Fusion,Sensor fusion,Task (project management),Linguistics,Engineering,Philosophy,Systems engineering",,https://www.isca-archive.org//interspeech_2023/lee23g_interspeech.pdf,Shi-wook Lee,"In addition to linguistic information, speech contains non-lexical information, such as emotion, gender, and speaker identity. Recent self-supervised learning methods for speech representation can provide powerful initial feature spaces. However, a few training samples in speech emotion recognition cannot fully utilize the vast pretrained feature space. Herein, we propose an effective use of the feature space. First, to obtain more complementary information, diverse features are extracted by mapping the same utterance to different clusters via multitask learning. Thereafter, fusion methods are investigated according to the correlation among the diversely mapped features. The proposed methods are evaluated on two emotional speech corpora. The experimental results show that the proposed methods can effectively utilize the vast pretrained feature space and achieve state-of-the-art performance, with an unweighted average recall of 78.45% on the benchmark IEMOCAP corpus."
INTERSPEECH2022,emotional,Predicting Emotional Intensity in Political Debates via Non-verbal Signals,0,Opinion Dynamics and Social Influence,"Intensity (physics),Politics,Computer science,Artificial intelligence,Political science,Physics,Optics,Law",,https://www.isca-archive.org//interspeech_2022/yoon22_interspeech.pdf,"Jeewoo Yoon, Jinyoung Han, Erik Bucy, Jungseock Joo","Non-verbal expressions of politicians are important in election. In particular, the emotional intensity of politician revealed in a debate can be strongly linked to voters' evaluation. This paper proposes a multimodal deep-learning model for predicting the perceived emotional intensity of a candidate, which utilizes voice, face, and gesture to capture the comprehensive information of one's emotional intensity revealed in a debate. We collect a dataset of political debate videos from the 2020 Democratic presidential primaries in the USA, and train the proposed model with randomly sampled clips from the debate videos. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls."
EMNLP2023,emotion,A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection,4,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Debiasing,Counterfactual thinking,Computer science,Artificial intelligence,Training set,Generalization,Natural language processing,Machine learning,Context (archaeology),Cognitive psychology,Psychology,Social psychology,Mathematics,Mathematical analysis,Paleontology,Biology",,https://aclanthology.org/2023.emnlp-main.967.pdf,"Geng Tu,Ran Jing,Bin Liang,Min Yang,Kam-Fai Wong,Ruifeng Xu","Unintended dataset biases typically exist in existing Emotion Recognition in Conversations (ERC) datasets, including label bias, where models favor the majority class due to imbalanced training data, as well as the speaker and neutral word bias, where models make unfair predictions because of excessive correlations between specific neutral words or speakers and classes. However, previous studies in ERC generally focus on capturing context-sensitive and speaker-sensitive dependencies, ignoring the unintended dataset biases of data, which hampers the generalization and fairness in ERC. To address this issue, we propose a Training-Free Debiasing framework (TFD) that operates during prediction without additional training. To ensure compatibility with various ERC models, it does not balance data or modify the model structure. Instead, TFD extracts biases from the model by generating counterfactual utterances and contexts and mitigates them using simple yet empirically robust element-wise subtraction operations. Extensive experiments on three public datasets demonstrate that TFD effectively improves generalization ability and fairness across different ERC models."
EMNLP2024,emo,BLSP-Emo: Towards Empathetic Large Speech-Language Models,0,Topic Modeling,"Psychology,Linguistics,Indirect speech,Cognitive psychology,Philosophy",,https://aclanthology.org/2024.emnlp-main.1070.pdf,"Chen Wang,Minpeng Liao,Zhongqiang Huang,Junhong Wu,Chengqing Zong,Jiajun Zhang","The recent release of GPT-4o showcased the potential of end-to-end multimodal models, not just in terms of low latency but also in their ability to understand and generate expressive speech with rich emotions. While the details are unknown to the open research community, it likely involves significant amounts of curated data and compute, neither of which is readily accessible. In this paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with Emotion support), a novel approach to developing an end-to-end speech-language model capable of understanding both semantics and emotions in speech and generate empathetic responses. BLSP-Emo utilizes existing speech recognition (ASR) and speech emotion recognition (SER) datasets through a two-stage process. The first stage focuses on semantic alignment, following recent work on pretraining speech-language models using ASR data. The second stage performs emotion alignment with the pretrained speech-language model on an emotion-aware continuation task constructed from SER data. Our experiments demonstrate that the BLSP-Emo model excels in comprehending speech and delivering empathetic responses, both in instruction-following tasks and conversations."
INTERSPEECH2024,emotion,Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition,0,"Emotion and Mood Recognition,Human Pose and Action Recognition,Video Surveillance and Tracking Methods","Matching (statistics),Computer science,Modal,Emotion recognition,Fusion,Artificial intelligence,Pattern recognition (psychology),Sensor fusion,Computer vision,Speech recognition,Mathematics,Linguistics,Statistics,Chemistry,Philosophy,Polymer chemistry",,https://www.isca-archive.org//interspeech_2024/li24z_interspeech.pdf,"Qifei Li, Yingming Gao, Yuhua Wen, Cong Wang, Ya Li","To address the limitation in multimodal emotion recognition (MER) performance arising from inter-modal information fusion, we propose a novel MER framework based on multitask learning where fusion occurs after alignment, called Foal-Net. The framework is designed to enhance the effectiveness of modality fusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL) and cross-modal emotion label matching (MEM). First, AVEL achieves alignment of emotional information in audio-video representations through contrastive learning. Then, a modal fusion network integrates the aligned features. Meanwhile, MEM assesses whether the emotions of the current sample pair are the same, providing assistance for modal information fusion and guiding the model to focus more on emotional information. The experimental results conducted on IEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and emotion alignment is necessary before modal fusion. The code is open-source."
INTERSPEECH2024,emotion,LoRA-MER: Low-Rank Adaptation of Pre-Trained Speech Models for Multimodal Emotion Recognition Using Mutual Information,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Adaptation (eye),Mutual information,Rank (graph theory),Hidden Markov model,Artificial intelligence,Natural language processing,Psychology,Mathematics,Combinatorics,Neuroscience",,https://www.isca-archive.org//interspeech_2024/cai24b_interspeech.pdf,"Yunrui Cai, Zhiyong Wu, Jia Jia, Helen Meng","Multimodal emotion recognition (MER) is crucial for machines to understand human intentions. Although many deep learning models have been proposed, MER still faces practical challenges. The key challenge is how to extract high-dimensional features that are more relevant to emotions. Another challenge is how to effectively model multimodal features, achieving a balance between similarity and diversity. In this paper, we propose the method of LoRA-MER using mutual information. We fine-tune a pre-trained speech model with Low-Rank Adaptation (LoRA) strategy and utilize a frozen pre-trained text model to robustly extract emotional features. Additionally, we adopt a multimodal fusion approach based on Mutual Information Neural Estimation (MINE) to enhance their correlation. Experimental results demonstrate the effectiveness of each module proposed in our method, and the performance of our model surpasses that of state-of-the-art speaker-independent approaches on IEMOCAP dataset."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
WWW2021,emotion,Dr.Emotion: Disentangled Representation Learning for Emotion Analysis on Social Media to Improve Community Resilience in the COVID-19 Era and Beyond.,16,"Vaccine Coverage and Hesitancy,Misinformation and Its Impacts,SARS-CoV-2 and COVID-19 Research","Misinformation,Herd immunity,Pandemic,Coronavirus disease 2019 (COVID-19),Social media,Medicine,Family medicine,Vaccination,Public relations,Political science,Immunology,Infectious disease (medical specialty),Disease,Pathology,Law",,,"Mingxuan Ju,Wei Song,Shiyu Sun,Yanfang Ye,Yujie Fan,Shifu Hou,Kenneth A. Loparo,Liang Zhao",
WWW2021,emotions,Modeling Human Motives and Emotions from Personal Narratives Using External Knowledge And Entity Tracking.,8,"Topic Modeling,Advanced Text Analysis Techniques,Natural Language Processing Techniques","Narrative,Computer science,Tracking (education),Knowledge management,Human–computer interaction,Psychology,Philosophy,Linguistics,Pedagogy",,,"Prashanth Vijayaraghavan,Deb Roy",
WWW2023,affects,Web Table Formatting Affects Readability on Mobile Devices.,0,"Interactive and Immersive Displays,Green IT and Sustainability,Innovative Human-Technology Interaction","Disk formatting,Readability,Computer science,Table (database),Usability,Mobile device,Task (project management),The Internet,Multimedia,Reading (process),World Wide Web,Human–computer interaction,Database,Operating system,Management,Political science,Law,Economics,Programming language",,,"Christopher Tensmeyer,Zoya Bylinskii,Tianyuan Cai,Dave Miller,Ani Nenkova,Aleena Gertrudes Niklaus,Shaun Wallace",
WWW2023,emotion,Learning Robust Multi-Modal Representation for Multi-Label Emotion Recognition via Adversarial Masking and Perturbation.,7,"Emotion and Mood Recognition,Adversarial Robustness in Machine Learning,Anomaly Detection Techniques and Applications","Computer science,Modalities,Artificial intelligence,Adversarial system,Overfitting,Machine learning,Modality (human–computer interaction),Representation (politics),Generalization,Masking (illustration),Pattern recognition (psychology),Speech recognition,Artificial neural network,Mathematics,Art,Mathematical analysis,Social science,Sociology,Politics,Political science,Law,Visual arts",,,"Shiping Ge,Zhiwei Jiang,Zifeng Cheng,Cong Wang,Yafeng Yin,Qing Gu",
WWW2022,"emotion,emotional",Emotion Bubbles: Emotional Composition of Online Discourse Before and After the COVID-19 Outbreak.,10,"Misinformation and Its Impacts,Humor Studies and Applications,Emotions and Moral Behavior","Coronavirus disease 2019 (COVID-19),Composition (language),Outbreak,2019-20 coronavirus outbreak,Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2),Psychology,Computer science,Virology,Medicine,Linguistics,Philosophy,Disease,Pathology,Infectious disease (medical specialty)",,,"Assem Zhunis,Gabriel Lima,Hyeonho Song,Jiyoung Han,Meeyoung Cha",
WWW2022,affects,How Misinformation Density Affects Health Information Search.,5,"Misinformation and Its Impacts,Media Influence and Politics,Vaccine Coverage and Hesitancy","Misinformation,Computer science,Health information,Internet privacy,Computer security,Health care,Political science,Law",,,"Qiurong Song,Jiepu Jiang",
WWW2022,emotions,"""I Have No Text in My Post"": Using Visual Hints to Model User Emotions in Social Media.",6,"Sentiment Analysis and Opinion Mining,Mental Health via Writing,Advanced Text Analysis Techniques","Social media,Computer science,Baseline (sea),Psychology,Cognitive psychology,Artificial intelligence,World Wide Web,Oceanography,Geology",,,"Junho Song,Kyungsik Han,Sang-Wook Kim",
INTERSPEECH2024,"emotion,emo",Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition,4,"Sentiment Analysis and Opinion Mining,Hate Speech and Cyberbullying Detection,Digital Communication and Language","Emotion recognition,Speech recognition,Computer science,Scale (ratio),Cognitive psychology,Artificial intelligence,Natural language processing,Psychology,Physics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2024/lin24i_interspeech.pdf,"Yi-Cheng Lin, Haibin Wu, Huang-Cheng Chou, Chi-Chun Lee, Hung-yi Lee","The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males.  Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact. "
INTERSPEECH2021,emotional,Exploring Emotional Prototypes in a High Dimensional TTS Latent Space,8,"Language and cultural evolution,Speech and dialogue systems,Speech Recognition and Synthesis","Prosody,Generative grammar,Space (punctuation),Generative model,Perception,Cognitive psychology,Latent semantic analysis,Computer science,Emotional expression,Psychology,Speech recognition,Natural language processing,Artificial intelligence,Neuroscience,Operating system",,https://www.isca-archive.org//interspeech_2021/rijn21_interspeech.pdf,"Pol van Rijn, Silvan Mertes, Dominik Schiller, Peter M.C. Harrison, Pauline Larrouy-Maestri, Elisabeth André, Nori Jacoby","Recent TTS systems are able to generate prosodically varied and realistic speech. However, it is unclear how this prosodic variation contributes to the perception of speakers’ emotional states. Here we use the recent psychological paradigm ‘Gibbs Sampling with People’ to search the prosodic latent space in a trained Global Style Token Tacotron model to explore prototypes of emotional prosody. Participants are recruited online and collectively manipulate the latent space of the generative speech model in a sequentially adaptive way so that the stimulus presented to one group of participants is determined by the response of the previous groups. We demonstrate that (1) particular regions of the model’s latent space are reliably associated with particular emotions, (2) the resulting emotional prototypes are well-recognized by a separate group of human raters, and (3) these emotional prototypes can be effectively transferred to new sentences. Collectively, these experiments demonstrate a novel approach to the understanding of emotional speech by providing a tool to explore the relation between the latent space of generative models and human semantics."
ACL2023,emotion,PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent,2,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Digital Mental Health Interventions","Politeness,Feeling,Empathy,Psychology,Applied psychology,Function (biology),Internet privacy,Psychotherapist,Social psychology,Computer science,Philosophy,Linguistics,Evolutionary biology,Biology",,https://aclanthology.org/2023.acl-long.685.pdf,"Kshitij Mishra,Priyanshu Priya,Asif Ekbal","The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client’s emotions and ensure client’s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client’s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client’s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need."
ACL2023,emotion,Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification,2,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Computer science,Hyperbolic space,Benchmark (surveying),Task (project management),Space (punctuation),Code (set theory),Artificial intelligence,Euclidean geometry,Euclidean space,Euclidean distance,Entropy (arrow of time),Source code,Semantic space,Theoretical computer science,Machine learning,Mathematics,Physics,Geometry,Management,Geodesy,Set (abstract data type),Quantum mechanics,Pure mathematics,Economics,Programming language,Geography,Operating system",,https://aclanthology.org/2023.acl-long.613.pdf,"Chih Yao Chen,Tun Min Hung,Yi-Li Hsu,Lun-Wei Ku","Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate hyperbolic embeddings to improve the FEC task. First, we learn label embeddings in the hyperbolic space to better capture their hierarchical structure, and then our model projects contextualized representations to the hyperbolic space to compute the distance between samples and labels. Experimental results show that incorporating such distance to weight cross entropy loss substantially improve the performance on two benchmark datasets, with around 3% improvement compared to previous state-of-the-art, and could even improve up to 8.6% when the labels are hard to distinguish. Code is available at "
INTERSPEECH2021,emotion,Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition,7,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Categorical variable,Annotation,Computer science,Sample size determination,Sample (material),Training (meteorology),Artificial intelligence,Emotion recognition,Confidence interval,Pattern recognition (psychology),Speech recognition,Statistics,Machine learning,Mathematics,Chromatography,Geography,Chemistry,Meteorology",,https://www.isca-archive.org//interspeech_2021/ryumina21_interspeech.pdf,"Elena Ryumina, Oxana Verkholyak, Alexey Karpov","Commonly adapted design of emotional corpora includes multiple annotations for the same instance from several annotators. Most of the previous studies assume the ground truth to be an average between all labels or the most frequently used label. Current study shows that this approach may not be optimal for training. By filtering training data according to the level of annotation agreement, it is possible to increase the performance of the system even on unreliable test samples. However, increasing the annotation confidence inevitably leads to a loss of data. Therefore, balancing the trade-off between annotation quality and sample size requires careful investigation. This study presents experimental findings of audio-visual emotion classification on a recently introduced RAMAS dataset, which contains rich categorical partially-continuous annotation for 6 basic emotions, and reveals important conclusions about optimal formulation of ground truth. By applying the proposed approach, it is possible to achieve classification accuracy of UAR=70.51% on the speech utterances with more than 60% agreement, which surpasses previously reported values on this corpus in the literature."
INTERSPEECH2023,emotion,Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario,0,"Social Robot Interaction and HRI,IoT-based Smart Home Systems","Human–robot interaction,Computer science,Emotion recognition,Human–computer interaction,Robot,Speech recognition,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/grageda23_interspeech.pdf,"Nicolás Grágeda, Eduardo Alvarado, Rodrigo Mahu, Carlos Busso, Néstor Becerra Yoma","Social robotics and human-robot partnership are becoming very relevant topics defining many challenges for state-of-the-art speech technology. This paper presents the first evaluation of speech emotion recognition (SER) technology with nonacted speech data recorded in a real indoor humanrobot interaction (HRI) scenario. The challenge is typified by distant speech processing, reverberation, and additive external and robot engine noise. We train and evaluate a machine learning-based based on simulated acoustic modelling that includes room impulse responses (RIRs), external noise, and beamforming response. We observe increased performance in the prediction of arousal, valence, and dominance with the proposed training procedure combined with delayandsum and minimum variance distortionless response (MVDR), with gain as high as 180%, compared with the result obtained with the model trained with the original data in controlled environments. Moreover, the degradation achieved when compared with the original matched training/testing condition is just 39%."
INTERSPEECH2023,emotion,Multi-Scale Temporal Transformer For Speech Emotion Recognition,5,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Transformer,Scale (ratio),Artificial intelligence,Engineering,Electrical engineering,Voltage,Physics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2023/li23m_interspeech.pdf,"Zhipeng Li, Xiaofen Xing, Yuanbo Fang, Weibin Zhang, Hengsheng Fan, Xiangmin Xu","Speech emotion recognition plays a crucial role in human-machine interaction systems. Recently various optimized Transformers have been successfully applied to speech emotion recognition. However, the existing Transformer architectures focus more on global information and require large computation. On the other hand, abundant speech emotional representations exist locally on different parts of the input speech. To tackle these problems, we propose a Multi-Scale TRansfomer (MSTR) for speech emotion recognition. It comprises of three main components: (1) a multi-scale temporal feature operator, (2) a fractal self-attention module, and (3) a scale mixer module. These three components can effectively enhance the transformer's ability to learn multi-scale local emotion representations. Experimental results demonstrate that the proposed MSTR model significantly outperforms a vanilla Transformer and other state-of-the-art methods across three speech emotion datasets: IEMOCAP, MELD and, CREMAD. In addition, it can greatly reduce the computational cost."
INTERSPEECH2023,emotion,A novel frequency warping scale for speech emotion recognition,0,Emotion and Mood Recognition,"Speech recognition,Computer science,Dynamic time warping,Image warping,Scale (ratio),Emotion recognition,Artificial intelligence,Physics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2023/singh23c_interspeech.pdf,"Premjeet Singh, Goutam Saha","We investigate an optimised non-linear frequency warping scale for speech emotion recognition (SER). The proposed scale maps the speech spectrogram onto another time-frequency domain which is invariant to speaker-specific variations. Generally, the famous mel-scale designed on human audio perception is considered the de facto standard of frequency warping. However, designed mainly for speech recognition, the generalisability of mel on other speech processing tasks is debatable. Our experiments show that an emotion-specific scale designed on an SER database outperforms the standard mel-scale. Along with performance improvement, the proposed approach also provides insight into the emotion-relevant frequency regions for SER. Despite the database-dependent design of our approach, we find that the scale obtained from our experiments also shows SER performance improvement when tested on two other databases."
INTERSPEECH2023,emotion,Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining,9,"Speech Recognition and Synthesis,Music and Audio Processing,Speech and Audio Processing","Computer science,Speech recognition,Speaker verification,Speaker recognition,Artificial intelligence,Natural language processing",,https://www.isca-archive.org//interspeech_2023/gao23d_interspeech.pdf,"Yuan Gao, Chenhui Chu, Tatsuya Kawahara","This paper addresses effective pretraining of automatic speech recognition (ASR) and gender recognition to improve wav2vec 2.0 embedding for speech emotion recognition (SER). Specifically, we propose a two-stage finetuning method, which first pretrains the self-supervised learning (SSL) model with ASR to learn the linguistic information and address the gradient conflict problem of conventional multi-task learning. Experimental results on the IEMOCAP dataset show that ASR pretraining can significantly outperform the simple MTL with ASR, and thus demonstrate the effectiveness of the two-stage finetuning method. We also investigate how to combine gender recognition with ASR pretraining to derive more effective embedding for SER. As the upper layers of the SSL model are focused on ASR, incorporating skip-connection can effectively embed the gender information. Compared with the single-task learning baseline, our method achieves a UA of 76.10% with an absolute improvement of 3.97%."
INTERSPEECH2023,emotion,Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models,2,Emotion and Mood Recognition,"Computer science,Natural language processing,Speech recognition,Artificial intelligence,Information retrieval",,https://www.isca-archive.org//interspeech_2023/deoliveira23_interspeech.pdf,"Danilo de Oliveira, Navin Raj Prabhu, Timo Gerkmann","In large part due to their implicit semantic modeling, self-supervised learning (SSL) methods have significantly increased the performance of valence recognition in speech emotion recognition (SER) systems. Yet, their large size may often hinder practical implementations. In this work, we take HuBERT as an example of an SSL model and analyze the relevance of each of its layers for SER. We show that shallow layers are more important for arousal recognition while deeper layers are more important for valence. This observation motivates the importance of additional textual information for accurate valence recognition, as the distilled framework lacks the depth of its large-scale SSL teacher. Thus, we propose an audio-textual distilled SSL framework that, while having only ~20% of the trainable parameters of a large SSL model, achieves on par performance across the three emotion dimensions (arousal, valence, dominance) on the MSP-Podcast v1.10 dataset."
INTERSPEECH2023,emotion,Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition,2,Speech and Audio Processing,"Computer science,Downstream (manufacturing),Emotion recognition,Speech recognition,Transfer of learning,Natural language processing,Artificial intelligence,Engineering,Operations management",,https://www.isca-archive.org//interspeech_2023/fang23b_interspeech.pdf,"Yuanbo Fang, Xiaofen Xing, Xiangmin Xu, Weibin Zhang","Huge progress has been made in self-supervised audio representation learning recently, and transformer based downstream model using Multi-head Self-Attention and Feed-Forward Network (MSA-FFN) as the basic block delivered promising transfer performance on downstream speech tasks. However, it is unclear whether the traditional transformer architecture is appropriate for downstream transfer. In this paper, we adopt a block architecture search strategy (BAS) to explore this issue, taking speech emotion recognition as an example. We found that 1) it is crucial to incorporate an FFN-like representation learning module without MSA design in the early stages of the downstream model; 2) with the use of self-supervised features, it is good enough to use a simple FFN for the downstream task. This work can serve as a source of inspiration for all other downstream speech tasks that utilize self-supervised features."
INTERSPEECH2023,affects,"Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",0,"Humor Studies and Applications,EFL/ESL Teaching and Learning","Laughter,Task (project management),Computer science,Human–computer interaction,Cognitive psychology,Psychology,Social psychology,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2023/branco23_interspeech.pdf,"Catarina Branco, Isabel Trancoso, Paulo Infante, Khiet P. Truong","Map task corpora are not typically used to study laughter, but they allow an interesting analysis of multiple factors such as familiarity between the participants, their gender, and eye contact. We conducted linear/generalized mixed-effects analysis to study if co-laughter, laughter rate, and the percentage of voiced frames in laughs are influenced by such factors. Our results show that, in conversations without eye contact, the gender of the participant was statistically relevant regarding laughter rate and the percentage of voiced frames, and the difference in gender was relevant regarding co-laughter. On the other hand, with eye contact, familiarity was statistically relevant with respect to co-laughter, laughter rate, and the percentage of voiced frames. Most of our results align and extend what has been previously found, except for voiced laughs between friends. This study emphasizes the highly variable character of laughter and its dependence on interlocutors' characteristics."
INTERSPEECH2023,emotion,Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection,0,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Emotion detection,Emotion recognition,Internet privacy,Speech recognition,Voice activity detection,Human–computer interaction,Computer security,Speech processing",,https://www.isca-archive.org//interspeech_2023/lavania23_interspeech.pdf,"Chandrashekhar Lavania, Sanjiv Das, Xin Huang, Kyu J. Han","Audio privacy has been undertaken using adversarial task training or adversarial models based on GANs, where the models also suppress scoring of other attributes (e.g., emotion, etc.), but embeddings still retain enough information to bypass speaker privacy. We use methods for feature importance from the explainability literature to modify embeddings from adversarial task training, providing a simple and accurate approach to generating embeddings for preserving speaker privacy while not attenuating utility for related tasks (e.g., emotion recognition). This enables better adherence with privacy regulations around biometrics and voiceprints, while retaining the usefulness of audio representation learning."
INTERSPEECH2023,emotion,Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations,2,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and dialogue systems","Speech recognition,Computer science,Speaker diarisation,Speaker recognition,Task (project management),Conversation,Segmentation,Word (group theory),Encoder,Natural language processing,Emotion recognition,Artificial intelligence,Voice activity detection,Speech processing,Linguistics,Communication,Psychology,Philosophy,Management,Economics,Operating system",,https://www.isca-archive.org//interspeech_2023/wu23_interspeech.pdf,"Wen Wu, Chao Zhang, Philip C. Woodland","Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD."
MM2021,emotion,HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition.,11,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Graph,Pattern recognition (psychology),Artificial intelligence,Modal,Convolutional neural network,Theoretical computer science,Chemistry,Polymer chemistry",,,"Ziyu Jia,Youfang Lin,Jing Wang,Zhiyang Feng,Xiangheng Xie,Caijie Chen",
MM2021,emotion,Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality.,16,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Modality (human–computer interaction),Multimodality,Eye movement,Electroencephalography,Artificial intelligence,Emotion recognition,Decoding methods,Speech recognition,Machine learning,Psychology,World Wide Web,Telecommunications,Psychiatry",,,"Xu Yan,Li-Ming Zhao,Bao-Liang Lu",
MM2021,emotion,Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation.,324,"AI-based Problem Solving and Planning,Action Observation and Synchronization,Child and Animal Learning Development","Computer science,Human–robot interaction,Robot,Human–computer interaction,Affordance,Social intelligence,Cognitive architecture,Artificial intelligence,Situated,Semantics (computer science),Variety (cybernetics),Cognition,Cognitive science,Psychology,Developmental psychology,Neuroscience,Programming language",,,"Feiyu Chen,Zhengxiao Sun,Deqiang Ouyang,Xueliang Liu,Jie Shao",
MM2021,emotion,Zero-shot Video Emotion Recognition via Multimodal Protagonist-aware Transformer Network.,8,"Emotion and Mood Recognition,Human Pose and Action Recognition,Video Surveillance and Tracking Methods","Discriminative model,Computer science,Transformer,Emotion recognition,Semantic space,Speech recognition,Artificial intelligence,Human–computer interaction,Voltage,Physics,Quantum mechanics",,,"Fan Qi,Xiaoshan Yang,Changsheng Xu",
MM2021,affective,Affective Color Fields: Reimagining Rothkoesque Artwork as an Interactive Companion for Artistic Self-Expression.,-1,,,,,"Aiden Kang,Liang Wang,Ziyu Zhou,Zhe Huang,Robert J. K. Jacob",
MM2022,emotion,Disentangled Representation Learning for Multimodal Emotion Recognition.,96,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Modalities,Computer science,Modality (human–computer interaction),Artificial intelligence,Feature learning,Linear subspace,Multimodal learning,Encoder,Redundancy (engineering),Representation (politics),Feature (linguistics),Subspace topology,Machine learning,Natural language processing,Mathematics,Social science,Linguistics,Philosophy,Geometry,Politics,Political science,Law,Operating system,Sociology",,,"Dingkang Yang,Shuai Huang,Haopeng Kuang,Yangtao Du,Lihua Zhang",
MM2021,affective,Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning.,58,"Human Pose and Action Recognition,Speech and dialogue systems,Social Robot Interaction and HRI","Gesture,Adversarial system,Computer science,Expression (computer science),Generative grammar,Speech recognition,Natural language processing,Artificial intelligence,Human–computer interaction,Programming language",,,"Uttaran Bhattacharya,Elizabeth Childs,Nicholas Rewkowski,Dinesh Manocha",
MM2021,emotion,SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction.,26,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Electroencephalography,Computer science,Pattern recognition (psychology),Artificial intelligence,Feature extraction,Robustness (evolution),Artificial neural network,Psychology,Biochemistry,Chemistry,Psychiatry,Gene",,,"Xiangwen Deng,Junlin Zhu,Shangming Yang",
INTERSPEECH2023,emotional,Pre-Finetuning for Few-Shot Emotional Speech Recognition,0,"Speech Recognition and Synthesis,Music and Audio Processing,Speech and Audio Processing","Computer science,Overfitting,Artificial intelligence,Generalization,Natural language processing,Speech recognition,Adaptation (eye),Transfer of learning,Machine learning,Artificial neural network,Psychology,Mathematical analysis,Mathematics,Neuroscience",,https://www.isca-archive.org//interspeech_2023/chen23b_interspeech.pdf,"Maximillian Chen, Zhou Yu","Speech models have long been known to overfit individual speakers for many classification tasks. This leads to poor generalization in settings where the speakers are out-of-domain or out-of-distribution, as is common in production environments. We view speaker adaptation as a few-shot learning problem and propose investigating transfer learning approaches inspired by recent success with pre-trained models in natural language tasks. We propose pre-finetuning speech models on difficult tasks to distill knowledge into few-shot downstream classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of four multiclass emotional speech recognition corpora and evaluate our pre-finetuned models through 33,600 few-shot fine-tuning trials on the Emotional Speech Dataset."
INTERSPEECH2023,"emotion,emotionnas",EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition,4,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Speech recognition,Architecture,Emotion recognition,Artificial intelligence,Natural language processing,History,Archaeology",,https://www.isca-archive.org//interspeech_2023/sun23d_interspeech.pdf,"Haiyang Sun, Zheng Lian, Bin Liu, Ying Li, Jianhua Tao, Licai Sun, Cong Cai, Meng Wang, Yuan Cheng","Speech emotion recognition (SER) is an important research topic in human-computer interaction. Existing works mainly rely on human expertise to design models. Despite their success, different datasets often require distinct structures and hyperparameters. Searching for an optimal model for each dataset is time-consuming and labor-intensive. To address this problem, we propose a two-stream neural architecture search (NAS) based framework, called ""EmotionNAS"". Specifically, we take two-stream features (i.e., handcrafted and deep features) as the inputs, followed by NAS to search for the optimal structure for each stream. Furthermore, we incorporate complementary information in different streams through an efficient information supplement module. Experimental results demonstrate that our method outperforms existing manually-designed and NAS-based models, setting the new state-of-the-art record."
INTERSPEECH2023,emotion,MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer,0,"Speech and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Speech recognition,Distortion (music),Computer science,Metric (unit),Noise (video),Speech enhancement,Artificial intelligence,Noise reduction,Engineering,Telecommunications,Amplifier,Operations management,Bandwidth (computing),Image (mathematics)",,https://www.isca-archive.org//interspeech_2023/wu23c_interspeech.pdf,"Ya-Tse Wu, Chi-Chun Lee","Noise-robust speech emotion recognition (SER) systems are important in real world applications. Conventionally, noise robustness is achieved by training on a noise-augmented dataset. In this work, instead of pre-defining noise SNRs to augment the clean set, we propose an augment-while-train strategy while referencing speech distortion metric. This strategy (MetricAug) constructs an augmented set per each training epoch by assessing the effect of different distortion levels have on degrading the SER performances. That is, we augment more of those noisy data that degrade the SER performance the most dynamically at each learning epoch. We evaluate our framework on two databases, MSP-Podcast and MELD. Our framework shows consistent robustness against varying levels and even unseen noise types. Further analysis reveals that by choosing STOI as the metric of noise distortion, it leads the construction of augmented sets better than metrics of PESQ and fwSNRseg."
INTERSPEECH2022,emotion,Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition,2,Emotion and Mood Recognition,"Emotion recognition,Speech recognition,Computer science,Wearable computer,Representation (politics),Fusion,Sensor fusion,Artificial intelligence,Natural language processing,Pattern recognition (psychology),Linguistics,Philosophy,Politics,Political science,Law,Embedded system",,https://www.isca-archive.org//interspeech_2022/dissanayake22_interspeech.pdf,"Vipula Dissanayake, Sachith Seneviratne, Hussel Suriyaarachchi, Elliott Wen, Suranga Nanayakkara","Even with modern-day advanced machine learning techniques, Speech Emotion Recognition (SER) is a challenging task. Speech signals alone might not provide enough information to build robust emotion recognition models. The widespread usage of wearable devices provides multiple signal streams containing physiological and contextual cues, which could be incredibly beneficial to improving an SER system. However, research around multimodal emotion recognition with wearable and speech signals is limited. Also, the scarcity of annotated data for such scenarios limits the applicability of deep learning techniques. This paper presents a self-supervised fusion method for speech and wearable signals and evaluates its usage in the SER context. We further discuss three different fusion techniques in the context of multimodal emotion recognition. Our evaluations show that pretraining in the fusion stage significantly impacts the downstream emotion recognition task. Our method was able to achieve F1 Scores of 82.59% (arousal), 83.05% (valence) and 72.95% (emotion categories) for K-EmoCon dataset."
INTERSPEECH2024,emotion,E-ODN: An Emotion Open Deep Network for Generalised and Adaptive Speech Emotion Recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Emotion recognition,Speech recognition,Computer science,Artificial intelligence,Psychology",,https://www.isca-archive.org//interspeech_2024/ma24_interspeech.pdf,"Liuxian Ma, Lin Shen, Ruobing Li, Haojie Zhang, Kun Qian, Bin Hu, Björn W. Schuller, Yoshiharu Yamamoto","Recognising the widest range of emotions possible is a major challenge in the task of Speech Emotion Recognition(SER), especially for complex and mixed emotions. However, due to the limited number of emotional types and uneven distribution of data within existing datasets, current SER models are typically trained and used in a narrow range of emotional types. In this paper, we propose the Emotion Open Deep Network(E-ODN) model to address this issue. Besides, we introduce a novel Open-Set Recognition method that maps sample emotional features into a three-dimensional emotional space. The method can infer unknown emotions and initialise new type weights, enabling the model to dynamically learn and infer emerging emotional types. The empirical results show that our recognition model outperforms the state-of-the-art(SOTA) models in dealing with multi-type unbalanced data, and it can also perform finer-grained emotion recognition."
INTERSPEECH2024,affects,Quantity-sensitivity affects recall performance of word stress,0,"Speech and dialogue systems,Intelligent Tutoring Systems and Adaptive Learning,Topic Modeling","Stress (linguistics),Sensitivity (control systems),Recall,Computer science,Word (group theory),Cognitive psychology,Psychology,Engineering,Mathematics,Electronic engineering,Linguistics,Philosophy,Geometry",,https://www.isca-archive.org//interspeech_2024/kaland24b_interspeech.pdf,"Constantijn Kaland, Maria Lialiou","Previous studies showed that word stress patterns need to be lexically stored when they are highly variable. Hence, listeners are better at memorizing stress patterns in recall tasks when their native language has variable stress patterns than when their native language has fixed stress patterns. The current study hypothesizes that in quantity-sensitive stress languages, segmental information facilitates stress perception and therefore makes smaller demands on lexical storage than in languages with quantity-insensitive stress patterns. This prediction is tested in two stress recall tasks with German (var. stress, q-sensitive) and Greek (var. stress, q-insensitive) listeners. Results show that German listeners indeed perform worse than Greek listeners. The outcomes provide an important novel perspective on the interaction between segmental and suprasegmental information in speech perception and nuance lexical statistics accounts of word stress."
ACL2023,affect,How Do In-Context Examples Affect Compositional Generalization?,9,"Topic Modeling,Neurobiology of Language and Bilingualism,Computational and Text Analysis Methods","Generalization,Zhàng,Chen,Context (archaeology),Affect (linguistics),Computer science,Linguistics,Natural language processing,Epistemology,History,Philosophy,Geology,China,Archaeology,Paleontology",,https://aclanthology.org/2023.acl-long.618.pdf,"Shengnan An,Zeqi Lin,Qiang Fu,Bei Chen,Nanning Zheng,Jian-Guang Lou,Dongmei Zhang","Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm."
ACL2023,emotion,Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,21,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Anomaly Detection Techniques and Applications","Adversarial system,Computer science,Artificial intelligence,Robustness (evolution),Class (philosophy),Machine learning,Context (archaeology),Consistency (knowledge bases),Feature learning,Boosting (machine learning),Feature (linguistics),Natural language processing,Pattern recognition (psychology),Linguistics,Paleontology,Biochemistry,Chemistry,Philosophy,Biology,Gene",,https://aclanthology.org/2023.acl-long.606.pdf,"Dou Hu,Yinan Bao,Lingwei Wei,Wei Zhou,Songlin Hu","Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model’s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT."
INTERSPEECH2024,emotion,Depression Enhances Internal Inconsistency between Spoken and Semantic Emotion: Evidence from the Analysis of Emotion Expression in Conversation,0,"Emotional Intelligence and Performance,Educational and Psychological Assessments","Conversation,Expression (computer science),Psychology,Computer science,Linguistics,Depression (economics),Natural language processing,Cognitive psychology,Communication,Philosophy,Economics,Macroeconomics,Programming language",,https://www.isca-archive.org//interspeech_2024/wu24j_interspeech.pdf,"Xinyi Wu, Changqing Xu, Nan Li, Rongfeng Su, Lan Wang, Nan Yan","Spoken emotion and semantic emotion are two components of emotion expression. In human conversation, emotions expressed by these two modalities are similar in healthy individuals. However, rich evidence documents that depression might affect emotional expression. Nevertheless, the consistency between spoken and semantic emotion in depressed patients has rarely been studied previously. In the present study, we investigated the consistency between emotions expressed by acoustical features and text content in depressed and healthy individuals during natural conversations. It was found that depressed patients tended to talk about negative topics in a neutral emotional tone and talk about neutral or positive topics in a depressed tone. These findings suggest that depression not only affects the emotion expression of a single modality but also results in an inconsistency between emotions expressed by these two modalities."
INTERSPEECH2024,"emotion,emotional",Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction and Recognition in Conversation,0,Sentiment Analysis and Opinion Mining,"Conversation,Emotion recognition,Modal,Computer science,Fusion,Emotion detection,Speech recognition,Extraction (chemistry),Feature extraction,Artificial intelligence,Sensor fusion,Psychology,Natural language processing,Communication,Linguistics,Chemistry,Philosophy,Chromatography,Polymer chemistry",,https://www.isca-archive.org//interspeech_2024/shi24d_interspeech.pdf,"Haoxiang Shi, Ziqi Liang, Jun Yu","Emotion Prediction in Conversation (EPC) aims to forecast the emotions of forthcoming utterances by utilizing preceding dialogues. Previous EPC approaches relied on simple context modeling for emotion extraction, overlooking fine-grained emotion cues at the word level. Additionally, prior works failed to account for the intrinsic differences between modalities, resulting in redundant information. To overcome these limitations, we propose an emotional cues extraction and fusion network, which consists of two stages: a modality-specific learning stage that utilizes word-level labels and prosody learning to construct emotion embedding spaces for each modality, and a two-step fusion stage for integrating multi-modal features. Moreover, the emotion features extracted by our model are also applicable to the Emotion Recognition in Conversation (ERC) task. Experimental results validate the efficacy of the proposed method, demonstrating superior performance on both IEMOCAP and MELD datasets."
INTERSPEECH2024,emotion,MM-NodeFormer: Node Transformer Multimodal Fusion for Emotion Recognition in Conversation,0,Emotion and Mood Recognition,"Conversation,Computer science,Emotion recognition,Speech recognition,Transformer,Fusion,Node (physics),Natural language processing,Artificial intelligence,Human–computer interaction,Psychology,Engineering,Communication,Linguistics,Electrical engineering,Voltage,Philosophy,Structural engineering",,https://www.isca-archive.org//interspeech_2024/huang24b_interspeech.pdf,"Zilong Huang, Man-Wai Mak, Kong Aik Lee","Emotion Recognition in Conversation (ERC) has great prospects in human-computer interaction and medical consultation. Existing ERC approaches mainly focus on information in the text and speech modalities and often concatenate multimodal features without considering the richness of emotional information in individual modalities. We propose a multimodal network called MM-NodeFormer for ERC to address this issue. The network leverages the characteristics of different Transformer encoding stages to fuse the emotional features from the text, audio, and visual modalities according to their emotional richness. The module considers text as the main modality and audio and visual as auxiliary modalities, leveraging the complementarity between the main and auxiliary modalities. We conducted extensive experiments on two public benchmark datasets, IEMOCAP and MELD, achieving an accuracy of 74.24% and 67.86%, respectively, significantly higher than many state-of-the-art approaches."
INTERSPEECH2021,emotion,Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech,16,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Speech recognition,Categorical variable,Computer science,Emotion recognition,Artificial neural network,Natural language processing,Artificial intelligence,Machine learning",,https://www.isca-archive.org//interspeech_2021/keesing21_interspeech.pdf,"Aaron Keesing, Yun Sing Koh, Michael Witbrock","Many features have been proposed for use in speech emotion recognition, from signal processing features to bag-of-audio-words (BoAW) models to abstract neural representations. Some of these feature types have not been directly compared across a large number of speech corpora to determine performance differences. We propose a full factorial design and to compare speech processing features, BoAW and neural representations on 17 emotional speech datasets. We measure the performance of features in a categorical emotion classification problem for each dataset, using speaker-independent cross-validation with diverse classifiers. Results show statistically significant differences between features and between classifiers, with large effect sizes between features. In particular, standard acoustic feature sets still perform competitively to neural representations, while neural representations have a larger range of performance, and BoAW features lie in the middle. The best and worst neural representations were wav2veq and VGGish, respectively, with wav2vec performing best out of all tested features. These results indicate that standard acoustic feature sets are still very useful baselines for emotional classification, but high quality neural speech representations can be better."
INTERSPEECH2021,emotion,Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition,13,"Emotion and Mood Recognition,Music and Audio Processing,Speech Recognition and Synthesis","Speech recognition,Computer science,Duration (music),Emotion recognition,Artificial neural network,Time delay neural network,Hidden Markov model,Pattern recognition (psychology),Artificial intelligence,Acoustics,Physics",,https://www.isca-archive.org//interspeech_2021/kumawat21_interspeech.pdf,"Pooja Kumawat, Aurobinda Routray","We have analyzed the Time Delay Neural Network (TDNN) based architectures for speech emotion classification. TDNN models efficiently capture the temporal information and provide an utterance level prediction. Emotions are dynamic in nature and require temporal context for reliable prediction. In our work, we have applied the TDNN based x-vector and emphasized channel attention, propagation & aggregation based TDNN (ECAPA-TDNN) architectures for speech emotion identification with RAVDESS, Emo-DB, and IEMOCAP databases. The results show that the TDNN architectures are very efficient for predicting emotion classes and ECAPA-TDNN outperforms the TDNN based x-vector architecture. Next, we investigated the performance of ECAPA-TDNN with various training chunk durations and test utterance durations. We have identified that in spite of very promising emotion recognition performance the TDNN models have a strong training chunk duration-based bias. Earlier research work revealed that individual emotion class accuracy depends largely on the test utterance duration. Most of these studies were based on frame level emotions predictions. However, utterance level based emotion recognition is relatively less explored. The results show that even with the TDNN models, the accuracy of the different emotion classes is dependent on the utterance duration."
INTERSPEECH2021,emotion,Graph Isomorphism Network for Speech Emotion Recognition,15,"Advanced Graph Neural Networks,Recommender Systems and Techniques,Multimodal Machine Learning Applications","Computer science,Graph isomorphism,Subgraph isomorphism problem,Induced subgraph isomorphism problem,Isomorphism (crystallography),Speech recognition,Emotion recognition,Graph,Artificial intelligence,Theoretical computer science,Line graph,Voltage graph,Chemistry,Crystal structure,Crystallography",,https://www.isca-archive.org//interspeech_2021/liu21k_interspeech.pdf,"Jiawang Liu, Haoxiang Wang","Previous deep learning approaches such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) have been broadly used in speech emotion recognition (SER). In these approaches, speech signals are generally modeled in the Euclidean space. In this paper, a novel SER model (LSTM-GIN) is proposed, which applies Graph Isomorphism Network (GIN) on LSTM outputs for global emotion modeling in the non-Euclidean space. In our LSTM-GIN model, speech signals are represented as graph-structured data so that we can better extract global feature representation. The deep frame-level features generated from the bidirectional LSTM are converted into an undirected graph with nodes represented by frame-level features and connections defined according to temporal relations between speech frames. GIN is adopted to classify the graph representations of utterances, as it is proved of excellent discriminative power in comparative experiments. We conduct experiments on the IEMOCAP dataset, and the results show that our proposed LSTM-GIN model surpasses other recent graph-based models and deep learning models by achieving 64.65% of weighted accuracy (WA) and 65.53% of unweighted accuracy (UA)."
INTERSPEECH2021,emotion,Emotion Recognition from Speech Using wav2vec 2.0 Embeddings,216,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Speech recognition,Computer science,Emotion recognition,Natural language processing",,https://www.isca-archive.org//interspeech_2021/pepino21_interspeech.pdf,"Leonardo Pepino, Pablo Riera, Luciana Ferrer","Emotion recognition datasets are relatively small, making the use of deep learning techniques challenging. In this work, we propose a transfer learning method for speech emotion recognition (SER) where features extracted from pre-trained wav2vec 2.0 models are used as input to shallow neural networks to recognize emotions from speech. We propose a way to combine the output of several layers from the pre-trained model, producing richer speech representations than the model’s output alone. We evaluate the proposed approaches on two standard emotion databases, IEMOCAP and RAVDESS, and compare different feature extraction techniques using two wav2vec 2.0 models: a generic one, and one finetuned for speech recognition. We also experiment with different shallow architectures for our speech emotion recognition model, and report baseline results using traditional features. Finally, we show that our best performing models have better average recall than previous approaches that use deep neural networks trained on spectrograms and waveforms or shallow neural networks trained on features extracted from wav2vec 1.0."
INTERSPEECH2021,emotion,Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition,8,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Domain (mathematical analysis),Transfer of learning,Emotion recognition,Scripting language,Domain adaptation,Context (archaeology),Adaptation (eye),Artificial intelligence,Natural language processing,Speech recognition,Psychology,Mathematics,Neuroscience,Classifier (UML),Paleontology,Biology,Operating system,Mathematical analysis",,https://www.isca-archive.org//interspeech_2021/li21k_interspeech.pdf,"Haoqi Li, Yelin Kim, Cheng-Hao Kuo, Shrikanth S. Narayanan","Key challenges in developing generalized automatic emotion recognition systems include scarcity of labeled data and lack of gold-standard references. Even for the cues that are labeled as the same emotion category, the variability of associated expressions can be high depending on the elicitation context e.g., emotion elicited during improvised conversations vs. acted sessions with predefined scripts. In this work, we regard the emotion elicitation approach as domain knowledge, and explore domain transfer learning techniques on emotional utterances collected under different emotion elicitation approaches, particularly with limited labeled target samples. Our emotion recognition model combines the gradient reversal technique with an entropy loss function as well as the softlabel loss, and the experiment results show that domain transfer learning methods can be employed to alleviate the domain mismatch between different elicitation approaches. Our work provides new insights into emotion data collection, particularly the impact of its elicitation strategies, and the importance of domain adaptation in emotion recognition aiming for generalized systems."
INTERSPEECH2021,emotion,Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition,9,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Speech recognition,Process (computing),Regression,Emotion recognition,Artificial intelligence,Natural language processing,Statistics,Mathematics,Operating system",,https://www.isca-archive.org//interspeech_2021/t21_interspeech.pdf,"Mani Kumar T, Enrique Sanchez, Georgios Tzimiropoulos, Timo Giesbrecht, Michel Valstar","In this work, we pose continuous apparent emotion recognition from speech as a problem of learning distributions of functions, and do so using Stochastic Processes Regression. We presume that the relation between speech signals and their corresponding emotion labels is governed by some underlying stochastic process, in contrast to existing speech emotion recognition methods that are mostly based on deterministic regression models (static or recurrent). We treat each training sequence as an instance of the underlying stochastic process which we aim to discover using a neural latent variable model, which approximates the distribution of functions with a stochastic latent variable using an encoder-decoder composition: the encoder infers the distribution over the latent variable, which the decoder uses to predict the distribution of output emotion labels. To this end, we build on the previously proposed Neural Processes theory by using (a). noisy label predictions of a backbone instead of ground truth labels for latent variable inference and (b). recurrent encoder-decoder models to alleviate the effect of commonly encountered temporal misalignment between audio features and emotion labels due to annotator reaction lag. We validated our method on AVEC’19 cross-cultural emotion recognition dataset, achieving state-of-the-art results."
INTERSPEECH2021,emotional,Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit,6,"Infant Health and Development,Language Development and Disorders,Infant Development and Preterm Care","Computer science,Valence (chemistry),Speech recognition,Recall,Arousal,Generalization,Binary classification,Artificial intelligence,Adaptation (eye),Context (archaeology),Machine learning,Natural language processing,Cognitive psychology,Psychology,Social psychology,Mathematical analysis,Mathematics,Paleontology,Physics,Quantum mechanics,Neuroscience,Support vector machine,Biology",,https://www.isca-archive.org//interspeech_2021/vaaras21_interspeech.pdf,"Einari Vaaras, Sari Ahlqvist-Björkroth, Konstantinos Drossos, Okko Räsänen","Researchers have recently started to study how the emotional speech heard by young infants can affect their developmental outcomes. As a part of this research, hundreds of hours of daylong recordings from preterm infants’ audio environments were collected from two hospitals in Finland and Estonia in the context of so-called APPLE study. In order to analyze the emotional content of speech in such a massive dataset, an automatic speech emotion recognition (SER) system is required. However, there are no emotion labels or existing in-domain SER systems to be used for this purpose. In this paper, we introduce this initially unannotated large-scale real-world audio dataset and describe the development of a functional SER system for the Finnish subset of the data. We explore the effectiveness of alternative state-of-the-art techniques to deploy a SER system to a new domain, comparing cross-corpus generalization, WGAN-based domain adaptation, and active learning in the task. As a result, we show that the best-performing models are able to achieve a classification performance of 73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification for valence and arousal, respectively. The results also show that active learning achieves the most consistent performance compared to the two alternatives."
INTERSPEECH2021,emotion,Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition,25,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining","Modality (human–computer interaction),Speech recognition,Computer science,Emotion recognition,Artificial intelligence,Natural language processing",,https://www.isca-archive.org//interspeech_2021/li21j_interspeech.pdf,"Hang Li, Wenbiao Ding, Zhongqin Wu, Zitao Liu","Speech emotion recognition is a challenging task because the emotion expression is complex, multimodal and fine-grained. In this paper, we propose a novel multimodal deep learning approach to perform fine-grained emotion recognition from real-life speeches. We design a temporal alignment mean-max pooling mechanism to capture the subtle and fine-grained emotions implied in every utterance. In addition, we propose a cross modality excitement module to conduct sample-specific adjustment on cross modality embeddings and adaptively recalibrate the corresponding values by its aligned latent features from the other modality. Our proposed model is evaluated on two well-known real-world speech emotion recognition datasets. The results demonstrate that our approach is superior on the prediction tasks for multimodal speech utterances, and it outperforms a wide range of baselines in terms of prediction accuracy. Furthermore, we conduct detailed ablation studies to show that our temporal alignment mean-max pooling mechanism and cross modality excitement significantly contribute to the promising results. In order to encourage the research reproducibility, we make the code publicly available."
INTERSPEECH2021,emotion,Temporal Context in Speech Emotion Recognition,21,Emotion and Mood Recognition,"Computer science,Speech recognition,Context (archaeology),Emotion recognition,Natural language processing,Artificial intelligence,History,Archaeology",,https://www.isca-archive.org//interspeech_2021/xia21b_interspeech.pdf,"Yangyang Xia, Li-Wei Chen, Alexander Rudnicky, Richard M. Stern","We investigate the importance of temporal context for speech emotion recognition (SER). Two SER systems trained on traditional and learned features, respectively, are developed to predict categorical labels of emotion. For traditional acoustical features, we study the combination of filterbank features and prosodic features and the impact on SER when the temporal context of these features is expanded by learnable spectro-temporal receptive fields (STRFs). Experiments show that the system trained on learnable STRFs outperforms other reported systems evaluated with a similar setup. We also demonstrate that the wav2vec features, pretrained with long temporal context, are superior to traditional features. We then introduce a novel segment-based learning objective to constrain our classifier to extract local emotion features from the large temporal context. Combined with the learning objective and fine-tuning strategy, our top-line system using wav2vec features reaches state-of-the-art performance on the IEMOCAP dataset."
EMNLP2024,emotion,Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health,0,"Mental Health via Writing,Sentiment Analysis and Opinion Mining","Granularity,Aggregate (composite),Computer science,Mental health,Psychology,Psychiatry,Materials science,Composite material,Operating system",,https://aclanthology.org/2024.emnlp-main.1069.pdf,"Krishnapriya Vishnubhotla,Daniela Teodorescu,Mallory J Feldman,Kristen Lindquist,Saif M. Mohammad","We are united in how emotions are central to shaping our experiences; yet, individuals differ greatly in how we each identify, categorize, and express emotions. In psychology, variation in the ability of individuals to differentiate between emotion concepts is called emotion granularity (determined through self-reports of one’s emotions). High emotion granularity has been linked with better mental and physical health; whereas low emotion granularity has been linked with maladaptive emotion regulation strategies and poor health outcomes. In this work, we propose computational measures of emotion granularity derived from temporally-ordered speaker utterances in social media (in lieu of self reports that suffer from various biases). We then investigate the effectiveness of such text-derived measures of emotion granularity in functioning as markers of various mental health conditions (MHCs). We establish baseline measures of emotion granularity derived from textual utterances, and show that, at an aggregate level, emotion granularities are significantly lower for people self-reporting as having an MHC than for the control population. This paves the way towards a better understanding of the MHCs, and specifically the role emotions play in our well-being."
EMNLP2024,emoji,Semantics and Sentiment: Cross-lingual Variations in Emoji Use,0,"Digital Communication and Language,Linguistics, Language Diversity, and Identity","Emoji,Computer science,Sentiment analysis,Semantics (computer science),Natural language processing,Artificial intelligence,Information retrieval,World Wide Web,Social media,Programming language",,https://aclanthology.org/2024.emnlp-main.1041.pdf,"Giulio Zhou,Sydelle De Souza,Ella Markham,Oghenetekevwe Kwakpovwe,Sumin Zhao","Over the past decade, the use of emojis in social media has seen a rapid increase. Despite their popularity and image-grounded nature, previous studies have found that people interpret emojis inconsistently when presented in context and in isolation. In this work, we explore whether emoji semantics differ across languages and how semantics interacts with sentiment in emoji use across languages. To do so, we developed a corpus containing the literal meanings for a set of emojis, as defined by L1 speakers in English, Portuguese and Chinese. We then use these definitions to assess whether speakers of different languages agree on whether an emoji is being used literally or figuratively in the context where they are grounded in, as well as whether this literal and figurative use correlates with the sentiment of the context itself. We found that there were varying levels of disagreement on the definition for each emoji but that these stayed fairly consistent across languages. We also demonstrated a correlation between the sentiment of a tweet and the figurative use of an emoji, providing theoretical underpinnings for empirical results in NLP tasks, particularly offering insights that can benefit sentiment analysis models."
WWW2021,emotion,Mining Dual Emotion for Fake News Detection.,166,"Misinformation and Its Impacts,Sentiment Analysis and Opinion Mining,Spam and Phishing Detection","Emotion detection,Dual (grammatical number),Computer science,Focus (optics),Arousal,Fake news,Set (abstract data type),Social media,Sentiment analysis,Task (project management),Emotion classification,Dual role,Psychology,Emotion recognition,Artificial intelligence,Social psychology,Internet privacy,World Wide Web,Linguistics,Philosophy,Physics,Chemistry,Management,Optics,Economics,Combinatorial chemistry,Programming language",,,"Xueyao Zhang,Juan Cao,Xirong Li,Qiang Sheng,Lei Zhong,Kai Shu",
WWW2022,affects,To Trust or Not To Trust: How a Conversational Interface Affects Trust in a Decision Support System.,28,"Mobile Crowdsensing and Crowdsourcing,Human-Automation Interaction and Safety,Personal Information Management and User Behavior","Computer science,User interface,Interface (matter),Decision support system,Human–computer interaction,Crowdsourcing,Graphical user interface,Set (abstract data type),World Wide Web,Artificial intelligence,Bubble,Maximum bubble pressure method,Operating system,Parallel computing,Programming language",,,"Akshit Gupta,Debadeep Basu,Ramya Ghantasala,Sihang Qiu,Ujwal Gadiraju",
WWW2022,emotions,Moral Emotions Shape the Virality of COVID-19 Misinformation on Social Media.,40,"Misinformation and Its Impacts,Media Influence and Politics,Opinion Dynamics and Social Influence","Misinformation,Rumor,Social media,Coronavirus disease 2019 (COVID-19),Psychology,Fake news,Social psychology,Social distance,False accusation,Emotional contagion,Politics,Pandemic,Deception,Internet privacy,Computer science,Political science,Medicine,Computer security,Public relations,Disease,Pathology,World Wide Web,Infectious disease (medical specialty),Law",,,"Kirill Solovev,Nicolas Pröllochs",
WWW2023,emotion,A Multi-task Model for Emotion and Offensive Aided Stance Detection of Climate Change Tweets.,8,"Sentiment Analysis and Opinion Mining,Misinformation and Its Impacts,Social Media and Politics","Offensive,Task (project management),Computer science,Emotion detection,Change detection,Human–computer interaction,Artificial intelligence,Emotion recognition,Engineering,Operations research,Systems engineering",,,"Apoorva Upadhyaya,Marco Fisichella,Wolfgang Nejdl",
INTERSPEECH2023,emotion,Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition,1,"Emotion and Mood Recognition,Face recognition and analysis","Adaptation (eye),Emotion recognition,Computer science,Speech recognition,Resource (disambiguation),Psychology,Computer network,Neuroscience",,https://www.isca-archive.org//interspeech_2023/cahyawijaya23_interspeech.pdf,"Samuel Cahyawijaya, Holy Lovenia, Willy Chung, Rita Frieske, Zihan Liu, Pascale Fung","Speech emotion recognition plays a crucial role in human-computer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages--English, Mandarin Chinese, and Cantonese; and 2 different age groups--adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability."
INTERSPEECH2023,emotion,Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection,1,"Emotion and Mood Recognition,Speech Recognition and Synthesis","Computer science,Stress (linguistics),Speech recognition,Feature selection,Selection (genetic algorithm),Filter (signal processing),Artificial intelligence,Emotion recognition,Feature (linguistics),Natural language processing,Feature extraction,Pattern recognition (psychology),Linguistics,Computer vision,Philosophy",,https://www.isca-archive.org//interspeech_2023/tabassum23_interspeech.pdf,"Nowshin Tabassum, Tasfia Tabassum, Fardin Saad, Tahiya Sultana Safa, Hasan Mahmud, Md. Kamrul Hasan","In Speech Emotion Recognition (SER), significant progress has been made. Despite cutting-edge developments, faultless human-computer interaction remains a distant goal since established SoTA models cannot perceive the speaker's emotional state flawlessly. On the contrary, several studies in SER uncovered the possibility of language and culture-specific differences in this domain. Emotion recognition in speech can vary from person to person based on age, gender, language, and accent, amongst others. In this study, we explore and investigate how assorted accents of the English language influence SER. We employ four different English accents: American, British, Canadian, and Bengali English. Then we extracted a subset of best-performing accent-neutral features by incorporating filter and wrapper-based feature selection methods. Our investigations reveal that pitch, intensity, and MFCC-related features more effectively recognize emotions regardless of accent."
INTERSPEECH2022,emotion,Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation,12,"Speech Recognition and Synthesis,Speech and Audio Processing","Naturalness,Computer science,Speech recognition,Variety (cybernetics),Similarity (geometry),Speech synthesis,Artificial intelligence,Physics,Quantum mechanics,Image (mathematics)",,https://www.isca-archive.org//interspeech_2022/terashima22_interspeech.pdf,"Ryo Terashima, Ryuichi Yamamoto, Eunwoo Song, Yuma Shirahata, Hyun-Wook Yoon, Jae-Min Kim, Kentaro Tachibana","Data augmentation via voice conversion (VC) has been successfully applied to low-resource expressive text-to-speech (TTS) when only neutral data for the target speaker are available. Although the quality of VC is crucial for this approach, it is challenging to learn a stable VC model because the amount of data is limited in low-resource scenarios, and highly expressive speech has large acoustic variety. To address this issue, we propose a novel data augmentation method that combines pitch-shifting and VC techniques. Because pitch-shift data augmentation enables the coverage of a variety of pitch dynamics, it greatly stabilizes training for both VC and TTS models, even when only 1,000 utterances of the target speaker's neutral data are available. Subjective test results showed that a FastSpeech 2-based emotional TTS system with the proposed method improved naturalness and emotional similarity compared with conventional methods."
INTERSPEECH2022,affects,Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS),4,"Speech Recognition and Synthesis,Topic Modeling,Natural Language Processing Techniques","Polyglot,Computer science,Natural language processing,Feature (linguistics),Representation (politics),Artificial intelligence,Speech recognition,Divide and conquer algorithms,Speech synthesis,Linguistics,Programming language,Philosophy,Politics,Political science,Law",,https://www.isca-archive.org//interspeech_2022/sanchez22_interspeech.pdf,"Ariadna Sanchez, Alessio Falai, Ziyao Zhang, Orazio Angelini, Kayoko Yanagisawa","An essential design decision for multilingual Neural Text-To-Speech (NTTS) systems is how to represent input linguistic features within the model. Looking at the wide variety of approaches in the literature, two main paradigms emerge, unified and separate representations. The former uses a shared set of phonetic tokens across languages, whereas the latter uses unique phonetic tokens for each language. In this paper, we conduct a comprehensive study comparing multilingual NTTS systems models trained with both representations. Our results reveal that the unified approach consistently achieves better crosslingual synthesis with respect to both naturalness and accent. Separate representations tend to have an order of magnitude more tokens than unified ones, which may affect model capacity. For this reason, we carry out an ablation study to understand the interaction of the representation type with the size of the token embedding. We find that the difference between the two paradigms only emerges above a certain threshold embedding size. This study provides strong evidence that unified representations should be the preferred paradigm when building multilingual NTTS systems."
IJCAI2024,emotional,AVIN-Chat: An Audio-Visual Interactive Chatbot System with Emotional State Tuning,0,Multimedia Communication and Technology,"Chatbot,Computer science,Human–computer interaction,Multimedia,Audio visual,Immersion (mathematics),World Wide Web,Mathematics,Pure mathematics",https://www.youtube.com/watch?v=Z74uIV9k7_k.,https://www.ijcai.org/proceedings/2024/1027.pdf,"Chanhyuk Park, Jungbin Cho, Junwan Kim, Seongmin Lee, Jungsu Kim, Sanghoon Lee","This work presents an audio-visual interactive chatbot (AVIN-Chat) system that allows users to have face-to-face conversations with 3D avatars in real-time. Compared to the previous chatbot services, which provide text-only or speech-only communications, the proposed AVIN-Chat can offer audio-visual communications providing users with a superior experience quality. In addition, the proposed AVIN-Chat emotionally speaks and expresses according to the user's emotional state. Thus, it enables users to establish a strong bond with the chatbot system, increasing the user's immersion. Through user subjective tests, it is demonstrated that the proposed system provides users with a higher sense of immersion than previous chatbot systems. The demonstration video is available at https://www.youtube.com/watch?v=Z74uIV9k7_k."
IJCAI2024,emotion,"NEGOTIATOR: A Comprehensive Framework for Human-Agent Negotiation Integrating Preferences, Interaction, and Emotion",0,Multi-Agent Systems and Negotiation,"Negotiation,Computer science,Human interaction,Human–computer interaction,Multi-agent system,Artificial intelligence,Sociology,Social science",,https://www.ijcai.org/proceedings/2024/1012.pdf,"Mehmet Onur Keskin, Berk Buzcu, Berkecan Koçyiğit, Umut Çakan, Anıl Doğru, Reyhan Aydoğan","The paper introduces a comprehensive human-agent negotiation framework designed to facilitate the development and evaluation of research studies on human-agent negotiation without building each component from scratch. Leveraging the interoperability and reusability of its components, this framework offers various functionalities, including speech-to-text conversion, emotion recognition, a repository of negotiation strategies, and an interaction manager capable of managing gestures designed for Nao, Pepper, and QT, and coordinating message exchanges in a turn-taking fashion. This framework aims to lower the entry barrier for researchers in human-agent negotiation by providing a versatile platform that supports a wide range of research directions, including affective computing, natural language processing, decision-making, and non-verbal communication."
IJCAI2022,emotion,"Automatic Multimodal Emotion Recognition Using Facial Expression, Voice, and Text",0,"Emotion and Mood Recognition,Social Robot Interaction and HRI,Face recognition and analysis","Facial expression,Computer science,Empathy,Multimodality,Human–computer interaction,Expression (computer science),Emotion recognition,Representation (politics),Speech recognition,Dialog system,Natural language processing,Artificial intelligence,Psychology,World Wide Web,Dialog box,Psychiatry,Politics,Political science,Law,Programming language",,https://www.ijcai.org/proceedings/2022/0843.pdf,Hélène Tran,"It has been a long-time dream for humans to interact with a machine as we would with a person, in a way that it understands us, advises us, and looks after us with no human supervision. Despite being efficient on logical reasoning, current advanced systems lack empathy and user understanding. Estimating the user's emotion could greatly help the machine to identify the user's needs and adapt its behaviour accordingly. This research project aims to develop an automatic emotion recognition system based on facial expression, voice, and words. We expect to address the challenges related to multimodality, data complexity, and emotion representation."
ACL2024,emotional,Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation,2,Mental Health via Writing,"Supporter,Conversation,Preference,Computer science,Psychology,Communication,Microeconomics,Archaeology,Economics,History",,https://aclanthology.org/2024.acl-long.813.pdf,"Dongjin Kang,Sunghwan Kim,Taeyoon Kwon,Seungjun Moon,Hyunsouk Cho,Youngjae Yu,Dongha Lee,Jinyoung Yeo","Emotional Support Conversation (ESC) is a task aimed at alleviating individuals’ emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs."
ACL2023,emotion,Unsupervised Extractive Summarization of Emotion Triggers,2,"Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques,Topic Modeling","Automatic summarization,Computer science,Context (archaeology),Artificial intelligence,Code (set theory),Labeled data,Natural language processing,PageRank,Machine learning,Data science,Information retrieval,Paleontology,Set (abstract data type),Biology,Programming language",,https://aclanthology.org/2023.acl-long.531.pdf,"Tiberiu Sosea,Hongli Zhan,Junyi Jessy Li,Cornelia Caragea","Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting (Zhan et al., 2022)’s abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from external sources combined with a language understanding module, and outperforms strong baselines. We release our data and code at "
INTERSPEECH2021,emotional,A Prototypical Network Approach for Evaluating Generated Emotional Speech,1,"Speech and Audio Processing,Music and Audio Processing,Music Technology and Sound Studies","Computer science,Spectrogram,Context (archaeology),Similarity (geometry),Set (abstract data type),Class (philosophy),Artificial intelligence,Embedding,Speech recognition,Data set,Natural language processing,Machine learning,Paleontology,Image (mathematics),Biology,Programming language",,https://www.isca-archive.org//interspeech_2021/baird21_interspeech.pdf,"Alice Baird, Silvan Mertes, Manuel Milling, Lukas Stappen, Thomas Wiest, Elisabeth André, Björn W. Schuller","The collection of emotional speech data is a time-consuming and costly endeavour. Generative networks can be applied to augment the limited audio data artificially. However, it is challenging to evaluate generated audio for its similarity to source data, as current quantitative metrics are not necessarily suited to the audio domain. We explore the use of a prototypical network to evaluate four classes of generated emotional audio with this in mind. We first extract spectrogram images from  WaveGan generated audio and other audio augmentation approaches, comparing similarity to the class prototype and diversity within the embedding space. Furthermore, we augment the source training set with each augmentation type and perform a classification to explore the generated audio plausibility. Results suggest that quality and diversity can be quantitatively observed with this approach. In the chosen context, we see that  WaveGan generated data is recognisable as a source data class (F"
INTERSPEECH2023,emotion,Emotion Prompting for Speech Emotion Recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Emotion recognition,Speech recognition,Computer science,Psychology",,https://www.isca-archive.org//interspeech_2023/zhou23f_interspeech.pdf,"Xingfa Zhou, Min Li, Lan Yang, Rui Sun, Xin Wang, Huayi Zhan","Speech Emotion Recognition (SER) classifies speech into emotion categories such as: Happy, Angry. Most prior works for SER focused on how to mine compelling features to improve performance. However, these methods ignore the influence of emotional label information on SER. Recent studies have attempted to prompt pre-trained language models and yield good performance for NLP tasks. Nevertheless, few works have attempted to prompt pre-trained speech models (PSM) on speech tasks. In light of these, we propose a simple but effective prompt-based method that prompts PSM for SER. Firstly, we reframe SER as an entailment task. Next, we generate speech prompts and combine them with the raw audio to form the input for PSM. Finally, we build a multi-task learning framework to extract more compelling features by simultaneously performing automatic speech recognition (ASR) and SER. Experiments on the IEMOCAP benchmark show that our method outperforms state-of-the-art baselines on the SER task."
INTERSPEECH2022,emotional,Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion,16,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Style (visual arts),Identity (music),Computer science,Speech recognition,Speaker recognition,Linguistics,Art,Aesthetics,Philosophy,Literature",,https://www.isca-archive.org//interspeech_2022/du22c_interspeech.pdf,"Zongyang Du, Berrak Sisman, Kun Zhou, Haizhou Li","Expressive voice conversion performs identity conversion for emotional speakers by jointly converting speaker identity and emotional style. Due to the hierarchical structure of speech emotion, it is challenging to disentangle the emotional style for different speakers. Inspired by the recent success of speaker disentanglement with variational autoencoder (VAE), we propose an any-to-any expressive voice conversion framework, that is called StyleVC. StyleVC is designed to disentangle linguistic content, speaker identity, pitch, and emotional style information. We study the use of style encoder to model emotional style explicitly. At run-time, StyleVC converts both speaker identity and emotional style for arbitrary speakers. Experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations."
INTERSPEECH2024,emotion,Hierarchical Distribution Adaptation for Unsupervised Cross-corpus Speech Emotion Recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Adaptation (eye),Emotion recognition,Speech recognition,Natural language processing,Artificial intelligence,Psychology,Neuroscience",,https://www.isca-archive.org//interspeech_2024/lu24e_interspeech.pdf,"Cheng Lu, Yuan Zong, Yan Zhao, Hailun Lian, Tianhua Qi, Björn Schuller, Wenming Zheng","The primary issue of unsupervised cross-corpus speech emotion recognition (SER) is that domain shift between the training and testing data undermines the SER model’s ability to generalize on unknown testing datasets. In this paper, we propose a straightforward and effective strategy, called Hierarchical Distribution Adaptation (HDA), to address the domain bias issue. HDA leverages a hierarchical emotion representation module based on nested Transformers to extract speech emotion features at different levels (e.g., frame/segment/utterance-level), for capturing multiple-scale emotion correlations in speech. Furthermore, a hierarchical distribution adaptation module, including frame-level distribution adaptation (FDA), segment- level distribution adaptation (SDA), and utterance-level distribution adaptation (UDA), is developed to align the hierarchical-level emotion representations of the training and testing speech samples to effectively eliminate domain discrepancy. Extensive experimental results demonstrate the superiority of our proposed HDA over other state-of-the art (SOTA) methods."
INTERSPEECH2024,emotion,"Keep, Delete, or Substitute: Frame Selection Strategy for Noise-Robust Speech Emotion Recognition",0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Selection (genetic algorithm),Noise (video),Frame (networking),Artificial intelligence,Pattern recognition (psychology),Telecommunications,Image (mathematics)",,https://www.isca-archive.org//interspeech_2024/leem24_interspeech.pdf,"Seong-Gyun Leem, Daniel Fulford, Jukka-Pekka Onnela, David Gard, Carlos Busso","Speech emotion recognition (SER) system can exploit an Speech enhancement (SE) model to increase its noise robustness by suppressing the background noise. However, SE could also suppress emotionally discriminative features, affecting the emotion prediction. We propose an alternative framework, Keep or Delete (KoD), to keep the information of the original speech while minimizing the influence of background noise. We train a frame reliability predictor that determines clean frames to keep, discarding the noisy frames. We expand this framework by replacing the dropped frames with those extracted from the enhanced speech to keep the lexical information. We refer to this implementation as Keep or Substitute (KoS). Our experiment shows that the KoD model improves the SER results under noisy conditions without fine-tuning the whole model. Also, the KoS framework performs better than enhancing all the frames, indicating the importance of avoiding speech distortion."
INTERSPEECH2024,emotion,Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition,0,Emotion and Mood Recognition,"Personalization,Emotion recognition,Computer science,Speech recognition,Human–computer interaction,World Wide Web",,https://www.isca-archive.org//interspeech_2024/triantafyllopoulos24c_interspeech.pdf,"Andreas Triantafyllopoulos, Björn Schuller","The expression of emotion is highly individualistic. However, contemporary speech emotion recognition (SER) systems typically rely on population-level models that adopt a `one-size-fits-all' approach for predicting emotion. Moreover, standard evaluation practices measure performance also on the population level, thus failing to characterise how models work across different speakers. In the present contribution, we present a new method for capitalising on individual differences to adapt an SER model to each new speaker using a minimal set of enrolment utterances. In addition, we present novel evaluation schemes for measuring fairness across different speakers. Our findings show that aggregated evaluation metrics may obfuscate fairness issues on the individual-level, which are uncovered by our evaluation, and that our proposed method can improve performance both in aggregated and disaggregated terms."
INTERSPEECH2024,emotion,Multimodal Fusion of Music Theory-Inspired and Self-Supervised Representations for Improved Emotion Recognition,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Artificial intelligence,Sensor fusion,Fusion,Speech recognition,Natural language processing,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2024/shi24i_interspeech.pdf,"Xiaohan Shi, Xingfeng Li, Tomoki Toda","Multimodal emotion recognition (MER) is a rapidly evolving field aimed at integrating information from various modalities, such as speech and text, to deepen our understanding of emotions. However, challenges in feature extraction and fusion hinder further advancements in MER performance. To address these challenges, we propose a MER method using self-supervised representations and handcrafted music theory-inspired representations across different modalities to comprehensively capture emotional information. Additionally, we introduce a novel multimodal fusion method to explore modality-specific and modality-invariant relationships, thereby reducing distribution gaps between different modalities in MER. Extensive experimental validation underscores the effectiveness of our approach, with state-of-the-art results showing a 3.55% improvement compared with the baseline. These results validate the effectiveness of our proposed method, signifying a notable enhancement in MER performance."
INTERSPEECH2024,emotion,MFDR: Multiple-stage Fusion and Dynamically Refined Network for Multimodal Emotion Recognition,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Fusion,Artificial intelligence,Pattern recognition (psychology),Philosophy,Linguistics",,https://www.isca-archive.org//interspeech_2024/zhao24g_interspeech.pdf,"Ziping Zhao, Tian Gao, Haishuai Wang, Björn Schuller","Emotion recognition in conversation should not rely solely on discovering emotion keywords but also make comprehensive judgments after considering the context. To this end, we propose the MFDR to efficiently integrate acoustic and textual information. Specifically, acoustic-word combination and context perception are modeled sequentially in stages through the Sliding Adaptive Window Attention (SAWA) and Gated Context Perception Unit. More importantly, without additional memory overhead, SAWA allows the perception range to be adaptively adjusted according to the correlation strength to solve the misalignment and information loss caused by window truncation, modeling fusion under variable granularity. Furthermore, emotion refinement through Dynamic Frame Convolution strips out emotion-irrelevant frames, thereby generating a compact and emotionally discriminative fusion representation. The efficacy of MFDR is confirmed by IEMOCAP and CMU-MOSEI, where it demonstrates promising performance."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ICLR2022,affect,Comparing Distributions by Measuring Differences that Affect Decision Making,26285,"Meta-analysis and systematic reviews,Hemodynamic Monitoring and Therapy,Health Sciences Research and Education","Systematic review,Checklist,Psychological intervention,Guideline,MEDLINE,Health care,Medicine,CLARITY,Psychology,Management science,Nursing,Pathology,Political science,Law,Economics,Cognitive psychology,Biochemistry,Chemistry",,https://openreview.net/pdf/e99719a7a6796b569cc6afdf6f42024d0df2fbea.pdf,"Shengjia Zhao,Abhishek Sinha,Yutong He,Aidan Perreault,Jiaming Song,Stefano Ermon","Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks."
ICLR2024,affects,The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning,4789,"Motivation and Self-Concept in Sports,Creativity in Education and Neuroscience,Behavioral Health and Interventions",Computer science,,https://openreview.net/pdf/6a6c078824f86b9c8353c38a86794c78253b804f.pdf,"Tian Jin,Nolan Clement,Xin Dong,Vaishnavh Nagarajan,Michael Carbin,Jonathan Ragan-Kelley,Gintare Karolina Dziugaite","We study how down-scaling large language model (LLM) size impacts LLM capabilities. We begin by measuring the effects of weight pruning – a popular technique for reducing model size – on the two abilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in context. Surprisingly, we find that existing pruning techniques affect these two abilities of LLMs differently. For example, pruning more than 30% of weights significantly decreases an LLM’s ability to recall facts presented during pre-training. Yet pruning 60-70% of weights largely preserves an LLM’s ability to process information in-context, ranging from retrieving answers based on information presented in context to learning parameterized functions such as a linear classifier based on a few examples. Moderate pruning impairs LLM’s ability to recall facts learnt from pre-training. However, its effect on model’s ability to process information presented in context is much less pronounced. The said disparate effects similarly arise when replacing the original model with a smaller dense one with reduced width and depth. This similarity suggests that model size reduction in general underpins the said disparity."
INTERSPEECH2024,emotion,Can Modelling Inter-Rater Ambiguity Lead To Noise-Robust Continuous Emotion Predictions?,0,"Wine Industry and Tourism,Sensory Analysis and Statistical Methods","Ambiguity,Computer science,Lead (geology),Noise (video),Artificial intelligence,Geology,Geomorphology,Image (mathematics),Programming language",,https://www.isca-archive.org//interspeech_2024/wu24d_interspeech.pdf,"Ya-Tse Wu, Jingyao Wu, Vidhyasaharan Sethu, Chi-Chun Lee","There has been increasing attention drawn to modelling inter-rater ambiguity in Continuous Emotion Recognition (CER) systems using probability distributions for arousal and valence. However, the relationship between modelling label ambiguity and robustness to noise, and more broadly, the impact of real-world noise on CER systems remains insufficiently explored. In this study, we argue that incorporating inter-rater ambiguity during training can regularize the noise response, leading to noise robustness. To this end, we propose a novel loss function that incorporates inter-rater ambiguity into model training. Experiments conducted on the RECOLA dataset demonstrate that our proposed method achieves a maximum Concordance Correlation Coefficient (CCC) improvement of 0.117 and 0.077 for mean and standard deviation predictions, respectively, across all noise conditions. We further integrate traditional noisy augmentation strategies with our proposed method and observe promising results."
INTERSPEECH2024,emotion,"PERSONA: an application for emotion recognition, gender recognition and age estimation",0,Face recognition and analysis,"Persona,Emotion recognition,Estimation,Speech recognition,Psychology,Computer science,Artificial intelligence,Pattern recognition (psychology),Cognitive psychology,Human–computer interaction,Economics,Management",,https://www.isca-archive.org//interspeech_2024/koshal24_interspeech.pdf,"Devyani Koshal, Orchid Chetia Phukan, Sarthak Jain, Arun Balaji Buduru, Rajesh Sharma","Emotion Recognition (ER), Gender Recognition (GR), and Age Estimation (AE) constitute paralinguistic tasks that rely not on the spoken content but primarily on speech characteristics such as pitch and tone. While previous research has made significant strides in developing models for each task individually, there has been comparatively less emphasis on concurrently learning these tasks, despite their inherent interconnectedness. As such in this demonstration, we present PERSONA, an application for predicting ER, GR, and AE with a single model in the backend. One notable point is we show that representations from speaker recognition pre-trained model (PTM) is better suited for such a multi-task learning format than the state-of-the-art (SOTA) self-supervised (SSL) PTM by carrying out a comparative study. Our methodology obviates the need for deploying separate models for each task and can potentially conserve resources and time during the training and deployment phases."
INTERSPEECH2021,"emotion,emotional",Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions,12,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Robustness (evolution),Speech recognition,Computer science,Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Biochemistry,Chemistry,Gene",,https://www.isca-archive.org//interspeech_2021/leem21_interspeech.pdf,"Seong-Gyun Leem, Daniel Fulford, Jukka-Pekka Onnela, David Gard, Carlos Busso",When 
INTERSPEECH2021,emotion,Speaker Attentive Speech Emotion Recognition,8,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Emotion recognition,Speech recognition,Speaker recognition,Recall,Task (project management),Focus (optics),Speaker diarisation,Identity (music),Adaptation (eye),Artificial neural network,Mechanism (biology),Artificial intelligence,Psychology,Cognitive psychology,Philosophy,Physics,Management,Epistemology,Neuroscience,Acoustics,Optics,Economics",,https://www.isca-archive.org//interspeech_2021/moine21_interspeech.pdf,"Clément Le Moine, Nicolas Obin, Axel Roebel","Speech Emotion Recognition (SER) task has known significant improvements over the last years with the advent of Deep Neural Networks (DNNs). However, even the most successful methods are still rather failing when adaptation to specific speakers and scenarios is needed, inevitably leading to poorer performances when compared to humans. In this paper, we present novel work based on the idea of teaching the emotion recognition network about speaker identity. Our system is a combination of two ACRNN classifiers respectively dedicated to speaker and emotion recognition. The first informs the latter through a Self Speaker Attention (SSA) mechanism that is shown to considerably help to focus on emotional information of the speech signal. Speaker-dependant experiments on social attitudes database Att-HACK and IEMOCAP corpus demonstrate the effectiveness of the proposed method and achieve the state-of-the-art performance in terms of unweighted average recall."
INTERSPEECH2021,"emotion,emovie,emotional",EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model,13,"Speech Recognition and Synthesis,Topic Modeling,Sentiment Analysis and Opinion Mining","Computer science,Speech recognition,Task (project management),Mandarin Chinese,Speech synthesis,Natural language processing,Artificial intelligence,Embedding,Artificial neural network,Linguistics,Philosophy,Management,Economics",,https://www.isca-archive.org//interspeech_2021/cui21c_interspeech.pdf,"Chenye Cui, Yi Ren, Jinglin Liu, Feiyang Chen, Rongjie Huang, Ming Lei, Zhou Zhao","Recently, there has been an increasing interest in neural speech synthesis. While the deep neural network achieves the state-of-the-art result in text-to-speech (TTS) tasks, how to generate a more emotional and more expressive speech is becoming a new challenge to researchers due to the scarcity of high-quality emotion speech dataset and the lack of advanced emotional TTS model. In this paper, we first briefly introduce and publicly release a Mandarin emotion speech dataset including 9,724 samples with audio files and its emotion human-labeled annotation. After that, we propose a simple but efficient architecture for emotional speech synthesis called EMSpeech. Unlike those models which need additional reference audio as input, our model could predict emotion labels just from the input text and generate more expressive speech conditioned on the emotion embedding. In the experiment phase, we first validate the effectiveness of our dataset by an emotion classification task. Then we train our model on the proposed dataset and conduct a series of subjective evaluations. Finally, by showing a comparable performance in the emotional speech synthesis task, we successfully demonstrate the ability of the proposed model."
IJCAI2022,emotional,"CounterGeDi: A Controllable Approach to Generate Polite, Detoxified and Emotional Counterspeech",10,Hate Speech and Cyberbullying Detection,"Politeness,Computer science,Generalization,Generative model,Explosive material,Emotion recognition,Speech recognition,Artificial intelligence,Natural language processing,Generative grammar,Machine learning,Linguistics,Mathematics,Mathematical analysis,Philosophy,Chemistry,Organic chemistry",,https://www.ijcai.org/proceedings/2022/0716.pdf,"Punyajoy Saha, Kanishk Singh, Adarsh Kumar, Binny Mathew, Animesh Mukherjee","Recently, many studies have tried to create generation models to assist counter speakers by providing counterspeech suggestions for combating the explosive proliferation of online hate. However, since these suggestions are from a vanilla generation model, they might not include the appropriate properties required to counter a particular hate speech instance. In this paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi) to guide the generation of a DialoGPT model toward more polite, detoxified, and emotionally laden counterspeech. We generate counterspeech using three datasets and observe significant improvement across different attribute scores. The politeness and detoxification scores increased by around 15% and 6% respectively, while the emotion in the counterspeech increased by at least 10% across all the datasets. We also experiment with triple-attribute control and observe significant improvement over single attribute results when combining complementing attributes, e.g., politeness, joyfulness and detoxification. In all these experiments, the relevancy of the generated text does not deteriorate due to the application of these controls."
IJCAI2023,emotion,Mimicking the Thinking Process for Emotion Recognition in Conversation with Prompts and Paraphrasing,6,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Utterance,Conversation,Computer science,Context (archaeology),Task (project management),Natural language processing,Semantics (computer science),Process (computing),Emotion recognition,Artificial intelligence,Speech recognition,Psychology,Communication,Paleontology,Management,Economics,Biology,Programming language,Operating system",,https://www.ijcai.org/proceedings/2023/0699.pdf,"Ting Zhang, Zhuang Chen, Ming Zhong, Tieyun Qian","                        Emotion recognition in conversation,  which aims to predict the emotion for all utterances,  has attracted considerable research attention in recent years. It is a challenging task since the  recognition of the emotion in one  utterance  involves many complex factors, such as the conversational context, the speaker's  background, and the subtle difference between emotion labels. In this paper, we propose a novel framework which mimics the thinking process when modeling these factors. Specifically, we first comprehend the conversational context with a history-oriented prompt to selectively gather  information from predecessors of the target utterance. We then  model the speaker's background with an experience-oriented prompt  to retrieve the similar utterances from all conversations.  We finally  differentiate the subtle label semantics with a paraphrasing mechanism  to elicit the intrinsic label related knowledge. We conducted extensive experiments on three benchmarks. The empirical results demonstrate the superiority of our proposed framework over the state-of-the-art baselines.                     "
INTERSPEECH2021,emotions,In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German,0,"Linguistic Variation and Morphology,Categorization, perception, and language,Phonetics and Phonology Research","Prosody,German,Variety (cybernetics),Happiness,Surprise,Psychology,Linguistics,Perception,Sadness,Anger,Cognitive psychology,Computer science,Communication,Social psychology,Artificial intelligence,Philosophy,Neuroscience",,https://www.isca-archive.org//interspeech_2021/jakob21_interspeech.pdf,"Moritz Jakob, Bettina Braun, Katharina Zahner-Ritter",Various studies on the perception of vocally expressed emotions have shown that recognition rates are higher if speaker and listener belong to the same cultural or linguistic group. This so-called 
INTERSPEECH2023,emotion,Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement,0,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Video Surveillance and Tracking Methods","Computer science,Animation,Face (sociological concept),Feature (linguistics),Computer facial animation,Multimedia,Computer animation,Computer graphics (images),Social science,Linguistics,Philosophy,Sociology",,https://www.isca-archive.org//interspeech_2023/ren23_interspeech.pdf,"Xin Ren, Juan Luo, Xionghu Zhong, Minjie Cai","In this paper, we tackle the problem of audio-driven face animation which aims to synthesize a realistic talking face given a piece of driven speech. Directly modeling the mapping from audio feature to facial expression is challenging, since people tend to have different talking styles with momentary emotion states as well as identity-dependent vocal characteristics. To address this challenge, we propose a contrastive feature disentanglement method for emotion-aware face animation. The key idea is to disentangle the features for speech content, momentary emotion and identity-dependent vocal characteristics from audio features with a contrastive learning strategy. Experiments on public datasets show that our method can generate more realistic facial expression and enables synthesis of diversified face animation with different emotion."
INTERSPEECH2023,emotion,Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition,4,Emotion and Mood Recognition,"Computer science,Speech recognition,Modal,Emotion recognition,Architecture,Fusion,Speaker recognition,Natural language processing,Artificial intelligence,Human–computer interaction,Linguistics,Art,Chemistry,Philosophy,Polymer chemistry,Visual arts",,https://www.isca-archive.org//interspeech_2023/zhao23e_interspeech.pdf,"Huan Zhao, Bo Li, Zixing Zhang","Conversational Emotion Recognition (CER) is an important topic in the construction of intelligent human-machine interaction systems. The emotion is mainly influenced by the conversational context and the speakers. In addition, sufficient utilization of the relevant features of both speech and text modes is also crucial to the performance of CER. Based on the above considerations, we propose a novel Speaker-aware Cross-modal Fusion Architecture (SCFA). Within a single modality, we design a conversation encoder, including a context encoder and a speaker-aware encoder, to model the conversational content and the intra- and inter-speaker influence, respectively. On this basis, cross-modal fusion attention is introduced to extract the cross-modal characteristics of the conversation, so as to better detect the emotions in conversation. We conduct experiments on the IEMOCAP and MELD datasets. Compared with state-of-the-art baselines, SCFA achieves better performance on average."
INTERSPEECH2023,emotion,Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition,2,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Crossmodal,Computer science,Speech recognition,Transformer,Focus (optics),Emotion recognition,Metric (unit),Psychology,Perception,Engineering,Visual perception,Electrical engineering,Neuroscience,Operations management,Physics,Optics,Voltage",,https://www.isca-archive.org//interspeech_2023/kim23c_interspeech.pdf,"Keulbit Kim, Namhyun Cho","Recognizing emotions in speech is essential for improving human-computer interactions, which require understanding and responding to the users' emotional states. Integrating multiple modalities, such as speech and text, enhances the performance of speech emotion recognition systems by providing a varied source of emotional information. In this context, we propose a model that enhances cross-modal transformer fusion by applying focus attention mechanisms to align and combine the salient features of two different modalities, namely, speech and text. The analysis of the disentanglement of the emotional representation various multiple embedding spaces using deep metric learning confirmed that our method shows enhanced emotion recognition performance. Furthermore, the proposed approach was evaluated on the IEMOCAP dataset. Experimental results demonstrated that our model achieves the best performance among other relevant multimodal speech emotion recognition systems."
INTERSPEECH2023,emotional,Evaluation of delexicalization methods for research on emotional speech,1,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Phonetics and Phonology Research","Computer science,Speech recognition,Natural language processing",,https://www.isca-archive.org//interspeech_2023/audibert23_interspeech.pdf,"Nicolas Audibert, Francesca Carbone, Maud Champagne-Lavau, Aurélien Said Housseini, Caterina Petrone","Perceptual evaluation of non-controlled emotional speech requires delexicalization to neutralize semantic variation. However, most existing methods imply losing spectral cues crucial to emotional attribution, related to both laryngeal and supralaryngeal settings. We propose a method relying on voice morphing to retain part of the spectral information of the original stimuli, as an additional step to diphone synthesis delexicalization. After previous assessment of intelligibility loss, this study evaluates the naturalness of angry and neutral expressions in French films, delexicalized using low-pass filtering and the proposed method implemented with MBROLA and STRAIGHT. Results show that morphing-based delexicalization, which leads to accurate emotional attribution, is rated with a higher degree of naturalness than low-pass filtering. Implications for research in affective speech are discussed with regards to other delexicalization methods proposed in the literature."
INTERSPEECH2023,emotional,Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings,1,"Speech Recognition and Synthesis,Speech and Audio Processing","Speech recognition,Speaker recognition,Computer science,Speaker diarisation,Emotion recognition,Speech processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/essery23_interspeech.pdf,"Robert Essery, Philip Harrison, Vincent Hughes","In forensic contexts, speakers often feel emotional, which will likely influence their speech. Emotional mismatch between samples is therefore a source of variability which could have substantial effects on the performance of a forensic automatic speaker recognition system. This paper examines the issue of emotional speech in forensic casework, both in terms of emotional match and mismatch between test samples and in terms of the data used to calibrate the system (i.e. the reference population). Specifically, we tested system performance on samples of neutral and acted angry and fearful speech data across 37 test conditions. The best system performance was achieved when the test data and reference population conditions matched exactly. However, in 16 of the 37 tests, the system produced a Cllr greater than 0.8, 10 of which also exceeded a Cllr of 1. As a result, caution should be used to interpret the results of automatic and semi-automatic forensic analysis on emotional speech data."
INTERSPEECH2022,emotion,Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities,3,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Phonetics and Phonology Research","Coding (social sciences),Computer science,Reliability (semiconductor),Speech recognition,Natural language processing,Artificial intelligence,Machine learning,Statistics,Power (physics),Physics,Mathematics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2022/mirheidari22_interspeech.pdf,"Bahman Mirheidari, Andre Bittar, Nicholas Cummins, Johnny Downs, Helen L. Fisher, Heidi Christensen","We present a novel feasibility study on the automatic recognition of Expressed Emotion (EE), a family environment concept based on caregivers speaking freely about their relative/family member. We describe an automated approach for determining the degree of warmth, a key component of EE, from acoustic and text features acquired from a sample of 37 recorded interviews. These recordings, collected over 20 years ago, are derived from a nationally representative birth cohort of 2,232 British twin children and were manually coded for EE. We outline the core steps of extracting usable information from recordings with highly variable audio quality and assess the efficacy of four machine learning approaches trained with different combinations of acoustic and text features. Despite the challenges of working with this legacy data, we demonstrated that the degree of warmth can be predicted with an F1-score of 61.5%. In this paper, we summarise our learning and provide recommendations for future work using real-world speech samples."
INTERSPEECH2022,emotional,Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS,11,Speech Recognition and Synthesis,"Style (visual arts),Computer science,Speech recognition,Control (management),Speaker recognition,Psychology,Artificial intelligence,Art,Literature",,https://www.isca-archive.org//interspeech_2022/shin22b_interspeech.pdf,"Yookyung Shin, Younggun Lee, Suhee Jo, Yeongtae Hwang, Taesu Kim","Expressive text-to-speech has shown improved performance in recent years. However, the style control of synthetic speech is often restricted to discrete emotion categories and requires training data recorded by the target speaker in the target style. In many practical situations, users may not have reference speech recorded in target emotion but still be interested in controlling speech style just by typing text description of desired emotional style. In this paper, we propose a text-based interface for emotional style control and cross-speaker style transfer in multi-speaker TTS. We propose the bi-modal style encoder which models the semantic relationship between text description embedding and speech style embedding with a pretrained language model. To further improve cross-speaker style transfer on disjoint, multi-style datasets, we propose the novel style loss. The experimental results show that our model can generate high-quality expressive speech even in unseen style."
INTERSPEECH2022,emotion,Speaker Trait Enhancement for Cochlear Implant Users: A Case Study for Speaker Emotion Perception,0,Speech and Audio Processing,"Cochlear implant,Perception,Speech recognition,Speaker recognition,Computer science,Trait,Audiology,Psychology,Medicine,Neuroscience,Programming language",,https://www.isca-archive.org//interspeech_2022/brueggeman22_interspeech.pdf,"Avamarie Brueggeman, John H.L. Hansen","Despite significant progress in areas such as speech recognition, cochlear implant users still experience challenges related to identifying various speaker traits such as gender, age, emotion, accent, etc. In this study, we focus on emotion as one trait. We propose the use of emotion intensity conversion to perceptually enhance emotional speech with the goal of improving speech emotion recognition for cochlear implant users. To this end, we utilize a parallel speech dataset containing emotion and intensity labels to perform conversion from normal to high intensity emotional speech. A non-negative matrix factorization method is integrated to perform emotion intensity conversion via spectral mapping. We evaluate our emotional speech enhancement using a support vector machine model for emotion recognition. In addition, we perform an emotional speech recognition listener experiment with normal hearing listeners using vocoded audio. It is suggested that such enhancement will benefit speaker trait perception for cochlear implant users."
INTERSPEECH2024,emotional,Applying Reinforcement Learning and Multi-Generators for Stage Transition in an Emotional Support Dialogue System,0,Speech and dialogue systems,"Reinforcement learning,Computer science,Transition (genetics),Stage (stratigraphy),Artificial intelligence,Paleontology,Biochemistry,Chemistry,Biology,Gene",,https://www.isca-archive.org//interspeech_2024/chang24_interspeech.pdf,"Jeremy Chang, Kuan-Yu Chen, Chung-Hsien Wu","The use of empathetic dialogue systems has grown recently. However, establishing them for users experiencing mental depression requires more advanced consoling skills. In this paper, a dialogue system based on Emotional Support was developed. The system offers coping strategies through stages designed to address users' distress in long-term conversations. It employs a recurrent-based approach integrated with reinforcement learning for a decision model, which selects a generator from three specialized conditional generation models to generate empathetic responses. Experimental results showed improvements in BLEU, Rouge-L, and Distinct-n metrics compared to the baseline. On average, the system's BLEU score increased by 0.87, Rouge-L by 1.85, Distinct-1 by 0.69, and Distinct-2 by 2.26. As a result, the system generates responses aligned with Emotional Support skills, ultimately comforting the user’s distress."
INTERSPEECH2024,emotion,Speech emotion recognition with deep learning beamforming  on a distant human-robot interaction scenario,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Beamforming,Human–robot interaction,Emotion recognition,Speech recognition,Robot,Artificial intelligence,Human interaction,Deep learning,Human–computer interaction,Telecommunications",,https://www.isca-archive.org//interspeech_2024/garcia24_interspeech.pdf,"Ricardo García, Rodrigo Mahu, Nicolás Grágeda, Alejandro Luzanto, Nicolas Bohmer, Carlos Busso, Néstor Becerra Yoma","Human-robot interaction (HRI) is becoming a truly relevant topic imposing many challenges for state-of-the-art speech technology. This paper describes the first evaluation of speech emotion recognition (SER) technology with non-acted speech data recorded in a real indoor HRI scenario using deep learning-based beamforming technologies. The results presented show that deep learning beamforming gives in average an average concordance correlation coefficient (CCC) that is 15.03% higher than the ordinary minimum variance distortionless response (MVDR) beamformer when the SER system was trained with simulated conditions, which included an acoustic model of the testing HRI environment. Training by simulating the test scenarios and testing with real HRI static data provides on average an average CCC that is just 22.5% smaller than the ideal condition where training and testing were performed with the original MSP-Podcast database. This suggests the possibility to train SER engines with methods that emulates complex testing scenarios without recording further data."
INTERSPEECH2024,emotion,Are you sure? Analysing Uncertainty Quantification Approaches for Real-world Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Natural language processing",,https://www.isca-archive.org//interspeech_2024/schrufer24_interspeech.pdf,"Oliver Schrüfer, Manuel Milling, Felix Burkhardt, Florian Eyben, Björn Schuller","Uncertainty Quantification (UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. Speech emotion recognition (SER) models can suffer from particularly many sources of uncertainty, such as the ambiguity of emotions, Out-of-Distribution (OOD) data or, in general, poor recording conditions. Reliable UQ methods are thus of particular interest as in many SER applications no prediction is better than a faulty prediction. While the effects of label ambiguity on uncertainty are well documented in the literature, we focus our work on an evaluation of UQ methods for SER under common challenges in real-world application, such as corrupted signals, and the absence of speech. We show that simple UQ methods can already give an indication of the uncertainty of a prediction and that training with additional OOD data can greatly improve the identification of such signals."
INTERSPEECH2024,emotion,An Investigation of Group versus Individual Fairness in Perceptually Fair Speech Emotion Recognition,0,Emotion and Mood Recognition,"Emotion recognition,Speech recognition,Computer science,Group (periodic table),Natural language processing,Psychology,Cognitive psychology,Chemistry,Organic chemistry",,https://www.isca-archive.org//interspeech_2024/chien24_interspeech.pdf,"Woan-Shiuan Chien, Chi-Chun Lee","Speech emotion recognition (SER) has been extensively integrated into voice-centric applications. A unique fairness issue of SER stems from the naturally biased labels given by raters as ground truth. While existing efforts primarily aim to advance SER fairness through a group (i.e., gender) fairness standpoint, our analysis reveals that label biases arising from individual raters also persist and require equal attention. Our work presents a systematic analysis to determine the effect of enhanced group (gender) fairness on individual fairness. Specifically, by evaluating two datasets we demonstrate that there exists a trade-off between group and individual fairness when removing group information. Moreover, our results indicate that achieving group fairness results in diminished individual fairness, particularly when the attribute distributions of the two groups are significantly distant. This work brings initial insights into issues of group and individual fairness in the SER systems."
INTERSPEECH2024,emotion,Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition,1,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Speech recognition,Emotion recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2024/sun24e_interspeech.pdf,"Haoqin Sun, Shiwan Zhao, Xiangyu Kong, Xuechen Wang, Hui Wang, Jiaming Zhou, Yong Qin","Recognizing emotions from speech is a daunting task due to the subtlety and ambiguity of expressions. Traditional speech emotion recognition (SER) systems, which typically rely on a singular, precise emotion label, struggle with this complexity. Therefore, modeling the inherent ambiguity of emotions is an urgent problem. In this paper, we propose an iterative prototype refinement framework (IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive learning and class prototypes. The former provides an efficient way to obtain high-quality representations of ambiguous samples. The latter are dynamically updated based on ambiguous labels—the similarity of the ambiguous data to all prototypes. These refined embeddings yield precise pseudo labels, thus reinforcing representation quality. Experimental evaluations conducted on the IEMOCAP dataset validate the superior performance of IPR over state-of-the-art methods, thus proving the effectiveness of our proposed method."
INTERSPEECH2024,emotional,The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability,0,"Computational and Text Analysis Methods,Advanced Text Analysis Techniques,Opinion Dynamics and Social Influence","Computer science,Artificial intelligence,Computer vision",,https://www.isca-archive.org//interspeech_2024/tavernor24_interspeech.pdf,"James Tavernor, Yara El-Tawil, Emily Mower Provost","Emotion expression and perception are nuanced, complex, and highly subjective processes. When multiple annotators label emotional data, the resulting labels contain high variability. Most speech emotion recognition tasks address this by averaging annotator labels as ground truth. However, this process omits the nuance of emotion and inter-annotator variability, which are important signals to capture. Previous work has attempted to learn distributions to capture emotion variability, but these methods also lose information about the individual annotators. We address these limitations by learning to predict individual annotators and by introducing a novel method to create distributions from continuous model outputs that permit the learning of emotion distributions during model training.  We show that this combined approach can result in emotion distributions that are more accurate than those seen in prior work, in both within- and cross-corpus settings."
INTERSPEECH2024,emotion,An Inter-Speaker Fairness-Aware Speech Emotion Regression Framework,0,"Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Speech recognition,Emotion recognition,Regression,Speaker verification,Regression analysis,Speaker recognition,Natural language processing,Machine learning,Statistics,Mathematics",,https://www.isca-archive.org//interspeech_2024/chou24_interspeech.pdf,"Hsing-Hang Chou, Woan-Shiuan Chien, Ya-Tse Wu, Chi-Chun Lee","Speech emotion recognition (SER) helps to achieve better human-to-machine interactions in voice technologies. Recent studies have pointed out critical fairness issues in the SER. While there are efforts in building fair SER, most of the works focus on fairness between demographic groups and rely on these broad categorical attributes to build a fair SER. In this paper, we instead focus on the fairness learning among individual speakers, which is rarely discussed yet much more intuitively appealing in constructing a fair SER model. To reduce the reliance on knowing speaker IDs, we perform unsupervised clustering on the utterance embeddings from a pretrained speaker verification model that puts utterances with different characteristics into clusters that roughly represent the true speaker index. Our evaluation demonstrates that with these cluster IDs, we can construct a fairness-aware SER model at an individual speaker-level without knowing speaker IDs upfront."
INTERSPEECH2021,affect,How f0 and Phrase Position Affect Papuan Malay Word Identification,0,"Linguistic, Cultural, and Literary Studies,Linguistic Variation and Morphology,Educational Methods and Technology","Malay,Phrase,Affect (linguistics),Computer science,Identification (biology),Linguistics,Word (group theory),Natural language processing,Speech recognition,Artificial intelligence,Philosophy,Botany,Biology",,https://www.isca-archive.org//interspeech_2021/kaland21_interspeech.pdf,"Constantijn Kaland, Matthew Gordon","This paper reports a perception experiment on Papuan Malay, an Eastern Indonesian language for which phrase prosody is largely underresearched. While phrase-final f0 movements are the most prominent ones in this language, it remains to be seen to what extent they signal phrase boundaries (demarcating) or whether they contribute to the prosodic prominence of words in that position (highlighting). Crucially, it is unclear whether these functions can actually be teased apart. In an attempt to investigate this issue, a word identification experiment was carried out using manipulated and original f0 word contours in phrase-medial and phrase-final positions. Results indicate that Papuan Malay listeners recognize words faster in phrase-final position, although the shape of the f0 movement did not significantly affect response latencies. The outcomes are discussed in a typological perspective, with particular attention to Trade Malay languages."
INTERSPEECH2021,emotion,Emotion Carrier Recognition from Personal Narratives,11,"Topic Modeling,Sentiment Analysis and Opinion Mining,Natural Language Processing Techniques","Narrative,Valence (chemistry),Emotion recognition,Task (project management),Computer science,Sentiment analysis,Cognitive psychology,Artificial intelligence,Psychology,Linguistics,Philosophy,Physics,Management,Quantum mechanics,Economics",,https://www.isca-archive.org//interspeech_2021/tammewar21_interspeech.pdf,"Aniruddha Tammewar, Alessandra Cervone, Giuseppe Riccardi","Personal Narratives (PN) — recollections of facts, events, and thoughts from one’s own experience — are often used in everyday conversations. So far, PNs have mainly been explored for tasks such as valence prediction or emotion classification (e.g. "
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
TPAMI2024,emotion,COLD Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition.,14,"Emotion and Mood Recognition,Speech and Audio Processing,Face and Expression Recognition","Computer science,Artificial intelligence,Context (archaeology),Ranking (information retrieval),Variance (accounting),Machine learning,Modality (human–computer interaction),Ground truth,Matching (statistics),Noise (video),Pattern recognition (psychology),Statistics,Mathematics,Business,Biology,Paleontology,Accounting,Image (mathematics)",,,"Mani Kumar Tellamekala,Shahin Amiriparian,Björn W. Schuller,Elisabeth André,Timo Giesbrecht,Michel F. Valstar",
TPAMI2021,emotion,SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild.,186,"Emotion and Mood Recognition,Multisensory perception and integration,Color perception and design","Computer science,Valence (chemistry),Audio visual,Arousal,Mirroring,Facial expression,Artificial intelligence,Database,Speech recognition,Human–computer interaction,Multimedia,Psychology,Communication,Physics,Quantum mechanics,Neuroscience",,,"Jean Kossaifi,Robert Walecki,Yannis Panagakis,Jie Shen,Maximilian Schmitt,Fabien Ringeval,Jing Han,Vedhas Pandit,Antoine Toisoul,Björn W. Schuller,Kam Star,Elnar Hajiyev,Maja Pantic",
TPAMI2023,emotional,Emotional Attention: From Eye Tracking to Computational Modeling.,5,"Visual Attention and Saliency Detection,Gaze Tracking and Assistive Technology,Face Recognition and Perception","Computer science,Artificial intelligence,Computer vision,Eye tracking",,,"Shaojing Fan,Zhiqi Shen,Ming Jiang,Bryan L. Koenig,Mohan S. Kankanhalli,Qi Zhao",
TPAMI2022,affective,Affective Image Content Analysis: Two Decades Review and New Perspectives.,99,"Color perception and design,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Artificial intelligence,Image processing,Content analysis,Content (measure theory),Image (mathematics),Computer vision,Pattern recognition (psychology),Mathematics,Mathematical analysis,Social science,Sociology",,,"Sicheng Zhao,Xingxu Yao,Jufeng Yang,Guoli Jia,Guiguang Ding,Tat-Seng Chua,Björn W. Schuller,Kurt Keutzer",
ECCV2024,"emote,emo",EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions,5,"Music and Audio Processing,Generative Adversarial Networks and Image Synthesis,Face recognition and analysis","Portrait,Computer science,Diffusion,Phase portrait,Computer graphics (images),Art history,Art,Physics,Nonlinear system,Quantum mechanics,Bifurcation,Thermodynamics",,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11028.pdf,"Linrui Tian*, Qi Wang*, Bang Zhang*, Liefeng Bo*","""In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonstrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism."""
ECCV2024,"affective,emotional",Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations,1,"Language, Metaphor, and Cognition,Multimodal Machine Learning Applications,Human Pose and Action Recognition","Dialog box,Computer science,Benchmark (surveying),Scale (ratio),Human–computer interaction,Artificial intelligence,Visual reasoning,Dialog system,Cognitive science,Natural language processing,Psychology,World Wide Web,Cartography,Geography",https://affective-visual-dialog.,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09516.pdf,"Kilichbek Haydarov*, Xiaoqian Shen, Avinash Madasu, Mahmoud Salem, Li-Jia Li, Gamaleldin F Elsayed, Mohamed Elhoseiny","""We introduce Affective Visual Dialog, an emotion explanation and reasoning task as a testbed for research on understanding constructed emotions in response to visually grounded conversations. The task involves three skills: (1) Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3) Affective explanation generation based on the dialog. Our key contribution is the collection of a large-scale dataset, dubbed AffectVisDial, consisting of 50K 10-turn visually grounded dialogs as well as concluding emotion attributions and dialog-informed textual emotion explanations, resulting in a total of 27,180 working hours. Notably, the dataset spans a broad range of visual stimuli, covering human heritage and contemporary life, with an average per-turn answer length of about 12 words — 5 times that of the VisDial dataset — and explanations exceeding 28 words on average. We explain our determining design decisions in collecting the dataset, data inclusion and exclusion criteria starting from over 100K dialogs for quality control, and introduce the questioner and answerer tasks that are associated with the participants in the conversation. We propose and demonstrate solid baselines adapted from state-of-the-art multimodal models. Remarkably, the responses generated by our models show promising emotional reasoning abilities in response to visually grounded conversations. Our project page with the dataset is available through https://affective-visual-dialog. github.io"""
INTERSPEECH2021,emotional,Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech,4,Consumer Retail Behavior Studies,"Estimation,Computer science,Advertising,State (computer science),Speech recognition,Business,Economics,Algorithm,Management",,https://www.isca-archive.org//interspeech_2021/nagano21_interspeech.pdf,"Mizuki Nagano, Yusuke Ijima, Sadao Hiroya","The characteristics of a speaker’s voice can affect the perceived impression or behavior of the listener. Previous studies of consumer behavior have shown that this can be well explained by the emotion-mediated behavior model. However, few studies of the emotion-mediated behavior model have used advertising speech. In this paper, we examine whether the stimulus-organism-response theory using emotional state can explain willingness to buy from advertising speech stimulus. The subjects listened to speech with modified speech features (mean F0, speech rate, spectral tilt, or standard deviation of F0) and rated their willingness to buy the products advertised in the speech and their own perceived emotions (pleasure, arousal, dominance). We found that the emotions partially mediate the influence of speech features on the willingness to buy. These results will be useful for developing a method of speech synthesis to increase people’s willingness to buy."
INTERSPEECH2021,emotional,Multi-Speaker Emotional Text-to-Speech Synthesizer,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Speech recognition,Computer science,Speech synthesis,Speaker diarisation,Natural language processing,Speaker recognition,Artificial intelligence",,https://www.isca-archive.org//interspeech_2021/cho21_interspeech.pdf,"Sungjae Cho, Soo-Young Lee","We present a methodology to train our multi-speaker emotional text-to-speech synthesizer that can express speech for 10 speakers’ 7 different emotions. All silences from audio samples are removed prior to learning. This results in fast learning by our model. Curriculum learning is applied to train our model efficiently. Our model is first trained with a large single-speaker neutral dataset, and then trained with neutral speech from all speakers. Finally, our model is trained using datasets of emotional speech from all speakers. In each stage, training samples of each speaker-emotion pair have equal probability to appear in mini-batches. Through this procedure, our model can synthesize speech for all targeted speakers and emotions. Our synthesized audio sets are available on our web page."
MM2021,emotional,Text-driven 3D Avatar Animation with Emotional and Expressive Behaviors.,8,"Human Motion and Animation,Face recognition and analysis,Human Pose and Action Recognition","Avatar,Computer science,Computer facial animation,Animation,Facial expression,Virtual actor,Human–computer interaction,Computer animation,Movement (music),Joint (building),Speech recognition,Virtual reality,Artificial intelligence,Computer graphics (images),Architectural engineering,Philosophy,Engineering,Aesthetics",,,"Li Hu,Jinwei Qi,Bang Zhang,Pan Pan,Yinghui Xu",
MM2021,emotional,Pairwise Emotional Relationship Recognition in Drama Videos: Dataset and Benchmark.,6,"Human Pose and Action Recognition,Emotion and Mood Recognition,Video Analysis and Summarization","Computer science,Task (project management),Benchmark (surveying),Pairwise comparison,Modal,Artificial intelligence,Speech recognition,Machine learning,Natural language processing,Chemistry,Management,Geodesy,Polymer chemistry,Economics,Geography",,,"Xun Gao,Yin Zhao,Jie Zhang,Longjun Cai",
MM2021,emotions,Learning to Compose Stylistic Calligraphy Artwork with Emotions.,11,"Generative Adversarial Networks and Image Synthesis,Aesthetic Perception and Analysis,Computer Graphics and Visualization Techniques","Calligraphy,Handwriting,Computer science,Artificial intelligence,Generative grammar,Chinese characters,Character (mathematics),Style (visual arts),Natural language processing,Painting,Art,Visual arts,Mathematics,Geometry",,,"Shaozu Yuan,Ruixue Liu,Meng Chen,Baoyang Chen,Zhijie Qiu,Xiaodong He",
MM2023,emotional,Feeling Positive? Predicting Emotional Image Similarity from Brain Signals.,6,"Optical Imaging and Spectroscopy Techniques,Advanced Chemical Sensor Technologies,EEG and Brain-Computer Interfaces","Similarity (geometry),Valence (chemistry),Computer science,Arousal,Artificial intelligence,Content-based image retrieval,Visualization,Neuroimaging,Feeling,Semantic similarity,Cognitive psychology,Content (measure theory),Pattern recognition (psychology),Psychology,Image retrieval,Image (mathematics),Social psychology,Mathematics,Neuroscience,Mathematical analysis,Physics,Quantum mechanics",,,"Tuukka Ruotsalo,Kalle Mäkelä,Michiel M. A. Spapé,Luis A. Leiva",
MM2023,emotion,Multimodal Physiological Signals Fusion for Online Emotion Recognition.,8,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Hypergraph,Fuse (electrical),Computer science,Modalities,Artificial intelligence,Multimodal learning,Machine learning,Sensor fusion,Modality (human–computer interaction),Pattern recognition (psychology),Mathematics,Social science,Electrical engineering,Discrete mathematics,Sociology,Engineering",,,"Tongjie Pan,Yalan Ye,Hecheng Cai,Shudong Huang,Yang Yang,Guoqing Wang",
MM2023,emotion,Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition.,33,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Computer science,Context (archaeology),Feature (linguistics),Task (project management),Conversation,Multimodality,Artificial intelligence,Utterance,Modality (human–computer interaction),Contextualization,Human–computer interaction,Machine learning,Speech recognition,Linguistics,Psychology,Communication,Engineering,Paleontology,Philosophy,Systems engineering,World Wide Web,Interpretation (philosophy),Biology,Programming language",,,"Bobo Li,Hao Fei,Lizi Liao,Yu Zhao,Chong Teng,Tat-Seng Chua,Donghong Ji,Fei Li",
MM2023,affective,Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intelligence.,2,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Perception,Computer science,Affective computing,Psychological research,Artificial intelligence,Cognitive psychology,Psychology,Social psychology,Neuroscience",,,"Yiwei Ru,Peipei Li,Muyi Sun,Yunlong Wang,Kunbo Zhang,Qi Li,Zhaofeng He,Zhenan Sun",
INTERSPEECH2023,emotion,SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition,3,Emotion and Mood Recognition,"Computer science,Sliding window protocol,Artificial intelligence,Classifier (UML),Pattern recognition (psychology),Feature (linguistics),Feature extraction,Emotion recognition,Reuse,Window (computing),Engineering,Linguistics,Philosophy,Waste management,Operating system",,https://www.isca-archive.org//interspeech_2023/zhao23b_interspeech.pdf,"Ziping Zhao, Tian Gao, Haishuai Wang, Björn W. Schuller","To achieve efficient feature fusion, existing research tends to employ cross-attention to control the contributions of different modalities in fusion. However, this inevitably causes high computational effort and introduces noise weights due to redundant computations. Therefore, this paper proposes sliding window attention (SliWa) to control the feature perception range and dynamically model the modality fusion at different granularities. In addition, we present a novel feature map classifier (FMC) based on high-response feature reuse (HRFR), which explicitly preserves the deep emotional feature structure, thus preventing the submersion of the crucial classification information after average flattening and the negative impacts of parameter flooding. We unify the mentioned modules in the SWRR framework, and the experimental results on the commonly used datasets IEMOCAP and CMU-MOSEI reveal the effectiveness of SWRR in improving the performance of emotion recognition."
INTERSPEECH2023,emotion,Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition,1,Emotion and Mood Recognition,"Speech recognition,Computer science,Encoding (memory),Word (group theory),Emotion recognition,Natural language processing,Artificial intelligence,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2023/stanley23_interspeech.pdf,"Eimear Stanley, Eric DeMattos, Anita Klementiev, Piotr Ozimek, Georgia Clarke, Michael Berger, Dimitri Palaz","Speech Emotion Recognition (SER) is an important and challenging task for human-computer interaction. Human emotions are complex and nuanced, hence difficult to represent. The standard representations of emotions, categorical or continuous, tend to oversimplify the problem. Recently, the label encoding approach has been proposed, where vectors are used to represent the emotion space. In this paper, we hypothesise that using a pre-existing vector space that encodes semantic information about emotion is beneficial for the task. To this aim, we propose using word embeddings obtained from a Language Model (LM) as labels for SER. We evaluate the performance of the proposed approach on the IEMOCAP corpus and show that it yields better performance than a standard baseline. We also present a method to combine free text labels, which are unusable in conventional approaches, and by doing so we show that the model can learn more nuanced representations of emotions."
NAACL2021,emotion,Seq2Emo: A Sequence to Multi-Label Emotion Classification Model,24,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Topic Modeling","Computer science,Natural language processing,Sequence (biology),Artificial intelligence,Association (psychology),Computational linguistics,Linguistics,Philosophy,Epistemology,Chemistry,Biochemistry",,https://aclanthology.org/2021.naacl-main.375.pdf,"Chenyang Huang,Amine Trabelsi,Xuebin Qin,Nawshad Farruque,Lili Mou,Osmar Zaïane","Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval’18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting."
NAACL2022,emotion,COGMEN: COntextualized GNN based Multimodal Emotion recognitioN,65,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Speech recognition",,https://aclanthology.org/2022.naacl-main.306.pdf,"Abhinav Joshi,Ashwani Bhat,Ayush Jain,Atin Singh,Ashutosh Modi","Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person’s emotions are influenced by the other speaker’s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels."
NAACL2024,emotion,“You are an expert annotator”: Automatic Best–Worst-Scaling Annotations for Emotion Intensity Modeling,0,Mental Health Research Topics,"Computer science,Scaling,Natural language processing,Artificial intelligence,Intensity (physics),Mathematics,Physics,Geometry,Quantum mechanics",,https://aclanthology.org/2024.naacl-long.439.pdf,"Christopher Bagdon,Prathamesh Karmalkar,Harsha Gurulingappa,Roman Klinger","Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best–worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best–worst scaling. We find that the latter shows the highest reliability. A transformer regressor fine-tuned on these data performs nearly on par with a model trained on the original manual annotations."
INTERSPEECH2023,emotion,Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Natural language processing,Human–computer interaction",,https://www.isca-archive.org//interspeech_2023/luo23_interspeech.pdf,"Jiachen Luo, Huy Phan, Joshua Reiss","Textual emotion recognition in conversations has gained increasing attention in recent years for the growing amount of applications it can serve, e.g., human-robot interactions, recommended systems. However, most existing approaches are either based on BERT-based model which fail to exploit crucial information about the long-text context, or resort to complex entanglement of neural network architectures resulting in less stable training procedures and slower inference time. To bridge this gap, we first propose a fast, compact and parameter-efficient framework based on fine-tuned pre-trained RoBERTa model with a CNN-LSTM network for textual emotion recognition in conversations. First, we fine-tune the pre-tranined RoBERTa model to effectively learn long-term emotion-relevant context information. Second, convolutional neural network coupled with the bidirectional long short-term memory and joint reinforced blocks are utilized to recognize emotion in conversations. Extensive experiments are conducted on benchmark emotion MELD dataset, and the results show that our model outperforms a wide range of strong baselines and achieves competitive results with the state-of-art approaches."
INTERSPEECH2023,emotion,LanSER: Language-Model Supported Speech Emotion Recognition,5,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining","Computer science,Natural language processing,Artificial intelligence,Speech recognition,Textual entailment,Emotion recognition,Language model,Logical consequence",,https://www.isca-archive.org//interspeech_2023/gong23c_interspeech.pdf,"Taesik Gong, Josh Belanich, Krishna Somandepalli, Arsha Nagrani, Brian Eoff, Brendan Jou","Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech."
MM2023,emotion,Unlocking the Power of Multimodal Learning for Emotion Recognition in Conversation.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Sentiment Analysis and Opinion Mining","Modalities,Conversation,Computer science,Utterance,Speech recognition,Prosody,Perception,Multimodal learning,Modality (human–computer interaction),Artificial intelligence,Psychology,Communication,Social science,Neuroscience,Sociology",,,"Yunxiao Wang,Meng Liu,Zhe Li,Yupeng Hu,Xin Luo,Liqiang Nie",
MM2023,"emotion,emo",Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and Audio Processing","Computer science,Discriminative model,Artificial intelligence,Decoupling (probability),Speech recognition,Natural language processing,Invariant (physics),Emotion recognition,Pattern recognition (psychology),Mathematics,Control engineering,Engineering,Mathematical physics",,,"Jiaxin Ye,Yujie Wei,Xin-Cheng Wen,Chenglong Ma,Zhizhong Huang,Kunhong Liu,Hongming Shan",
MM2023,emotionally,Emotionally Situated Text-to-Speech Synthesis in User-Agent Conversation.,1,"Speech Recognition and Synthesis,Topic Modeling,Speech and dialogue systems","Conversation,Situated,Computer science,Context (archaeology),Style (visual arts),Semantics (computer science),Human–computer interaction,Natural language processing,Speech synthesis,Dialog system,Artificial intelligence,Linguistics,World Wide Web,Dialog box,History,Paleontology,Philosophy,Archaeology,Biology,Programming language",,,"Yuchen Liu,Haoyu Zhang,Shichao Liu,Xiang Yin,Zejun Ma,Qin Jin",
MM2023,emotion,Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels.,6,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Disgust,Computer science,Surprise,Emotion classification,Electroencephalography,Artificial intelligence,Anger,Arousal,Speech recognition,Cognitive psychology,Psychology,Communication,Psychiatry,Neuroscience",,,"Wei-Bang Jiang,Xuan-Hao Liu,Wei-Long Zheng,Bao-Liang Lu",
MM2023,emotion,Graph to Grid: Learning Deep Representations for Multimodal Emotion Recognition.,3,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Grid,Artificial intelligence,Graph,Pattern recognition (psychology),Machine learning,Theoretical computer science,Geometry,Mathematics",,,"Ming Jin,Jinpeng Li",
MM2023,emotion,Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation.,2,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Feature learning,Modalities,Artificial intelligence,Conversation,Feature extraction,Benchmark (surveying),Multimodal learning,Transformer,Machine learning,Natural language processing,Pattern recognition (psychology),Speech recognition,Linguistics,Engineering,Social science,Philosophy,Geodesy,Voltage,Sociology,Electrical engineering,Geography",,,"Shihao Zou,Xianying Huang,Xudong Shen",
MM2023,affective,SAAML: A Framework for Semi-supervised Affective Adaptation via Metric Learning.,2,"Emotion and Mood Recognition,Human Pose and Action Recognition,Face recognition and analysis","Computer science,Emotion recognition,Domain adaptation,Metric (unit),Artificial intelligence,Facial expression,Adaptation (eye),Machine learning,Domain (mathematical analysis),Limiting,Affective computing,Affect (linguistics),Mechanical engineering,Mathematical analysis,Linguistics,Operations management,Physics,Philosophy,Mathematics,Classifier (UML),Optics,Economics,Engineering",,,"Minh Tran,Yelin Kim,Che-Chun Su,Cheng-Hao Kuo,Mohammad Soleymani",
ACL2024,emotion,The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Transformer,Emotion recognition,Artificial intelligence,Natural language processing,Engineering,Electrical engineering,Voltage",,https://aclanthology.org/2024.acl-long.752.pdf,"Enshi Zhang,Rafael Trujillo,Christian Poellabauer","Research in the field of speech emotion recognition (SER) relies on the availability of comprehensive datasets to make it possible to design accurate emotion detection models. This study introduces the Multimodal Emotion Recognition and Sentiment Analysis (MERSA) dataset, which includes both natural and scripted speech recordings, transcribed text, physiological data, and self-reported emotional surveys from 150 participants collected over a two-week period. This work also presents a novel emotion recognition approach that uses a transformer-based model, integrating pre-trained wav2vec 2.0 and BERT for feature extractions and additional LSTM layers to learn hidden representations from fused representations from speech and text. Our model predicts emotions on dimensions of arousal, valence, and dominance. We trained and evaluated the model on the MSP-PODCAST dataset and achieved competitive results from the best-performing model regarding the concordance correlation coefficient (CCC). Further, this paper demonstrates the effectiveness of this model through cross-domain evaluations on both IEMOCAP and MERSA datasets."
ACL2024,emotional,ESCoT: Towards Interpretable Emotional Support Dialogue Systems,0,"Speech and dialogue systems,Topic Modeling,Natural Language Processing Techniques","Computer science,Human–computer interaction,Artificial intelligence",,https://aclanthology.org/2024.acl-long.723.pdf,"Tenggan Zhang,Xinjie Zhang,Jinming Zhao,Li Zhou,Qin Jin","Understanding the reason for emotional support response is crucial for establishing connections between users and emotional support dialogue systems. Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. To empower the system with better interpretability, we propose an emotional support response generation scheme, named "
ACL2023,affection,CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,9,"Topic Modeling,Multimodal Machine Learning Applications,Natural Language Processing Techniques","Affection,Empathy,Cognition,Conversation,Computer science,Graph,Cognitive science,Isolation (microbiology),Psychology,Cognitive psychology,Social psychology,Communication,Theoretical computer science,Neuroscience,Microbiology,Biology",,https://aclanthology.org/2023.acl-long.457.pdf,"Jinfeng Zhou,Chujie Zheng,Bo Wang,Zheng Zhang,Minlie Huang","Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user’s cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ICASSP2021,emotion,Extending Music Based On Emotion And Tonality Via Generative Adversarial Network.,3,"Music and Audio Processing,Music Technology and Sound Studies,Neuroscience and Music Perception","Tonality,Autoencoder,Generative grammar,Computer science,Active listening,Speech recognition,Generative model,Generative adversarial network,Adversarial system,Artificial intelligence,Artificial neural network,Psychology,Musical,Deep learning,Communication,Art,Visual arts",,,"Bo-Wei Tseng,Yih-Liang Shen,Tai-Shih Chi",
ICASSP2021,emotion,Language-Sensitive Music Emotion Recognition Models: are We Really There Yet?,1,"Music and Audio Processing,Neuroscience and Music Perception,Speech and Audio Processing","Computer science,Natural language processing,Salience (neuroscience),Speech recognition,Mandarin Chinese,Artificial intelligence,Exploit,Linguistics,Philosophy,Computer security",,,"Juan Sebastián Gómez Cañón,Estefanía Cano,Ana Gabriela Pandrea,Perfecto Herrera,Emilia Gómez",
ICASSP2021,emotion,LSSED: A Large-Scale Dataset and Benchmark for Speech Emotion Recognition.,24,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Benchmark (surveying),Emotion recognition,Scale (ratio),Speech recognition,Natural language processing,Artificial intelligence,Machine learning,Physics,Geodesy,Quantum mechanics,Geography",,,"Weiquan Fan,Xiangmin Xu,Xiaofen Xing,Weidong Chen,Dongyan Huang",
ICASSP2021,emotional,Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset.,119,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Prosody,Computer science,Utterance,Autoencoder,Speech recognition,Set (abstract data type),Style (visual arts),Artificial intelligence,Natural language processing,Inference,Artificial neural network,Archaeology,History,Programming language",,,"Kun Zhou,Berrak Sisman,Rui Liu,Haizhou Li",
ICASSP2021,emotion,EEG-Based Emotion Classification Using Graph Signal Processing.,19,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Advanced Graph Neural Networks","Discriminative model,Computer science,Graph,Artificial intelligence,Pattern recognition (psychology),Emotion recognition,Class (philosophy),Electroencephalography,Machine learning,Theoretical computer science,Psychology,Psychiatry",,,"Seyed Saman Saboksayr,Gonzalo Mateos,Müjdat Çetin",
ICASSP2021,emotion,Hierarchical Attention-Based Temporal Convolutional Networks for Eeg-Based Emotion Recognition.,28,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Discriminative model,Spectrogram,Recurrent neural network,Electroencephalography,Convolutional neural network,Artificial intelligence,Speech recognition,Emotion recognition,Pattern recognition (psychology),Deep learning,Representation (politics),Channel (broadcasting),Feature learning,Artificial neural network,Psychology,Computer network,Psychiatry,Politics,Political science,Law",,,"Chao Li,Boyang Chen,Ziping Zhao,Nicholas Cummins,Björn W. Schuller",
ICASSP2022,emotion,Leveraging Sparse Coding for EEG Based Emotion Recognition in Shooting.,1,"EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis,Blind Source Separation Techniques","Computer science,Electroencephalography,Leverage (statistics),Artificial intelligence,Convolutional neural network,Emotion recognition,Speech recognition,Pattern recognition (psychology),Neural coding,Coding (social sciences),Machine learning,Psychology,Mathematics,Statistics,Psychiatry",,,"Yulu Wang,Yiwen Sun,Lei Fang,Changshui Zhang",
INTERSPEECH2023,emotional,Emotional Voice Conversion with Semi-Supervised Generative Modeling,0,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Generative grammar,Generative model,Speech recognition,Artificial intelligence,Natural language processing",,https://www.isca-archive.org//interspeech_2023/zhu23b_interspeech.pdf,"Hai Zhu, Huayi Zhan, Hong Cheng, Ying Wu","Emotional Voice Conversion (EVC) is a task that aims to convert the emotional state of speech from one to another while preserving the linguistic information and identity of the speaker. However, many studies are limited by the requirement for parallel speech data between different emotional patterns, which is not widely available in real-life applications. Furthermore, the annotation of emotional data is highly time-consuming and labor-intensive. To address these problems, in this paper, we propose SGEVC, a novel semi-supervised generative model for emotional voice conversion. This paper demonstrates that using as little as 1% supervised data is sufficient to achieve EVC. Experimental results show that our proposed model achieves state-of-the-art (SOTA) performance and consistently outperforms EVC baseline frameworks."
INTERSPEECH2022,emotion,Context-aware Multimodal Fusion for Emotion Recognition,9,Emotion and Mood Recognition,"Computer science,Context (archaeology),Emotion recognition,Human–computer interaction,Speech recognition,Artificial intelligence,Paleontology,Biology",,https://www.isca-archive.org//interspeech_2022/li22v_interspeech.pdf,"Jinchao Li, Shuai Wang, Yang Chao, Xunying Liu, Helen Meng","Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings."
INTERSPEECH2022,emotion,Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition,6,Speech and Audio Processing,"Computer science,Speech recognition,Domain adaptation,Emotion recognition,Feature (linguistics),Domain (mathematical analysis),Audio visual,Adaptation (eye),Artificial intelligence,Fusion,Pattern recognition (psychology),Psychology,Multimedia,Linguistics,Mathematical analysis,Philosophy,Mathematics,Neuroscience,Classifier (UML)",https://github.com/Janie1996/AV4SER.,https://www.isca-archive.org//interspeech_2022/wei22b_interspeech.pdf,"Jie Wei, Guanyu Hu, Xinyu Yang, Anh Tuan Luu, Yizhuo Dong","Speech emotion recognition has made significant progress in recent years, in which feature representation learning has been paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose MDSCM - a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that can reduce the feature redundancy through separating spatial-wise convolution and channel-wise convolution. MDSCM also enhances the feature discriminability by the multi-attention module that focuses on learning features with more emotional information. In addition, we propose an Audio-Visual Domain Adaptation Learning paradigm (AVDAL) to learn an audio-visual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge learned from the visual domain to complement and enhance the emotional features that only extracted from speech. Domain classifier and emotion classifier are used for encoder training to reduce the mismatching of domain features, and enhance the discriminability of features for emotion recognition. The experimental results on the IEMOCAP dataset demonstrate that our proposed method outperforms other state-of-the-art speech emotion recognition systems, achieving 72.43% on weighted accuracy and 73.22% on unweighted accuracy. The code is available at https://github.com/Janie1996/AV4SER."
INTERSPEECH2022,emotion,Impact of Background Noise and Contribution of Visual Information in Emotion Identification by Native Mandarin Speakers,2,"Color perception and design,Emotion and Mood Recognition","Mandarin Chinese,Identification (biology),Speech recognition,Computer science,Noise (video),Linguistics,Natural language processing,Psychology,Artificial intelligence,Image (mathematics),Philosophy,Botany,Biology",,https://www.isca-archive.org//interspeech_2022/zhang22r_interspeech.pdf,"Minyue Zhang, Hongwei Ding","Many studies on emotion processing considered little about the issue of ecological validity and insufficient attention has been drawn to uni-sensory and multisensory emotion perception in challenging environments. The current research explored how adding multi-talker babble noise impacts emotion perception and how visual information affects the results in comparison with the audio alone conditions. Forty native Mandarin participants (21 females and 19 males) were asked to identify the emotion according to the auditory or audiovisual information they received. Results showed that the emotion identification accuracy was significantly lower in noisy conditions than in noiseless ones, whether additional visual information was presented simultaneously or not. In noisy environments, providing multisensory emotional information greatly facilitated recognition performances even when the visual information was less reliable. To conclude, multi-talker babble noise had a corrupting effect on emotion identification, which worked in both unisensory and multisensory settings, and emotion perception is a robust multisensory situation that follows the inverse effectiveness principle."
INTERSPEECH2022,emotion,Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition,4,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies","Computer science,Sentiment analysis,Modal,Emotion recognition,Speech recognition,Artificial intelligence,Emotion detection,Machine learning,Natural language processing,Chemistry,Polymer chemistry",,https://www.isca-archive.org//interspeech_2022/yang22q_interspeech.pdf,"Wei Yang, Satoru Fukayama, Panikos Heracleous, Jun Ogata","Speech-based multimodal affective computing has recently attracted significant research attention. Previous experimental results have shown that the audio-only approach exhibits inferior performance than the text-only approach in sentiment analysis and emotion recognition tasks. In this paper, we propose a new strategy to improve the performance of uni-modal and bi-modal affective computing systems via fine-tuning of two pre-trained self-supervised learning models (Text-RoBERTa and Speech-RoBERTa). We fine-tune the models on sentiment analysis and emotion recognition tasks using a shallow architecture, and apply crossmodal attention fusion to the models for further learning and final prediction or classification. We evaluate our proposed method on the CMU-MOSI, CMU-MOSEI and IEMOCAP datasets. The experimental results demonstrate that our approach exhibits superior performance for all benchmarks compared to existing state-of-the-art results, establishing the effectiveness of the proposed method."
INTERSPEECH2024,emotion,Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction,0,"Time Series Analysis and Forecasting,Anomaly Detection Techniques and Applications","Ambiguity,Ode,Computer science,Dual (grammatical number),Artificial intelligence,Artificial neural network,Machine learning,Applied mathematics,Mathematics,Art,Literature,Programming language",,https://www.isca-archive.org//interspeech_2024/wu24_interspeech.pdf,"Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah","There has been a significant focus on modelling emotion ambiguity in recent years, with advancements made in representing emotions as distributions to capture ambiguity. However, there has been comparatively less effort devoted to the consideration of temporal dependencies in emotion distributions which encodes ambiguity in perceived emotions that evolve smoothly over time. Recognizing the benefits of using constrained dynamical neural ordinary differential equations (CD-NODE) to model time series as dynamic processes, we propose an ambiguity-aware dual-constrained Neural ODE approach to model the dynamics of emotion distributions on arousal and valence. In our approach, we utilize ODEs parameterised by neural networks to estimate the distribution parameters, and we integrate additional constraints to restrict the range of the system outputs to ensure the validity of predicted distributions. We evaluated our proposed system on the publicly available RECOLA dataset and observed very promising performance across a range of evaluation metrics."
INTERSPEECH2024,emotion,Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge,0,Speech Recognition and Synthesis,"Computer science,Representation (politics),Artificial intelligence,Natural language processing,Speech recognition,Intensity (physics),Physics,Quantum mechanics,Politics,Political science,Law",https://github.com/AI-S2-Lab/EMS.,https://www.isca-archive.org//interspeech_2024/liu24r_interspeech.pdf,"Rui Liu, Zening Ma","Speech Self-Supervised Learning (SSL) has demonstrated considerable efficacy in various downstream tasks. Nevertheless, prevailing self-supervised models often overlook the incorporation of emotion-related prior information, thereby neglecting the potential enhancement of emotion task comprehension through emotion prior knowledge in speech.  In this paper, we propose an emotion-aware speech representation learning with intensity knowledge. Specifically, we extract frame-level emotion intensities using an established speech-emotion understanding model. Subsequently, we propose a novel emotional masking strategy (EMS) to incorporate emotion intensities into the masking process. We selected two representative models based on Transformer and CNN, namely MockingJay and Non-autoregressive Predictive Coding (NPC), and conducted experiments on IEMOCAP dataset. Experiments have demonstrated that the representations derived from our proposed method outperform the original model in SER task. We release the source code at https://github.com/AI-S2-Lab/EMS."
INTERSPEECH2024,emotional,VECL-TTS: Voice identity and Emotional style controllable Cross-Lingual Text-to-Speech,0,"Speech Recognition and Synthesis,Speech and dialogue systems","Style (visual arts),Computer science,Identity (music),Speech recognition,Linguistics,Art,Visual arts,Philosophy,Aesthetics",,https://www.isca-archive.org//interspeech_2024/gudmalwar24_interspeech.pdf,"Ashishkumar Gudmalwar, Nirmesh Shah, Sai Akarsh, Pankaj Wasnik, Rajiv Ratn Shah","Despite the significant advancements in Text-to-Speech (TTS) systems, their full utilization in automatic dubbing remains limited. This task necessitates the extraction of voice identity and emotional style from a reference speech in a source language and subsequently transferring them to a target language using cross-lingual TTS techniques. While previous approaches have mainly concentrated on controlling voice identity within the cross-lingual TTS framework, there has been limited work on incorporating emotion and voice identity together. To this end, we introduce an end-to-end Voice Identity and Emotional Style Controllable Cross-Lingual (VECL) TTS system using multilingual speakers and an emotion embedding network. Moreover, we introduce content and style consistency losses to enhance the quality of synthesized speech further. The proposed system achieved an average relative improvement of 8.83% compared to the state-of-the-art (SOTA) methods on a database comprising English and three Indian languages (Hindi, Telugu, and Marathi)."
EMNLP2024,affect,Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?,0,"Explainable Artificial Intelligence (XAI),Intelligent Tutoring Systems and Adaptive Learning,Online Learning and Analytics","Computer science,Affect (linguistics),Inference,Training (meteorology),Programming language,Downstream (manufacturing),Artificial intelligence,Natural language processing,Linguistics,Engineering,Operations management,Philosophy,Physics,Meteorology",,https://aclanthology.org/2024.emnlp-main.1008.pdf,"Fumiya Uchiyama,Takeshi Kojima,Andrew Gambardella,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo","Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks.Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs."
EMNLP2024,emojis,Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training,0,"Digital Communication and Language,Natural Language Processing Techniques","Computer science,Training (meteorology),Graph,Natural language processing,Artificial intelligence,Theoretical computer science,Meteorology,Physics",,https://aclanthology.org/2024.emnlp-main.989.pdf,"Zhou Zhang,Dongzeng Tan,Jiaan Wang,Yilong Chen,Jiarong Xu","Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replace text. However, existing data mining approaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model’s ability to grasp the rich semantic information in emojis and the interaction between emojis and texts. Thus, it is necessary to release the emoji’s power in social media data mining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e. post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-train framework for text and emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods."
INTERSPEECH2024,emotion,SAMSEMO: New dataset for multilingual and multimodal emotion recognition,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Natural language processing,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2024/bujnowski24_interspeech.pdf,"Pawel Bujnowski, Bartlomiej Kuzma, Bartlomiej Paziewski, Jacek Rutkowski, Joanna Marhula, Zuzanna Bordzicka, Piotr Andruszkiewicz","The task of emotion recognition using image, audio and text modalities has recently attained popularity due to its various potential applications. However, the list of large-scale multimodal datasets is very short and all available datasets have significant limitations. We present SAMSEMO, a novel dataset for multimodal and multilingual emotion recognition. Our collection of over 23k video scenes is multilingual as it includes video scenes in 5 languages (EN, DE, ES, PL and KO). Video scenes are heterogeneous, they come from diverse sources and are accompanied with rich manually collected metadata and emotion annotations. In the paper, we also study the valence and arousal of audio features of our data for the most important emotion classes and compare them with the features of CMU-MOSEI data. Moreover, we perform multimodal experiments for emotion recognition with SAMSEMO and show how to use a multilingual model to improve the detection of imbalanced classes."
INTERSPEECH2023,"emotion,emo",Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion,0,"Speech Recognition and Synthesis,Voice and Speech Disorders,Speech and Audio Processing","Computer science,Emotion recognition,Identifier,Intelligibility (philosophy),Classifier (UML),Speech recognition,Emotion detection,Emotion classification,Artificial intelligence,Natural language processing,Philosophy,Epistemology,Programming language",,https://www.isca-archive.org//interspeech_2023/ghosh23_interspeech.pdf,"Suhita Ghosh, Arnab Das, Yamini Sinha, Ingo Siegert, Tim Polzehl, Sebastian Stober","Speech anonymisation prevents misuse of spoken data by removing any personal identifier while preserving at least linguistic content. However, emotion preservation is crucial for natural human-computer interaction. The well-known voice conversion technique StarGANv2-VC achieves anonymisation but fails to preserve emotion. This work presents an any-to-many semi-supervised StarGANv2-VC variant trained on partially emotion-labelled non-parallel data. We propose emotion-aware losses computed on the emotion embeddings and acoustic features correlated to emotion. Additionally, we use an emotion classifier to provide direct emotion supervision. Objective and subjective evaluations show that the proposed approach significantly improves emotion preservation over the vanilla StarGANv2-VC. This considerable improvement is seen over diverse datasets, emotions, target speakers, and inter-group conversions without compromising intelligibility and anonymisation."
MM2023,emotion,Progressive Visual Content Understanding Network for Image Emotion Classification.,2,"Image Retrieval and Classification Techniques,Advanced Image and Video Retrieval Techniques,Visual Attention and Saliency Detection","Computer science,Artificial intelligence,Perception,Benchmark (surveying),Emotion classification,Process (computing),Segmentation,Image (mathematics),Pattern recognition (psychology),Feature extraction,Semantic gap,Emotion perception,Machine learning,Image retrieval,Facial expression,Geodesy,Neuroscience,Biology,Geography,Operating system",,,"Jicai Pan,Shangfei Wang",
MM2023,emotion,Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation.,3,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Conversation,Discriminative model,Artificial intelligence,Machine learning,Generalization,Speech recognition,Natural language processing,Mathematical analysis,Philosophy,Linguistics,Mathematics",,,"Sidharth Anand,Naresh Kumar Devulapally,Sreyasee Das Bhattacharjee,Junsong Yuan",
MM2023,"emotionkd,emotion",EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals.,13,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Modal,Emotion recognition,Field (mathematics),Artificial intelligence,Distillation,Machine learning,Pattern recognition (psychology),Speech recognition,Chemistry,Mathematics,Organic chemistry,Polymer chemistry,Pure mathematics",,,"Yucheng Liu,Ziyu Jia,Haichao Wang",
MM2023,affective,COVES: A Cognitive-Affective Deep Model that Personalizes Math Problem Difficulty in Real Time and Improves Student Engagement with an Online Tutor.,0,"Online Learning and Analytics,Educational Games and Gamification,Intelligent Tutoring Systems and Adaptive Learning","TUTOR,Boredom,Outcome (game theory),Mathematics education,Computer science,Psychology,Cove,Artificial intelligence,Social psychology,Mathematics,Mathematical economics,Geomorphology,Geology",,,"Hao Yu,Danielle A. Allessio,Will Lee,William Rebelsky,Frank Sylvia,Tom Murray,John J. Magee,Ivon Arroyo,Beverly P. Woolf,Sarah Adel Bargal,Margrit Betke",
MM2023,affective,Efficient Labelling of Affective Video Datasets via Few-Shot & Multi-Task Contrastive Learning.,4,"Emotion and Mood Recognition,Humor Studies and Applications,Sentiment Analysis and Opinion Mining","Valence (chemistry),Computer science,Arousal,Artificial intelligence,Emotion classification,Facial expression,Natural language processing,Speech recognition,Machine learning,Psychology,Physics,Quantum mechanics,Neuroscience",,,"Ravikiran Parameshwara,Ibrahim Radwan,Akshay Asthana,Iman Abbasnejad,Ramanathan Subramanian,Roland Goecke",
MM2021,emotions,Similar Scenes Arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning.,4,"Multimodal Machine Learning Applications,Video Analysis and Summarization,Advanced Image and Video Retrieval Techniques","Stylized fact,Closed captioning,Computer science,Artificial intelligence,Image (mathematics),Natural language processing,Sentence,Inference,Scale (ratio),Style (visual arts),Speech recognition,Computer vision,History,Physics,Archaeology,Quantum mechanics,Economics,Macroeconomics",,,"Guodun Li,Yuchen Zhai,Zehao Lin,Yin Zhang",
MM2021,affect,How does Color Constancy Affect Target Recognition and Instance Segmentation?,5,"Color Science and Applications,Image Enhancement Techniques,Visual perception and processing mechanisms","Artificial intelligence,Computer science,Pipeline (software),Segmentation,Computer vision,Object (grammar),Object detection,Cognitive neuroscience of visual object recognition,Transformation (genetics),Color constancy,Pattern recognition (psychology),Encoding (memory),Image segmentation,Image (mathematics),Biochemistry,Chemistry,Gene,Programming language",,,"Siyan Xue,Shaobing Gao,Minjie Tan,Zhen He,Liangtian He",
INTERSPEECH2023,emotional,Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion,3,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Consistency (knowledge bases),Pipeline (software),Speech recognition,Artificial intelligence,Emotion recognition,Emotion classification,Transfer of learning,Natural language processing,Programming language",,https://www.isca-archive.org//interspeech_2023/chen23_interspeech.pdf,"Yun Chen, Lingxiao Yang, Qi Chen, Jian-Huang Lai, Xiaohua Xie","Emotional Voice Conversion aims to manipulate a speech according to a given emotion while preserving non-emotion components. Existing approaches cannot well express fine-grained emotional attributes. In this paper, we propose an Attention-based Interactive diseNtangling Network (AINN) that leverages instance-wise emotional knowledge for voice conversion. We introduce a two-stage pipeline to effectively train our network: Stage I utilizes inter-speech contrastive learning to model fine-grained emotion and intra-speech disentanglement learning to better separate emotion and content. In Stage II, we propose to regularize the conversion with a multi-view consistency mechanism. This technique helps us transfer fine-grained emotion and maintain speech content. Extensive experiments show that our AINN outperforms state-of-the-arts in both objective and subjective metrics."
INTERSPEECH2023,emotion,Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech,2,Speech Recognition and Synthesis,"Computer science,Speech recognition,Natural language processing,Speaker diarisation,Artificial intelligence,Speaker recognition",,https://www.isca-archive.org//interspeech_2023/shaheen23_interspeech.pdf,"Zein Shaheen, Tasnima Sadekova, Yulia Matveeva, Alexandra Shirshova, Mikhail Kudinov","Text-to-Speech (TTS) systems have recently seen great progress in synthesizing high-quality speech. However, the prosody of generated utterances often is not as diverse as prosody of the natural speech. In the case of multi-speaker or voice cloning systems, this problem becomes even worse as information about prosody may be present in the input text and the speaker embedding. In this paper, we study the phenomenon of the presence of emotional information in speaker embeddings recently revealed for i-vectors and x-vectors. We show that the produced embeddings may include devoted components encoding prosodic information. We further propose a technique for finding such components and generating emotional speaker embeddings by manipulating them. We then demonstrate that the emotional TTS system based on the proposed method shows good performance and has a smaller number of trained parameters compared to solutions based on fine-tuning."
INTERSPEECH2023,emotion,Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition,2,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Discriminative model,Computer science,Leverage (statistics),Artificial intelligence,Inference,Machine learning,Speech recognition,Cross entropy,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2023/wang23q_interspeech.pdf,"Xuechen Wang, Shiwan Zhao, Yong Qin","Speech Emotion Recognition (SER) is a challenging task due to limited data and blurred boundaries of certain emotions. In this paper, we present a comprehensive approach to improve the SER performance throughout the model lifecycle, including pre-training, fine-tuning, and inference stages. To address the data scarcity issue, we utilize a pre-trained model, wav2vec2.0. During fine-tuning, we propose a novel loss function that combines cross-entropy loss with supervised contrastive learning loss to improve the model's discriminative ability. This approach increases the inter-class distances and decreases the intra-class distances, mitigating the issue of blurred boundaries. Finally, to leverage the improved distances, we propose an interpolation method at the inference stage that combines the model prediction with the output from a k-nearest neighbors model. Our experiments on IEMOCAP demonstrate that our proposed methods outperform current state-of-the-art results."
INTERSPEECH2023,emotion,Learning Local to Global Feature Aggregation for Speech Emotion Recognition,7,"Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Feature (linguistics),Speech recognition,Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2023/lu23_interspeech.pdf,"Cheng Lu, Hailun Lian, Wenming Zheng, Yuan Zong, Yan Zhao, Sunan Li","Transformer has emerged in speech emotion recognition (SER) at present. However, its equal patch division not only damages frequency information but also ignores local emotion correlations across frames, which are key cues to represent emotion. To handle the issue, we propose a Local to Global Feature Aggregation learning (LGFA) for SER, which can aggregate long-term emotion correlations at different scales both inside frames and segments with entire frequency information to enhance the emotion discrimination of utterance-level speech features. For this purpose, we nest a Frame Transformer inside a Segment Transformer. Firstly, Frame Transformer is designed to excavate local emotion correlations between frames for frame embeddings. Then, the frame embeddings and their corresponding segment features are aggregated as different-level complements to be fed into Segment Transformer for learning utterance-level global emotion features. Experimental results show that the performance of LGFA is superior to the state-of-the-art methods."
INTERSPEECH2023,emotion,Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks,2,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Speech recognition,Emotion recognition,Natural language processing,Human–computer interaction,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/chetiaphukan23_interspeech.pdf,"Orchid Chetia Phukan, Arun Balaji Buduru, Rajesh Sharma","Speech emotion recognition (SER) is a field that has drawn a lot of attention due to its applications in diverse fields. A cur- rent trend in methods used for SER is to leverage embeddings from pre-trained models (PTMs) as input features to down- stream models. However, the use of embeddings from speaker recognition PTMs hasn't garnered much focus in comparison to other PTM embeddings. To fill this gap and in order to understand the efficacy of speaker recognition PTM embed- dings, we perform a comparative analysis of five PTM embed- dings. Among all, x-vector embeddings performed the best possibly due to its training for speaker recognition leading to capturing various components of speech such as tone, pitch, etc. Our modeling approach which utilizes x-vector embed- dings and mel-frequency cepstral coefficients (MFCC) as input features is the most lightweight approach while achieving com- parable accuracy to previous state-of-the-art (SOTA) methods in the CREMA-D benchmark."
INTERSPEECH2023,emotion,Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions,0,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Leverage (statistics),Computer science,Speech recognition,Noise (video),Distillation,Artificial intelligence,Feature extraction,Acoustic model,Machine learning,Speech processing,Chemistry,Organic chemistry,Image (mathematics)",,https://www.isca-archive.org//interspeech_2023/liu23b_interspeech.pdf,"Yang Liu, Haoqin Sun, Geng Chen, Qingyue Wang, Zhen Zhao, Xugang Lu, Longbiao Wang","Speech emotion recognition (SER) performance deteriorates significantly in the presence of noise, making it challenging to achieve competitive performance in noisy conditions. To this end, we propose a multi-level knowledge distillation (MLKD) method, which aims to transfer the knowledge from a teacher model trained on clean speech to a simpler student model trained on noisy speech. Specifically, we use clean speech features extracted by the wav2vec-2.0 as the learning goal and train the distil wav2vec-2.0 to approximate the feature extraction ability of the original wav2vec-2.0 under noisy conditions. Furthermore, we leverage the multi-level knowledge of the original wav2vec-2.0 to supervise the single-level output of the distil wav2vec-2.0. We evaluate the effectiveness of our proposed method by conducting extensive experiments using five types of noise-contaminated speech on the IEMOCAP dataset, which show promising results compared to state-of-the-art models."
INTERSPEECH2023,emotion,Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters,-1,,,,https://www.isca-archive.org//interspeech_2023/leem23_interspeech.pdf,"Seong-Gyun Leem, Daniel Fulford, Jukka-Pekka Onnela, David Gard, Carlos Busso","An appealing approach for speech emotion recognition (SER) is to pre-train a large speech representation model such as Wav2Vec2.0 or HuBERT. However, this large model should be adapted to different environments when deployed on real-world applications. This approach demands additional training time and stored parameters for each target environment. This paper proposes a computation and memory-efficient adaptation method. The approach trains skip connection adapters that generate environmental representations from the convolutional encoder, and denoise the self-supervised speech representations. Our experiments with the clean and contaminated version of the MSP-Podcast corpus show that our adapter-based approach not only improves the performance of the original fine-tuned SER model, but also reduces the computation and memory requirements. For each environment, the approach requires 59.16% decreased adaptation time and only 0.98% of the parameters of the transformer encoder."
INTERSPEECH2023,emotion,Implicit phonetic information modeling for speech emotion recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/purohit23_interspeech.pdf,"Tilak Purohit, Bogdan Vlasenko, Mathew Magimai.-Doss","This study investigates the efficacy of utilizing embedding spaces to model phonetic information in emotion utterances for speech emotion recognition. Our approach involves implicit modeling of phone information by deriving phone-based embeddings from networks specifically trained for phone recognition and pre-trained models fine-tuned for phone/character recognition. The results from evaluating our approach on three speech emotion databases, using both intra-corpus and inter-corpus evaluation methods demonstrate the competitive performance of implicit modeling of phonetic information compared to knowledge-based handcrafted features"
INTERSPEECH2023,emotion,On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Speech and dialogue systems","Computer science,Robustness (evolution),Speech recognition,Joint (building),Noise (video),Word error rate,Training set,Artificial intelligence,Engineering,Image (mathematics),Gene,Architectural engineering,Biochemistry,Chemistry",,https://www.isca-archive.org//interspeech_2023/bansal23_interspeech.pdf,"Lokesh Bansal, S. Pavankumar Dubagunta, Malolan Chetlur, Pushpak Jagtap, Aravind Ganapathiraju","New-age conversational agent systems perform both speech-emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches."
INTERSPEECH2023,emotion,SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition,1,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Emotion recognition,Speech recognition,Unsupervised learning,Artificial intelligence,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2023/zhang23y_interspeech.pdf,"Ruiteng Zhang, Jianguo Wei, Xugang Lu, Yongwei Li, Junhai Xu, Di Jin, Jianhua Tao","In cross-domain speech emotion recognition (SER), reducing the global probability distribution distance (GPDD) between different domains plays a crucial role in unsupervised domain adaptation (UDA), which can be naturally measured by optimal transport (OT). However, owing to the large intra-variations of emotion categories, samples distributed in overlap may induce negative transports. Moreover, OT only considers the GPDD and therefore cannot efficiently transport hard-discriminative samples without utilizing the local structures from intra-class distributions. We propose a self-supervised learning (SSL)-assisted optimal transport (SOT) algorithm for cross-domain SER. First, we regularized OT's transport coupling to mitigate negative transports; then, we designed an SSL module to emphasize local intra-class structure to assist OT in capturing those nontransferable acknowledge. Cross-domain SER experimental results showed that SOT dramatically outperformed state-of-the-art UDAs."
INTERSPEECH2023,emotion,Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition,4,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Paralanguage,Speech recognition,End-to-end principle,Computer science,Emotion recognition,Speech technology,Speech processing,Communication,Psychology,Artificial intelligence",,https://www.isca-archive.org//interspeech_2023/ioannides23_interspeech.pdf,"Georgios Ioannides, Michael Owen, Andrew Fletcher, Viktor Rozgic, Chao Wang","We propose a methodology for information aggregation from the various transformer layer outputs of a generic speech Encoder (e.g. WavLM, HuBERT) for the downstream task of Speech Emotion Recognition (SER). The proposed methodology significantly reduces the dependency of model predictions on linguistic content, while leading to competitive performance without requiring costly Encoder re-training. The proposed paradigm is evaluated via Accuracy, Positive Pointwise Mutual Information, and visualization of the learned attention weights. This methodology generalizes well to a multi-language SER setting in addition to single-language SER, suggesting existing cultural commonalities in the paralinguistic domain between different languages. Experimental results demonstrate this ability by testing our model on unseen languages in a zero-shot fashion, suggesting our proposed method is inclusive in the context of speech and language, thus, making it applicable to a wide audience of speakers."
NAACL2021,emotion,MUSER: MUltimodal Stress detection using Emotion Recognition as an Auxiliary Task,10,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Task (project management),Computer science,Stress (linguistics),Natural language processing,Speech recognition,Artificial intelligence,Linguistics,Engineering,Philosophy,Systems engineering",,https://aclanthology.org/2021.naacl-main.216.pdf,"Yiqun Yao,Michalis Papakostas,Mihai Burzo,Mohamed Abouelenien,Rada Mihalcea","The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER – a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluation on the Multimodal Stressed Emotion (MuSE) dataset shows that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results."
NAACL2021,emotional,WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations,22,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Estimation,Computer science,Natural language processing,Intensity (physics),Artificial intelligence,Association (psychology),Linguistics,Speech recognition,Psychology,Engineering,Philosophy,Physics,Systems engineering,Quantum mechanics,Psychotherapist",,https://aclanthology.org/2021.naacl-main.169.pdf,"Tomoyuki Kajiwara,Chenhui Chu,Noriko Takemura,Yuta Nakashima,Hajime Nagahara","We annotate 17,000 SNS posts with both the writer’s subjective emotional intensity and the reader’s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer’s subjective labels than the readers’. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models."
INTERSPEECH2023,emotion,From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies","Computer science,Interval (graph theory),Hidden Markov model,Artificial intelligence,Pattern recognition (psychology),Ordinal optimization,Speech recognition,Ordinal regression,Mathematics,Machine learning,Combinatorics",,https://www.isca-archive.org//interspeech_2023/wu23j_interspeech.pdf,"Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah","Ordinal labels along affect dimensions are garnering increasing interest in computation paralinguistics. However, they are rarely obtained directly from raters, and instead typically obtained by conversion from interval labels. Current approaches to such conversion map interval labels to either absolute ordinal labels (AOL) (e.g., low and high), or to relative ordinal labels (ROL) (e.g., one has higher arousal than the other), but never take both into account. This paper presents a novel approach to map time-continuous interval labels to time-continuous ordinal labels. It simultaneously considers both inter-rater ambiguity about where AOLs sit on the interval label scale and the consistency amongst different raters in terms of ROLs. We validate the proposed approach by comparing the converted ordinal labels to original interval labels and the categorical labels for the same speech using the publicly available MSP-Podcast and MSP-Conversation corpora."
INTERSPEECH2022,affecting,Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners,0,"Phonetics and Phonology Research,Linguistic research and analysis,Linguistic Variation and Morphology","Percept,German,Scale (ratio),Computer science,Psychology,Neuroscience,Perception,Geography,Linguistics,Cartography,Philosophy",,https://www.isca-archive.org//interspeech_2022/leemann22_interspeech.pdf,"Adrian Leemann, Péter Jeszenszky, Carina Steiner, Corinne Lanthemann","In May 2018, Yanny v. Laurel went viral: when listening to the same audio clip, some people claimed to hear only Yanny, others insisted it must be Laurel, and some had a mixed percept. Phoneticians have identified the acoustic features which caused this perceptual ambiguity, but we still know little about the factors affecting individuals' perception of the illusion. We conducted a controlled study with 974 Swiss German listeners, balanced for age, gender, and regional origin. Overall, nearly two thirds heard Yanny, one quarter Laurel, and about 12% had a mixed percept. We found age, gender, and electronic device to play a significant role: younger, female, and laptop-using participants demonstrated higher proportions of Yanny responses. These findings contribute to the growing body of research on polyperceivable words."
INTERSPEECH2024,emotion,From Text to Emotion: Unveiling the Emotion Annotation Capabilities of LLMs,0,"Natural Language Processing Techniques,Semantic Web and Ontologies","Annotation,Computer science,Emotion recognition,Natural language processing,Psychology,Artificial intelligence",,https://www.isca-archive.org//interspeech_2024/niu24d_interspeech.pdf,"Minxue Niu, Mimansa Jaiswal, Emily Mower Provost","Training emotion recognition models has relied heavily on human annotated data, which present diversity, quality, and cost challenges. In this paper, we explore the potential of Large Language Models (LLMs), specifically GPT-4, in automating or assisting emotion annotation. We compare GPT-4 with supervised models and/or humans in three aspects: agreement with human annotations, alignment with human perception, and impact on model training. We find that common metrics that use aggregated human annotations as ground truth can underestimate GPT-4's performance, and our human evaluation experiment reveals a consistent preference for GPT-4 annotations over humans across multiple datasets and evaluators. Further, we investigate the impact of using GPT-4 as an annotation filtering process to improve model training. Together, our findings highlight the great potential of LLMs in emotion annotation tasks and underscore the need for refined evaluation methodologies."
INTERSPEECH2024,emotion,DropFormer: A Dynamic Noise-Dropping Transformer for Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Speech recognition,Computer science,Transformer,Noise (video),Artificial intelligence,Electrical engineering,Engineering,Voltage,Image (mathematics)",,https://www.isca-archive.org//interspeech_2024/mai24_interspeech.pdf,"Jialong Mai, Xiaofen Xing, Weidong Chen, Xiangmin Xu","Speech Emotion Recognition (SER) is an important component for human-computer interaction. Recently, various optimized Transformer variants have been applied to SER. However, most of studies use all the information in the sample and tend to overlook local details, making it difficult to perceive emotional information that is present locally in speech. While there are studies exploring how to utilize local information, their approaches are not flexible enough or are overly complex. To address the issues, we propose DropFormer, a new architecture that focuses only on the emotional segments by dynamically dropping non-emotional information. DropFormer consists of two main components: (1) Drop Attention, proficient in capturing local emotion and highlighting emotion-related segments, (2) Token Dropout Module, capable of dropping tokens lacking emotional information. Experimental results show that our DropFormer achieves state-of-the-art performance on the IEMOCAP and MELD benchmarks."
INTERSPEECH2024,emotion,Dataset-Distillation Generative Model for Speech Emotion Recognition,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Advanced Data Compression Techniques","Generative grammar,Speech recognition,Computer science,Emotion recognition,Generative model,Distillation,Natural language processing,Artificial intelligence,Chemistry,Organic chemistry",,https://www.isca-archive.org//interspeech_2024/rittergutierrez24_interspeech.pdf,"Fabian Ritter-Gutierrez, Kuan-Po Huang, Jeremy H. M. Wong, Dianwen Ng, Hung-yi Lee, Nancy F. Chen, Eng-Siong Chng","Deep learning models for speech rely on large datasets, presenting computational challenges. Yet, performance hinges on training data size. Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it.  DD has been investigated in computer vision but not yet in speech. This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training. The GAN then replaces the original dataset and can sample custom synthetic dataset sizes. It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes. It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application."
INTERSPEECH2024,emotion,ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets,0,Emotion and Mood Recognition,"Extension (predicate logic),Computer science,Block (permutation group theory),Computer architecture,Parallel computing,Programming language,Mathematics,Geometry",https://huggingface.co/amiriparian/ExHuBERT.,https://www.isca-archive.org//interspeech_2024/amiriparian24_interspeech.pdf,"Shahin Amiriparian, Filip Packań, Maurice Gerczuk, Björn W. Schuller","Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT."
INTERSPEECH2021,affective,"Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent",3,"Autism Spectrum Disorder Research,Assistive Technology in Communication and Mobility,Language Development and Disorders","Autism spectrum disorder,Affect (linguistics),Task (project management),Nonverbal communication,Facial expression,Neuropsychology,Cognitive psychology,Psychology,Gesture,Autism,Finger tapping,Kinematics,Speech recognition,Computer science,Audiology,Artificial intelligence,Developmental psychology,Cognition,Communication,Neuroscience,Medicine,Physics,Management,Classical mechanics,Economics",,https://www.isca-archive.org//interspeech_2021/kothare21_interspeech.pdf,"Hardik Kothare, Vikram Ramanarayanan, Oliver Roesler, Michael Neumann, Jackson Liscombe, William Burke, Andrew Cornish, Doug Habberstad, Alaa Sakallah, Sara Markuson, Seemran Kansara, Afik Faerman, Yasmine Bensidi-Slimane, Laura Fry, Saige Portera, David Suendermann-Oeft, David Pautler, Carly Demopoulos","We explore the utility of an on-demand multimodal conversational platform in extracting speech and facial metrics in children with Autism Spectrum Disorder (ASD). We investigate the extent to which these metrics correlate with objective clinical measures, particularly as they pertain to the interplay between the affective, phonatory and motoric subsystems. 22 participants diagnosed with ASD engaged with a virtual agent in conversational affect production tasks designed to elicit facial and vocal affect. We found significant correlations between vocal pitch and loudness extracted by our platform during these tasks and accuracy in recognition of facial and vocal affect, assessed via the Diagnostic Analysis of Nonverbal Accuracy-2 (DANVA-2) neuropsychological task. We also found significant correlations between jaw kinematic metrics extracted using our platform and motor speed of the dominant hand assessed via a standardised neuropsychological finger tapping task. These findings offer preliminary evidence for the usefulness of these audiovisual analytic metrics and could help us better model the interplay between different physiological subsystems in individuals with ASD."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
TIP2024,"emotion,emotional",Emotional Video Captioning With Vision-Based Emotion Interpretation Network.,9,"Multimodal Machine Learning Applications,Human Pose and Action Recognition,Video Analysis and Summarization","Closed captioning,Computer science,Context (archaeology),Relevance (law),Artificial intelligence,Vocabulary,Natural language processing,Sentence,Interpretation (philosophy),Visualization,Linguistics,Image (mathematics),Programming language,Paleontology,Philosophy,Political science,Law,Biology",,,"Peipei Song,Dan Guo,Xun Yang,Shengeng Tang,Meng Wang",
TIP2022,emotion,Seeking Subjectivity in Visual Emotion Distribution Learning.,14,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Subjectivity,Computer science,Interpretability,Artificial intelligence,Voting,Cognitive psychology,Matching (statistics),Machine learning,Process (computing),Natural language processing,Psychology,Mathematics,Philosophy,Statistics,Epistemology,Politics,Political science,Law,Operating system",,,"Jun Yue,Leyuan Fang,Min He",
TIP2021,emotion,Stimuli-Aware Visual Emotion Analysis.,7,"Visual Attention and Saliency Detection,Emotion and Mood Recognition,Advanced Computing and Algorithms","Computer science,Interpretability,Artificial intelligence,Entropy (arrow of time),Cognition,Feature extraction,Pattern recognition (psychology),Ambiguity,Visualization,Visual perception,Perception,Machine learning,Psychology,Physics,Quantum mechanics,Neuroscience,Programming language",,,"Zenglin Shi,Yunlu Chen,Efstratios Gavves,Pascal Mettes,Cees G. M. Snoek",
TIP2021,emotion,SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network.,33,"Visual Attention and Saliency Detection,Multimodal Machine Learning Applications,Image Retrieval and Classification Techniques","Computer science,Artificial intelligence,Interpretability,Object (grammar),Visualization,Graph,Exploit,Robustness (evolution),Convolutional neural network,Computer vision,Theoretical computer science,Biochemistry,Chemistry,Computer security,Gene",,,"Hongyan Zhang,Hongyu Chen,Guangyi Yang,Liangpei Zhang",
ACL2021,emotion,DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations,137,"Topic Modeling,Sentiment Analysis and Opinion Mining,Natural Language Processing Techniques","Computer science,Joint (building),Volume (thermodynamics),Natural language processing,Computational linguistics,Artificial intelligence,Linguistics,Association (psychology),Emotion recognition,Cognitive science,Psychology,Engineering,Philosophy,Architectural engineering,Physics,Quantum mechanics,Psychotherapist",,https://aclanthology.org/2021.acl-long.547.pdf,"Dou Hu,Lingwei Wei,Xiaoyong Huai","Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model."
ACL2024,emotional,Self-chats from Large Language Models Make Small Emotional Support Chatbot Better,0,AI in Service Interactions,"Computer science,Chatbot,Natural language processing",https://github.com/pandazzh2020/ExTES.,https://aclanthology.org/2024.acl-long.611.pdf,"Zhonghua Zheng,Lizi Liao,Yang Deng,Libo Qin,Liqiang Nie","Large Language Models (LLMs) have shown strong generalization abilities to excel in various tasks, including emotion support conversations. However, deploying such LLMs like GPT-3 (175B parameters) is resource-intensive and challenging at scale. In this study, we utilize LLMs as “Counseling Teacher” to enhance smaller models’ emotion support response abilities, significantly reducing the necessity of scaling up model size. To this end, we first introduce an iterative expansion framework, aiming to prompt the large teacher model to curate an expansive emotion support dialogue dataset. This curated dataset, termed ExTES, encompasses a broad spectrum of scenarios and is crafted with meticulous strategies to ensure its quality and comprehensiveness. Based on this, we then devise a Diverse Response Inpainting (DRI) mechanism to harness the teacher model to produce multiple diverse responses by filling in the masked conversation context. This richness and variety serve as instructive examples, providing a robust foundation for fine-tuning smaller student models. Experiments across varied scenarios reveal that the teacher-student scheme with DRI notably improves the response abilities of smaller models, even outperforming the teacher model in some cases. The dataset and codes are available in https://github.com/pandazzh2020/ExTES."
INTERSPEECH2021,emotion,Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure,18,"Wireless Sensor Networks and IoT,Advanced Computing and Algorithms,Emotion and Mood Recognition","Computer science,Speech recognition,Measure (data warehouse),Word (group theory),Natural language processing,Emotion recognition,Artificial intelligence,Linguistics,Data mining,Philosophy",,https://www.isca-archive.org//interspeech_2021/santoso21_interspeech.pdf,"Jennifer Santoso, Takeshi Yamada, Shoji Makino, Kenkichi Ishizuka, Takekatsu Hiramura","Emotion recognition is essential for human behavior analysis and possible through various inputs such as speech and images. However, in practical situations, such as in call center analysis, the available information is limited to speech. This leads to the study of speech emotion recognition (SER). Considering the complexity of emotions, SER is a challenging task. Recently, automatic speech recognition (ASR) has played a role in obtaining text information from speech. The combination of speech and ASR results has improved the SER performance. However, ASR results are highly affected by speech recognition errors. Although there is a method to improve ASR performance on emotional speech, it requires the fine-tuning of ASR, which is costly. To mitigate the errors in SER using ASR systems, we propose the use of the combination of a self-attention mechanism and a word-level confidence measure (CM), which indicates the reliability of ASR results, to reduce the importance of words with a high chance of error. Experimental results confirmed that the combination of self-attention mechanism and CM reduced the effects of incorrectly recognized words in ASR results, providing a better focus on words that determine emotion recognition. Our proposed method outperformed the state-of-the-art methods on the IEMOCAP dataset."
INTERSPEECH2023,emotion,Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder,2,"Stuttering Research and Treatment,Speech Recognition and Synthesis","Identification (biology),Bipolar disorder,Mood,Emotion recognition,Computer science,Psychology,Speech recognition,Cognitive psychology,Social psychology,Botany,Biology",,https://www.isca-archive.org//interspeech_2023/niu23b_interspeech.pdf,"Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost","Emotion is a complex behavioral phenomenon, which is expressed and perceived through various modalities, such as language, vocal and facial expressions. Psychiatric research has suggested that the lack of emotional alignment between modalities is a symptom of emotion disorders. In this work, we quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional MisMatch (EMM), as an intermediate step for mood identification. We use a longitudinal dataset collected from people with Bipolar Disorder (BP) and show that symptomatic mood episodes show significantly more EMM, compared to euthymic moods. We propose a fully automatic mood identification pipeline with automatic speech transcription, emotion recognition, and EMM feature extraction. We find that EMM features, although smaller in size, outperform a language-based baseline, and consistently provide improvement when combined with language and/or raw emotion features on mood classification."
INTERSPEECH2023,emotion,A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition,2,Emotion and Mood Recognition,"Dual (grammatical number),Computer science,Modality (human–computer interaction),Emotion recognition,Fusion,Artificial intelligence,Human–computer interaction,Linguistics,Philosophy,Art,Literature",https://github.com/zxiaohen/,https://www.isca-archive.org//interspeech_2023/zhang23g_interspeech.pdf,"Xiaoheng Zhang, Yang Li","Multi-modal emotion recognition (MER) is an emerging research field in human-computer interactions. However, previous studies have explored several fusion methods to deal with the asynchronism and the heterogeneity of multimodal data but they mostly neglect the importance of discriminative unimodal information resulting in the ignorance of independence of uni-modality. Furthermore, the complementarity among different fusion strategies is seldom taken in consideration. To address these limitations, we propose a modality-collaborative fusion network (MCFN) consisting of three main components: a dual attention-based intra-modal learning module which is devoted to build the initial embedding spaces, a modality-collaborative learning approach is to reconcile the emotional information across modalities, and a two-stage fusion strategy to integrate multimodal features which are improved by a mutual adjustment approach. The proposed framework outperforms the state-of-the-art methods in overall experiments on two well-known public datasets. Our model will be available at https://github.com/zxiaohen/ Speech-emotion-recognition-MCFN"
INTERSPEECH2022,emotional,Telling self-defining memories: An acoustic study of natural emotional speech productions,1,"Neuroscience and Music Perception,Language Development and Disorders,Neurobiology of Language and Bilingualism","Lexicon,Valence (chemistry),Psychology,Arousal,Meaning (existential),Emotional valence,Cognitive psychology,Task (project management),Speech recognition,Audiology,Computer science,Cognition,Natural language processing,Social psychology,Medicine,Physics,Management,Quantum mechanics,Neuroscience,Economics,Psychotherapist",,https://www.isca-archive.org//interspeech_2022/delvaux22_interspeech.pdf,"Veronique Delvaux, Audrey Lavallée, Fanny Degouis, Xavier Saloppe, Jean-Louis Nandrino, Thierry Pham","Vocal cues in emotion encoding are rarely studied based on real-life, naturalistic emotional speech. In the present study, 20 speakers aged 25-35 were recorded while orally telling 5 successive self-defining autobiographic memories (SDM). By definition, this task is highly emotional, although emotional load and emotion regulation are expected to vary across SDM. Seven acoustic parameters were extracted: MeanF0, MedianFo, StandardDeviationF0, MinF0, MaxF0, Duration and SpeechRate. All SDM were manually transcribed, then their emotional lexicon was analysed using Emotaix. First, speech productions were examined in reference with SDM characteristics (specificity, integrative meaning and affective valence) as determined by 3 independent investigators. Results showed that overall the speech parameters did not change over the time course of the experiment, or as a function of integrative meaning. Specific memories were recounted at a higher speech rate and at greater length than non specific ones. SDM with positive affective valence were shorter and included less variability in fundamental frequency than negative SDM. Second, emotionally-charged (positive vs. negative; high vs. low arousal) vs. emotionally-neutral utterances as to Emotaix classification were compared over all SDM. Only a few significant effects were observed, which led us to discuss the role of emotion regulation in the SDM task."
INTERSPEECH2022,emotion,Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks,9,Speech and Audio Processing,"Computer science,Speech recognition,Emotion recognition,Domain (mathematical analysis),Artificial intelligence,Natural language processing,Mathematics,Mathematical analysis",,https://www.isca-archive.org//interspeech_2022/goncalves22_interspeech.pdf,"Lucas Goncalves, Carlos Busso","ISpeech emotion recognition (SER) is a challenging task due to the limited availability of real-world labeled datasets. Since it is easier to find unlabeled data, the use of self-supervised learning (SSL) has become an attractive alternative. This study proposes new pre-text tasks for SSL to improve SER. While our target application is SER, the proposed pre-text tasks includes audiovisual formulations, leveraging the relationship between acoustic and facial features. Our proposed approach introduces three new unimodal and multimodal pre-text tasks that are carefully designed to learn better representations for predicting emotional cues from speech. Task 1 predicts energy variations (high or low) from a speech sequence. Task 2 uses speech features to predict facial activation (high or low) based on facial landmark movements. Task 3 performs a multi-class emotion recognition task on emotional labels obtained from combinations of action units (AUs) detected across a video sequence. We pre-train a network with 60.92 hours of unlabeled data, fine-tuning the model for the downstream SER task. The results on the CREMA-D dataset show that the model pre-trained on the proposed domain-specific pre-text tasks significantly improves the precision (up to 5.1%), recall (up to 4.5%), and F1-scores (up to 4.9%) of our SER system."
INTERSPEECH2022,emotions,The Magnitude and Phase based Speech Representation Learning using Autoencoder for Classifying Speech Emotions using Deep Canonical Correlation Analysis,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Innovation in Digital Healthcare Systems","Canonical correlation,Autoencoder,Computer science,Speech recognition,Artificial intelligence,Correlation,Representation (politics),Pattern recognition (psychology),Deep learning,Speech processing,Natural language processing,Mathematics,Geometry,Politics,Political science,Law",,https://www.isca-archive.org//interspeech_2022/gudmalwar22_interspeech.pdf,"Ashishkumar Gudmalwar, Biplove Basel, Anirban Dutta, Ch V Rama Rao",Speech Emotion Recognition (SER) from human speech utterances is a task of identifying emotions irrespective of their semantic content. It has an important role in making human-machine interaction natural. Conventional SER approaches emphasize more on magnitude spectrum for feature extraction and ignore phase information. Recent studies reveal that phase information has a significant part in analyzing speech acoustics. This work explores speech representation learning from magnitude and phase information using autoencoder for SER task. We trained the UNET autoencoder using Mel Frequency Cepstral Coefficients (MFCCs) and Modified Group Delay Function (MODGD) for learning representations. The encoder part of the trained UNET autoencoder is used as input to the neural network classifier and fine-tuned it concerning four emotions separately for MFCCs and MODGD. The learned representation for both MFCCs and MODGD are combined and given input to Support Vector Machine (SVM) for classification. The Deep Canonical Correlation Analysis (DCCA) is used to maximize the correlation between magnitude and phase information to improve the conventional SER system's performance. The performance analysis is carried out using the IEMOCAP database. The experimental results show improvement over MFCC features and existing approaches for unimodal SER.
INTERSPEECH2022,emotion,Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning,0,Speech and Audio Processing,"Adversarial system,Computer science,Speech recognition,Task (project management),Emotion recognition,Multi-task learning,Artificial intelligence,Natural language processing,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2022/parry22_interspeech.pdf,"Jack Parry, Eric DeMattos, Anita Klementiev, Axel Ind, Daniela Morse-Kopp, Georgia Clarke, Dimitri Palaz","Speech Emotion Recognition (SER) is an important and challenging task, especially when deploying systems in the wild i.e. on unseen data, as they tend to generalise poorly. One promising approach to improve the generalisation capabilities of SER systems is to incorporate attributes of the speech signal, such as corpus or speaker information, which can be a source of overfitting or confusion for the model. In this paper, we investigate using multi-task learning, where attribute prediction is given as an auxiliary task to the model, and adversarial learning, where the model is explicitly trained to incorrectly predict attributes. We compare two adversarial learning approaches: gradient reversal and an adversarial discriminator. We evaluate these approaches in a cross-corpus training setting using two unseen corpora as test sets. We use four attributes -- corpus, speaker, gender and language -- and evaluate all possible combinations of these attributes. We show that both multi-task learning and adversarial learning improve SER performance in the wild, with the gradient reversal approach being the most consistent across attributes and test sets."
INTERSPEECH2022,emotion,Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation,0,"Speech and dialogue systems,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Conversation,Decoding methods,Sequence (biology),Computer science,Emotion recognition,Speech recognition,Psychology,Communication,Algorithm,Biology,Genetics",,https://www.isca-archive.org//interspeech_2022/chen22n_interspeech.pdf,"Chun-Yu Chen, Yun-Shao Lin, Chi-Chun Lee","Emotion recognition in conversation (ERC) is an increasingly important topic as it improves user experiences when adopting speech technology in our daily life. In this work, we propose an emotion-shift aware decoder based on formulation of conditional random field (CRF) to address the perennial issue of poor performances when handling emotion shift in dialogues. We conduct speech emotion recognition experiments on the IEMOCAP and the NNIME and achieve a 74.47% unweighted accuracy, which is the current state-of-the-art performance in the four class emotion recognition on the IEMOCAP. This is also the first work for ERC on the NNIME that obtains an outstanding performance of 61.02\% weighted accuracy."
INTERSPEECH2022,emotion,Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition,8,"Wireless Sensor Networks and IoT,Advanced Algorithms and Applications","Computer science,Dimensionality reduction,Speech recognition,Cluster analysis,Artificial intelligence,Emotion recognition,Unsupervised learning,Semi-supervised learning,Pattern recognition (psychology),Reduction (mathematics),Machine learning,Mathematics,Geometry",,https://www.isca-archive.org//interspeech_2022/vaaras22_interspeech.pdf,"Einari Vaaras, Manu Airaksinen, Okko Räsänen","When domain experts are needed to perform data annotation for complex machine-learning tasks, reducing annotation effort is crucial in order to cut down time and expenses. For cases when there are no annotations available, one approach is to utilize the structure of the feature space for clustering-based active learning (AL) methods. However, these methods are heavily dependent on how the samples are organized in the feature space and what distance metric is used. Unsupervised methods such as contrastive predictive coding (CPC) can potentially be used to learn organized feature spaces, but these methods typically create high-dimensional features which might be challenging for estimating data density. In this paper, we combine CPC and multiple dimensionality reduction methods in search of functioning practices for clustering-based AL. Our experiments for simulating speech emotion recognition system deployment show that both the local and global topology of the feature space can be successfully used for AL, and that CPC can be used to improve clustering-based AL performance over traditional signal features. Additionally, we observe that compressing data dimensionality does not harm AL performance substantially, and that 2-D feature representations achieved similar AL performance as higher-dimensional representations when the number of annotations is not very low."
INTERSPEECH2024,emotion,SELM: Enhancing Speech Emotion Recognition for Out-of-Domain Scenarios,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Domain (mathematical analysis),Emotion recognition,Speech recognition,Computer science,Natural language processing,Mathematics,Mathematical analysis",,https://www.isca-archive.org//interspeech_2024/bukhari24_interspeech.pdf,"Hazim Bukhari, Soham Deshmukh, Hira Dhamyal, Bhiksha Raj, Rita Singh","Speech Emotion Recognition (SER) has been traditionally formulated as a classification task. However, emotions are generally a spectrum whose distribution varies from situation to situation leading to poor Out-of-Domain (OOD) performance. We take inspiration from statistical formulation of Automatic Speech Recognition (ASR) and formulate the SER task as generating the most likely sequence of text tokens to infer emotion. The formulation breaks SER into predicting acoustic model features weighted by language model prediction. As an instance of this approach, we present SELM, an audio-conditioned language model for SER that predicts different emotion views. We train SELM on curated speech emotion corpus and test it on three OOD datasets (RAVDESS, CREMAD, IEMOCAP) not used in training. SELM achieves significant improvements over the state-of-the-art baselines, with 17\% and 7\% relative accuracy gains for RAVDESS and CREMA-D, respectively. Moreover, SELM can further boost its performance by FewShot Learning using a few annotated examples. The results highlight the effectiveness of our SER formulation, especially to improve performance in OOD scenarios. "
IJCAI2023,emotions,Intensity-Valued Emotions Help Stance Detection of Climate Change Twitter Data,3,"Climate Change Communication and Perception,Public Relations and Crisis Communication,Social Media and Politics","Climate change,Task (project management),Benchmark (surveying),Computer science,Representation (politics),Artificial intelligence,Action (physics),Political science,Politics,Geography,Ecology,Management,Geodesy,Law,Economics,Biology,Physics,Quantum mechanics",,https://www.ijcai.org/proceedings/2023/0693.pdf,"Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl","Our study focuses on the United Nations Sustainable Development Goal 13: Climate Action, by identifying public attitudes on Twitter about climate change. Public consent and participation is the key factor in dealing with climate crises. However, discussions about climate change on Twitter are often influenced by the polarised beliefs that shape the discourse and divide it into communities of climate change deniers and believers. In our work, we propose a framework that helps identify different attitudes in tweets about climate change (deny, believe, ambiguous). Previous literature often lacks an efficient architecture or ignores the characteristics of climate-denier tweets. Moreover, the presence of various emotions with different levels of intensity turns out to be relevant for shaping discussions on climate change. Therefore, our paper utilizes emotion recognition and emotion intensity prediction as auxiliary tasks for our main task of stance detection. Our framework injects the words affecting the emotions embedded in the tweet to capture the overall representation of the attitude in terms of the emotions associated with it. The final task-specific and shared feature representations are fused with efficient embedding and attention techniques to detect the correct attitude of the tweet. Extensive experiments on our novel curated dataset, two publicly available climate change datasets (ClimateICWSM-2023 and ClimateStance-2022), and a benchmark dataset for stance detection (SemEval-2016) validate the effectiveness of our approach."
IJCAI2022,emotion,CauAIN: Causal Aware Interaction Network for Emotion Recognition in Conversations,42,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Utterance,Computer science,Perspective (graphical),Emotion recognition,Context (archaeology),Artificial intelligence,Natural language processing,Natural language understanding,Natural language,Paleontology,Biology",,https://www.ijcai.org/proceedings/2022/0628.pdf,"Weixiang Zhao, Yanyan Zhao, Xin Lu","Emotion Recognition in Conversations has attained increasing interest in the natural language processing community. Many neural-network based approaches endeavor to solve the challenge of emotional dynamics in conversations and gain appealing results. However, these works are limited in capturing deep emotional clues in conversational context because they ignore the emotion cause that could be viewed as stimulus to the target emotion. In this work, we propose Causal Aware Interaction Network (CauAIN) to thoroughly understand the conversational context with the help of emotion cause detection. Specifically, we retrieve causal clues provided by commonsense knowledge to guide the process of causal utterance traceback. Both retrieve and traceback steps are performed from the perspective of intra- and inter-speaker interaction simultaneously. Experimental results on three benchmark datasets show that our model achieves better performance over most baseline models."
INTERSPEECH2024,emotion,Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning,0,Speech and dialogue systems,"Computer science,Emotion recognition,Multi-task learning,Speech recognition,Cognitive psychology,Task (project management),Natural language processing,Human–computer interaction,Artificial intelligence,Psychology,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2024/goel24_interspeech.pdf,"Arnav Goel, Medha Hira, Anubha Gupta","Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy."
INTERSPEECH2024,emotion,Cross-modal Features Interaction-and-Aggregation Network with Self-consistency Training for Speech Emotion Recognition,0,Emotion and Mood Recognition,"Modal,Computer science,Speech recognition,Consistency (knowledge bases),Emotion recognition,Natural language processing,Artificial intelligence,Chemistry,Polymer chemistry",,https://www.isca-archive.org//interspeech_2024/hu24e_interspeech.pdf,"Ying Hu, Huamin Yang, Hao Huang, Liang He","In recent years, much research has been into speech emotion recognition (SER) using multimodal data. Selective fusion of the features from different modalities is critical for  multimodal SER. In this paper, we propose a cross-modal features interaction-and-aggregation network (CFIA-Net) with self-consistency training for SER. Specifically, we design a cross-modal features interaction-and-aggregation (CFIA) module to adaptively interact and integrate the features of audio and text modalities. Moreover, we introduce a self-consistency training strategy, which exploits the features from deeper layers to supervise those from shallower ones to obtain the SER task-related information. The experimental results show that compared with other bimodal SER methods, the CFIA-Net achieves the state-of-the-art performance on the weighted accuracy (WA) of 83.37%  and unweighted accuracy (UA) of 83.67% on the IEMOCAP dataset."
INTERSPEECH2021,emotion,Towards the Explainability of Multimodal Speech Emotion Recognition,28,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Emotion recognition,Speech recognition,Computer science,Natural language processing",,https://www.isca-archive.org//interspeech_2021/kumar21d_interspeech.pdf,"Puneet Kumar, Vishesh Kaushik, Balasubramanian Raman","In this paper, a multimodal speech emotion recognition system has been developed, and a novel technique to explain its predictions has been proposed. The audio and textual features are extracted separately using attention-based Gated Recurrent Unit (GRU) and pre-trained Bidirectional Encoder Representations from Transformers (BERT), respectively. Then they are concatenated and used to predict the final emotion class. The weighted and unweighted emotion recognition accuracy of 71.7% and 75.0% has been achieved on Emotional Dyadic Motion Capture (IEMOCAP) dataset containing speech utterances and corresponding text transcripts. The training and predictions of network layers have been analyzed qualitatively through emotion embedding plots and quantitatively by analyzing the intersection matrices for various emotion classes’ embeddings."
IJCAI2024,emotional,NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli,2,Topic Modeling,"Psychology,Computer science,Cognitive psychology",https://github.com/wangxu0820/NegativePrompt.,https://www.ijcai.org/proceedings/2024/0719.pdf,"Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu","Large Language Models (LLMs) have become integral to a wide spectrum of applications, ranging from traditional computing tasks to advanced artificial intelligence (AI) applications. This widespread adoption has spurred extensive research into LLMs across various disciplines, including the social sciences. Notably, studies have revealed that LLMs possess emotional intelligence, which can be further developed through positive emotional stimuli. This discovery raises an intriguing question: can negative emotions similarly influence LLMs, potentially enhancing their performance? In response to this question, we introduce NegativePrompt, a novel approach underpinned by psychological principles, involving ten specifically designed negative emotional stimuli. We embark on rigorous experimental evaluations of five LLMs including Flan-T5-Large, Vicuna, Llama 2, ChatGPT, and GPT-4, across a set of 45 tasks. The results are revealing: NegativePrompt markedly enhances the performance of LLMs, evidenced by relative improvements of 12.89% in Instruction Induction tasks and 46.25% in BIG-Bench tasks. Moreover, we conduct attention visualization experiments to decipher the underlying mechanisms of NegativePrompt's influence. Our research contributes significantly to the understanding of LLMs and emotion interaction, demonstrating the practical efficacy of NegativePrompt as an emotion-driven method and offering novel insights for the enhancement of LLMs in real-world applications. The code is available at https://github.com/wangxu0820/NegativePrompt."
IJCAI2022,emotional,"Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation",50,"Mental Health via Writing,Topic Modeling,Sentiment Analysis and Opinion Mining","Conversation,Computer science,Dialog box,Graph,Encoder,Hierarchical organization,Artificial intelligence,Human–computer interaction,Cognitive psychology,Psychology,Theoretical computer science,Communication,World Wide Web,Management,Economics,Operating system",,https://www.ijcai.org/proceedings/2022/0600.pdf,"Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun, Yunpeng Li","Emotional support conversation aims at reducing the emotional distress of the help-seeker, which is a new and challenging task. It requires the system to explore the cause of help-seeker's emotional distress and understand their psychological intention to provide supportive responses. However, existing methods mainly focus on the sequential contextual information, ignoring the hierarchical relationships with the global cause and local psychological intention behind conversations, thus leads to a weak ability of emotional support. In this paper, we propose a Global-to-Local Hierarchical Graph Network to capture the multi-source information (global cause, local intentions and dialog history) and model hierarchical relationships between them, which consists of a multi-source encoder, a hierarchical graph reasoner, and a global-guide decoder. Furthermore, a novel training objective is designed to monitor semantic information of the global cause. Experimental results on the emotional support conversation dataset, ESConv, confirm that the proposed GLHG has achieved the state-of-the-art performance on the automatic and human evaluations."
NAACL2022,affects,"How Gender Debiasing Affects Internal Model Representations, and Why It Matters",13,"Topic Modeling,Computational and Text Analysis Methods,Natural Language Processing Techniques","Debiasing,Computer science,Data science,Linguistics,Cognitive science,Psychology,Philosophy",,https://aclanthology.org/2022.naacl-main.188.pdf,"Hadas Orgad,Seraphina Goldfarb-Tarrant,Yonatan Belinkov","Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available."
NAACL2024,emona,EMONA: Event-level Moral Opinions in News Articles,1,Misinformation and Its Impacts,"Event (particle physics),Computer science,Physics,Quantum mechanics",,https://aclanthology.org/2024.naacl-long.293.pdf,"Yuanyuan Lei,Md Messal Monem Miah,Ayesha Qamar,Sai Ramana Reddy,Jonathan Tong,Haotian Xu,Ruihong Huang","Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions into three downstream tasks. The statistical analysis and experiments show that moral opinions of events can serve as informative features for identifying ideological bias or subjective events."
EMNLP2024,emotion,ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models,1,Mental Health via Writing,"Computer science,Natural language processing,Artificial intelligence,Speech recognition",,https://aclanthology.org/2024.emnlp-main.883.pdf,"Haiquan Zhao,Lingyu Li,Shisong Chen,Shuqi Kong,Jiaan Wang,Kexin Huang,Tianle Gu,Yixu Wang,Jian Wang,Liang Dandan,Zhixu Li,Yan Teng,Yanghua Xiao,Yingchun Wang","Emotion Support Conversation (ESC) is a crucial application, which aims to reduce human stress, offer emotional guidance, and ultimately enhance human mental and physical well-being. With the advancement of Large Language Models (LLMs), many researchers have employed LLMs as the ESC models. However, the evaluation of these LLM-based ESCs remains uncertain. In detail, we first re-organize 2,801 role-playing cards from seven existing datasets to define the roles of the role-playing agent. Second, we train a specific role-playing model called ESC-Role which behaves more like a confused person than GPT-4. Third, through ESC-Role and organized role cards, we systematically conduct experiments using 14 LLMs as the ESC models, including general AI-assistant LLMs (e.g., ChatGPT) and ESC-oriented LLMs (e.g., ExTES-Llama). We conduct comprehensive human annotations on interactive multi-turn dialogues of different ESC models. The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance. Moreover, to automate the scoring process for future ESC models, we developed ESC-RANK, which trained on the annotated data, achieving a scoring performance surpassing 35 points of GPT-4."
EMNLP2023,emotional,Countering Misinformation via Emotional Response Generation,2,"Misinformation and Its Impacts,Hate Speech and Cyberbullying Detection,Topic Modeling","Misinformation,Credibility,Computer science,Social media,Quality (philosophy),False accusation,Constructive,Pipeline (software),Internet privacy,Artificial intelligence,Psychology,Social psychology,World Wide Web,Computer security,Political science,Process (computing),Philosophy,Epistemology,Law,Programming language,Operating system",,https://aclanthology.org/2023.emnlp-main.703.pdf,"Daniel Russo,Shane Kaszefski-Yaschuk,Jacopo Staiano,Marco Guerini","The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread – often in good faith – misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities."
INTERSPEECH2023,affective,Affective attributes of French caregivers' professional speech,0,"Speech Recognition and Synthesis,Speech and dialogue systems,Speech and Audio Processing","Headset,Tone (literature),Computer science,Phone,Speech recognition,Quality (philosophy),Reading (process),Linguistics,Telecommunications,Philosophy,Epistemology",,https://www.isca-archive.org//interspeech_2023/rouas23_interspeech.pdf,"Jean-Luc Rouas, Yaru Wu, Takaaki Shochi","In this paper, we detail our approach to studying the vocal characteristics of caregivers in retirement homes. To achieve this goal, we conducted recordings of 20 professional caregivers across two retirement homes. Using headset microphones connected to smartphones, we were able to capture the caregivers' speech while allowing them complete freedom of movement without compromising sound quality. The recordings consisted of three tasks: reading text, informal interviews, and professional role-play scenarios with a fictitious patient. We processed the recordings using an automatic speech recognition system, which provided word or phone sequences and their corresponding timestamps. Our analysis focused on identifying differences in emotional tone, lexical content, speech rate, fundamental frequency, and intensity between spontaneous speech conditions. Ultimately, our aim is to develop automated training tools that capture the unique vocal characteristics of professional caregivers."
INTERSPEECH2023,emotion,MMER: Multimodal Multi-task Learning for Speech Emotion Recognition,12,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Emotion recognition,Speech recognition,Task (project management),Artificial intelligence,Human–computer interaction,Natural language processing,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2023/ghosh23b_interspeech.pdf,"Sreyan Ghosh, Utkarsh Tyagi, S Ramaneswaran, Harshvardhan Srivastava, Dinesh Manocha","In this paper, we propose MMER, a novel Multimodal Multi-task learning approach for Speech Emotion Recognition. MMER leverages a novel multimodal network based on early-fusion and cross-modal self-attention between text and acoustic modalities and solves three novel auxiliary tasks for learning emotion recognition from spoken utterances. In practice, MMER outperforms all our baselines and achieves state-of-the-art performance on the IEMOCAP benchmark. Additionally, we conduct extensive ablation studies and results analysis to prove the effectiveness of our proposed approach."
INTERSPEECH2022,emotion,Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition,2,Emotion and Mood Recognition,"Computer science,Speech recognition,Emotion recognition,Fusion,Head (geology),Artificial intelligence,Natural language processing,Linguistics,Philosophy,Geomorphology,Geology",,https://www.isca-archive.org//interspeech_2022/ahn22b_interspeech.pdf,"Chung-Soo Ahn, Chamara Kasun, Sunil Sivadas, Jagath Rajapakse","To infer emotions accurately from speech, fusion of audio and text is essential as words carry most information about semantics and emotions. Attention mechanism is essential component in multimodal fusion architecture as it dynamically pairs different regions within multimodal sequences. However, existing architecture lacks explicit structure to model dynamics between fused representations. Thus we propose recurrent multi-head attention in a fusion architecture, which selects salient fused representations and learns dynamics between them. Multiple 2-D attention layers select salient pairs among all possible pairs of audio and text representations, which are combined with fusion operation. Lastly, multiple fused representations are fed into recurrent unit to learn dynamics between fused representations. Our method outperforms existing approaches for fusion of audio and text for speech emotion recognition and achieves state-of-the-art accuracies on benchmark IEMOCAP dataset."
NAACL2022,emotion,Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding,8,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Topic Modeling","Zhàng,Modal,Computer science,Linguistics,Association (psychology),Cognitive science,Psychology,Artificial intelligence,Epistemology,Philosophy,History,China,Chemistry,Archaeology,Polymer chemistry",,https://aclanthology.org/2022.naacl-main.108.pdf,"Ao Jia,Yu He,Yazhou Zhang,Sagar Uprety,Dawei Song,Christina Lioma","Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task sentiment, emotion and desire dataset, which contains 9,190 text-image pairs, with English text. Each multi-modal sample is annotated with six desires, three sentiments and six emotions. We also propose the state-of-the-art baselines to evaluate the potential of MSED and show the importance of multi-task and multi-modal clues for desire understanding. We hope this study provides a benchmark for human desire analysis. MSED will be publicly available for research."
NAACL2022,emoji,Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate,32,"Hate Speech and Cyberbullying Detection,Spam and Phishing Detection,Misinformation and Its Impacts","Benchmarking,Suite,Emoji,Computer science,Test suite,Test (biology),Artificial intelligence,Machine learning,Natural language processing,Social media,Test case,World Wide Web,Geography,Paleontology,Regression analysis,Archaeology,Marketing,Biology,Business",,https://aclanthology.org/2022.naacl-main.97.pdf,"Hannah Kirk,Bertie Vidgen,Paul Rottger,Tristan Thrush,Scott Hale","Detecting online hate is a complex task, and low-performing models have harmful consequences when used for sensitive applications such as content moderation. Emoji-based hate is an emerging challenge for automated detection. We present HatemojiCheck, a test suite of 3,930 short-form statements that allows us to evaluate performance on hateful language expressed with emoji. Using the test suite, we expose weaknesses in existing hate detection models. To address these weaknesses, we create the HatemojiBuild dataset using a human-and-model-in-the-loop approach. Models built with these 5,912 adversarial examples perform substantially better at detecting emoji-based hate, while retaining strong performance on text-only hate. Both HatemojiCheck and HatemojiBuild are made publicly available."
INTERSPEECH2023,emotional,Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach,0,"Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques,Public Relations and Crisis Communication","Computer science,Natural language processing,Artificial intelligence,Data science",,https://www.isca-archive.org//interspeech_2023/stemmer23_interspeech.pdf,"Georg Stemmer, Paulo Lopez Meyer, Juan Del Hoyo Ontiveros, Jose Lopez, Hector A. Cordourier, Tobias Bocklet","Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach."
INTERSPEECH2023,emotion,Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Computer science,Joint (building),Subspace topology,Speech recognition,Feature (linguistics),Artificial intelligence,Pattern recognition (psychology),Domain (mathematical analysis),Emotion recognition,Feature extraction,Mathematics,Engineering,Architectural engineering,Mathematical analysis,Linguistics,Philosophy",,https://www.isca-archive.org//interspeech_2023/zhao23_interspeech.pdf,"Keke Zhao, Peng Song, Shaokai Li, Wenming Zheng","Speech emotion recognition is a popular research branch of speech signal processing. Many previous studies have proven that the generalization ability of the emotion recognition model across domains can be improved by using transfer learning methods. To solve the cross-domain speech emotion recognition problem, this paper proposes a novel transfer learning method, which simultaneously performs the instance reconstruction and subspace alignment. Firstly, we conduct the instance transferring based on coupled projection, which utilizes a weighting reconstruction strategy to exploit the intrinsic information of cross-domain samples and improve the contribution of essential features through an adaptive weighting matrix. Then, we conduct the feature transferring through a novel co-regularized term, which can make the source and target subspace be well aligned. Finally, extensive experiments indicate that our method is superior to several state-of-the-art methods."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
CVPR2023,"emotion,emotional",High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning,15,"Generative Adversarial Networks and Image Synthesis,Face recognition and analysis,Speech and Audio Processing","Computer science,Fidelity,Generator (circuit theory),Modal,Artificial intelligence,Encoder,Flexibility (engineering),Face (sociological concept),High fidelity,Modality (human–computer interaction),Speech recognition,Human–computer interaction,Physics,Quantum mechanics,Sociology,Polymer chemistry,Operating system,Statistics,Mathematics,Electrical engineering,Engineering,Telecommunications,Social science,Power (physics),Chemistry",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf,"Chao Xu, Junwei Zhu, Jiangning Zhang, Yue Han, Wenqing Chu, Ying Tai, Chengjie Wang, Zhifeng Xie, Yong Liu","Recently, emotional talking face generation has received considerable attention. However, existing methods only adopt one-hot coding, image, or audio as emotion conditions, thus lacking flexible control in practical applications and failing to handle unseen emotion styles due to limited semantics. They either ignore the one-shot setting or the quality of generated faces. In this paper, we propose a more flexible and generalized framework. Specifically, we supplement the emotion style in text prompts and use an Aligned Multi-modal Emotion encoder to embed the text, image, and audio emotion modality into a unified space, which inherits rich semantic prior from CLIP. Consequently, effective multi-modal emotion space learning helps our method support arbitrary emotion modality during testing and could generalize to unseen emotion styles. Besides, an Emotion-aware Audio-to-3DMM Convertor is proposed to connect the emotion condition and the audio sequence to structural representation. A followed style-based High-fidelity Emotional Face generator is designed to generate arbitrary high-resolution realistic identities. Our texture generator hierarchically learns flow fields and animated faces in a residual manner. Extensive experiments demonstrate the flexibility and generalization of our method in emotion control and the effectiveness of high-quality face synthesis."
CVPR2024,emotion,Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation,2,"Hand Gesture Recognition Systems,Speech and dialogue systems,Emotion and Mood Recognition","Transition (genetics),Computer science,Gesture,Speech recognition,Natural language processing,Artificial intelligence,Human–computer interaction,Cognitive psychology,Psychology,Biochemistry,Chemistry,Gene",https://xingqunqi-lab.github.io/Emo-Transition-Gesture/,https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_Weakly-Supervised_Emotion_Transition_Learning_for_Diverse_3D_Co-speech_Gesture_Generation_CVPR_2024_paper.pdf,"Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo",Generating vivid and emotional 3D co-speech gestures is crucial for virtual avatar animation in human-machine interaction applications. While the existing methods enable generating the gestures to follow a single emotion label they overlook that long gesture sequence modeling with emotion transition is more practical in real scenes. In addition the lack of large-scale available datasets with emotional transition speech and corresponding 3D human gestures also limits the addressing of this task. To fulfill this goal we first incorporate the ChatGPT-4 and an audio inpainting approach to construct the high-fidelity emotion transition human speeches. Considering obtaining the realistic 3D pose annotations corresponding to the dynamically inpainted emotion transition audio is extremely difficult we propose a novel weakly supervised training strategy to encourage authority gesture transitions. Specifically to enhance the coordination of transition gestures w.r.t. different emotional ones we model the temporal association representation between two different emotional gesture sequences as style guidance and infuse it into the transition generation. We further devise an emotion mixture mechanism that provides weak supervision based on a learnable mixed emotion label for transition gestures. Last we present a keyframe sampler to supply effective initial posture cues in long sequences enabling us to generate diverse gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models constructed by adapting single emotion-conditioned counterparts on our newly defined emotion transition task and datasets. Our code and dataset will be released on the project page: https://xingqunqi-lab.github.io/Emo-Transition-Gesture/
INTERSPEECH2023,emotion,Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation,2,Speech and dialogue systems,"Conversation,Utterance,Turn-taking,Computer science,Speech recognition,Emotion recognition,Emotion detection,Speaker diarisation,Natural language processing,Artificial intelligence,Speaker recognition,Psychology,Communication",,https://www.isca-archive.org//interspeech_2023/shi23e_interspeech.pdf,"Xiaohan Shi, Xingfeng Li, Tomoki Toda","The aim of emotion prediction in conversation (EPC) is to predict the future emotional state of a speaker based on context information, which is essential for conducting a friendly human-computer conversation. Most EPC works only investigated context information by merging a speaker's multiple utterances into a single utterance per turn and focused on conversations in a dual-speaker scenario, which ignored the information in multi-utterance turn and a more complex and natural scenario of multi-speaker conversations. This paper introduces a context information modeling approach that considers potential emotional interactive information within a speaker's multi-utterance turn, which dominates his/her future emotions. Moreover, our approach advances emotion prediction in both dual- and multi-speaker conversations. Experimental results show that such an approach significantly enhances context information modeling and renders a higher accuracy in EPC than reported in the literature."
INTERSPEECH2022,emotion,Extending RNN-T-based speech recognition systems with emotion and language classification,2,Speech Recognition and Synthesis,"Computer science,Speech recognition,Recurrent neural network,Emotion recognition,Artificial intelligence,Natural language processing,Artificial neural network",,https://www.isca-archive.org//interspeech_2022/kons22_interspeech.pdf,"Zvi Kons, Hagai Aronowitz, Edmilson Morais, Matheus Damasceno, Hong-Kwang Kuo, Samuel Thomas, George Saon","Speech transcription, emotion recognition, and language identification are usually considered to be three different tasks. Each one requires a different model with a different architecture and training process. We propose using a recurrent neural network transducer (RNN-T)-based speech-to-text (STT) system as a common component that can be used for emotion recognition and language identification as well as for speech recognition. Our work extends the STT system for emotion classification through minimal changes, and shows successful results on the IEMOCAP and MELD datasets. In addition, we demonstrate that by adding a lightweight component to the RNN-T module, it can also be used for language identification. In our evaluations, this new classifier demonstrates state-of-the-art accuracy for the NIST-LRE-07 dataset."
INTERSPEECH2022,emotion,Generative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition,2,"Speech and Audio Processing,Speech Recognition and Synthesis,Advanced Data Compression Techniques","Computer science,Speech recognition,Emotion recognition,Generative grammar,Artificial intelligence,Natural language processing",,https://www.isca-archive.org//interspeech_2022/wang22w_interspeech.pdf,"Shijun Wang, Hamed Hemati, Jón Guðnason, Damian Borth","Speech Emotion Recognition (SER) is crucial for human-computer interaction but still remains a challenging problem because of two major obstacles: data scarcity and imbalance. Many datasets for SER are substantially imbalanced, where data utterances of one class (most often Neutral) are much more frequent than those of other classes. Furthermore, only a few data resources are available for many existing spoken languages. To address these problems, we exploit a GAN-based augmentation model guided by a triplet network, to improve SER performance given imbalanced and insufficient training data. We conduct experiments and demonstrate: 1) With a highly imbalanced dataset, our augmentation strategy significantly improves the SER performance (+8\% recall score compared with the baseline). 2) Moreover, in a cross-lingual benchmark, where we train a model with enough source language utterances but very few target language utterances (around 50 in our experiments), our augmentation strategy brings benefits for the SER performance of all three target languages."
INTERSPEECH2022,emotion,Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition,10,"Advanced Algorithms and Applications,Speech Recognition and Synthesis,Advanced Computing and Algorithms","Computer science,Artificial intelligence,Speech recognition,Regression,Natural language processing,Transfer of learning,Emotion recognition,Pattern recognition (psychology),Mathematics,Statistics",,https://www.isca-archive.org//interspeech_2022/zhao22h_interspeech.pdf,"Yan Zhao, Jincen Wang, Ru Ye, Yuan Zong, Wenming Zheng, Li Zhao","In this paper, we focus on the research of cross-corpus speech emotion recognition (SER), in which the training (source) and testing (target) speech samples come from different corpora leading to a feature distribution gap between them. To solve this problem, we propose a simple yet effective method called deep transductive transfer regression network (DTTRN). The basic idea of DTTRN is to learn a corpus invariant deep neural network to bridge the source and target speech samples and their label information. Following this idea, we make use of a transductive learning way to enforce a deep regressor to build the relationship between the features and emotional labels jointly in both speech corpora. Meanwhile, we also design an emotion guided regularization term for learning DTTRN by aligning source and target speech samples feature distributions from three different scales. Thus, the DTTRN only absorbing the label information provided by source speech samples is able to correctly predict the emotions of the target ones. To evaluate DTTRN, we conduct extensive cross-corpus SER experiments on EmoDB, CASIA, and eNTERFACE corpora. Experimental results show the superior performance of our DTTRN over recent state-of-the-art deep transfer learning methods in dealing with the cross-corpus SER tasks."
INTERSPEECH2024,"emosphere,emotional,emotion",EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech,1,"Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Speech recognition,Style (visual arts),Computer science,Intensity (physics),Psychology,Physics,Art,Optics,Literature",,https://www.isca-archive.org//interspeech_2024/cho24_interspeech.pdf,"Deok-Hyeon Cho, Hyung-Seok Oh, Seung-Bin Kim, Sang-Hoon Lee, Seong-Whan Lee","Despite rapid advances in the field of emotional text-to-speech (TTS), recent studies primarily focus on mimicking the average style of a particular emotion. As a result, the ability to manipulate speech emotion remains constrained to several predefined labels, compromising the ability to reflect the nuanced variations of emotion. In this paper, we propose EmoSphere-TTS, which synthesizes expressive emotional speech by using a spherical emotion vector to control the emotional style and intensity of the synthetic speech. Without any human annotation, we use the arousal, valence, and dominance pseudo-labels to model the complex nature of emotion via a Cartesian-spherical transformation. Furthermore, we propose a dual conditional adversarial network to improve the quality of generated speech by reflecting the multi-aspect characteristics. The experimental results demonstrate the model’s ability to control emotional style and intensity with high-quality expressive speech."
INTERSPEECH2024,"emotion,emotional",Emotion Arithmetic: Emotional Speech Synthesis via Weight Space Interpolation,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Interpolation (computer graphics),Arithmetic,Computer science,Space (punctuation),Speech synthesis,Speech recognition,Mathematics,Artificial intelligence,Motion (physics),Operating system",,https://www.isca-archive.org//interspeech_2024/kalyan24_interspeech.pdf,"Pavan Kalyan, Preeti Rao, Preethi Jyothi, Pushpak Bhattacharyya","While the idea of task arithmetic has been shown to be useful to steer the behaviour of neural models for NLP and vision tasks, it has not yet been used for speech. Moreover the tasks studied have been restricted to text classification and generation, and image classification. We extend the idea of task vectors to emotional speech synthesis in this work. We build emotion vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning for a given emotion. These emotion vectors can be modified or combined through arithmetic operations such as negation and addition, with the hope of steering the behaviour of the resulting model accordingly in the generation of emotional speech. We also show that the emotion vector can achieve the desired transfer of emotion to a speaker not seen during training."
IJCAI2022,emotion,Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,15,"Topic Modeling,Sentiment Analysis and Opinion Mining,Advanced Graph Neural Networks","Utterance,Conversation,Computer science,Natural language processing,Construct (python library),Commonsense knowledge,Causal reasoning,Graph,Artificial intelligence,Psychology,Cognition,Knowledge extraction,Communication,Theoretical computer science,Neuroscience,Programming language",,https://www.ijcai.org/proceedings/2022/0584.pdf,"Jiangnan Li, Fandong Meng, Zheng Lin, Rui Liu, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou","Conversational Causal Emotion Entailment aims to detect causal utterances for a non-neutral targeted utterance from a conversation. In this work, we build conversations as graphs to overcome implicit contextual modelling of the original entailment style. Following the previous work, we further introduce the emotion information into graphs. Emotion information can markedly promote the detection of causal utterances whose emotion is the same as the targeted utterance. However, it is still hard to detect causal utterances with different emotions, especially neutral ones. The reason is that models are limited in reasoning causal clues and passing them between utterances. To alleviate this problem, we introduce social commonsense knowledge (CSK) and propose a Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two utterances. As not all CSK is emotionally suitable for utterances, we therefore propose a sentiment-realized knowledge selecting strategy to filter CSK. To process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph networks. Experimental results show that our method outperforms baselines and infers more causes with different emotions from the targeted utterance."
IJCAI2022,emotion,Speaker-Guided Encoder-Decoder Framework for Emotion Recognition in Conversation,13,"Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining,Topic Modeling","Computer science,Conversation,Utterance,Speech recognition,Context (archaeology),Decoding methods,Task (project management),Encoder,Speaker recognition,Flexibility (engineering),Scalability,Speaker diarisation,Artificial intelligence,Natural language processing,Database,Algorithm,Communication,Psychology,Operating system,Paleontology,Statistics,Mathematics,Management,Economics,Biology",,https://www.ijcai.org/proceedings/2022/0562.pdf,"Yinan Bao, Qianwen Ma, Lingwei Wei, Wei Zhou, Songlin Hu","The emotion recognition in conversation (ERC) task aims to predict the emotion label of an utterance in a conversation. Since the dependencies between speakers are complex and dynamic, which consist of intra- and inter-speaker dependencies, the modeling of speaker-specific information is a vital role in ERC. Although existing researchers have proposed various methods of speaker interaction modeling, they cannot explore dynamic intra- and inter-speaker dependencies jointly, leading to the insufficient comprehension of context and further hindering emotion prediction. To this end, we design a novel speaker modeling scheme that explores intra- and inter-speaker dependencies jointly in a dynamic manner. Besides, we propose a Speaker-Guided Encoder-Decoder (SGED) framework for ERC, which fully exploits speaker information for the decoding of emotion. We use different existing methods as the conversational context encoder of our framework, showing the high scalability and flexibility of the proposed framework. Experimental results demonstrate the superiority and effectiveness of SGED."
EMNLP2021,affects,“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models,8,"Topic Modeling,Natural Language Processing Techniques,Language and cultural evolution","Generative grammar,Linguistics,Framing (construction),Proposition,Psychology,Natural language,Computer science,Cognitive psychology,Philosophy,History,Archaeology",,https://aclanthology.org/2021.emnlp-main.790.pdf,"Roma Patel,Ellie Pavlick","People use language in subtle and nuanced ways to convey their beliefs. For instance, saying "
EMNLP2021,emotion,Guilt by Association: Emotion Intensities in Lexical Representations,1,"Sentiment Analysis and Opinion Mining,Topic Modeling,Mental Health via Writing","Association (psychology),Emotion detection,Task (project management),Computer science,Natural language processing,Artificial intelligence,Correlation,Word (group theory),Emotion recognition,Ground truth,Psychology,Cognitive psychology,Linguistics,Mathematics,Philosophy,Geometry,Management,Economics,Psychotherapist",,https://aclanthology.org/2021.emnlp-main.781.pdf,"Shahab Raji,Gerard de Melo","What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data."
EMNLP2023,emotion,E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation,1,"Topic Modeling,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Correlation,Perception,Empathy,Emotion perception,Computer science,Context (archaeology),Emotion detection,Emotion recognition,Emotion classification,Cognitive psychology,Psychology,Artificial intelligence,Social psychology,Facial expression,Mathematics,Paleontology,Geometry,Neuroscience,Biology",,https://aclanthology.org/2023.emnlp-main.653.pdf,"Fengyi Fu,Lei Zhang,Quan Wang,Zhendong Mao","Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework, which comprehensively realizes emotion correlation learning, utilization, and supervising. Specifically, a multi-resolution emotion graph is devised to capture context-based emotion interactions from different resolutions, further modeling emotion correlation. Then we propose an emotion correlation enhanced decoder, with a novel correlation-aware aggregation and soft/hard strategy, respectively improving the emotion perception and response generation. Experimental results on the benchmark dataset demonstrate the superiority of our model in both empathetic perception and expression."
INTERSPEECH2024,emotion,Controlling Emotion in Text-to-Speech with Natural Language Prompts,0,Speech and dialogue systems,"Natural (archaeology),Computer science,Natural language,Natural language processing,Linguistics,Speech synthesis,Psychology,Speech recognition,Cognitive psychology,History,Philosophy,Archaeology",,https://www.isca-archive.org//interspeech_2024/bott24_interspeech.pdf,"Thomas Bott, Florian Lux, Ngoc Thang Vu","In recent years, prompting has quickly become one of the standard ways of steering the outputs of generative machine learning models, due to its intuitive use of natural language. In this work, we propose a system conditioned on embeddings derived from an emotionally rich text that serves as prompt. Thereby, a joint representation of speaker and prompt embeddings is integrated at several points within a transformer-based architecture. Our approach is trained on merged emotional speech and text datasets and varies prompts in each training iteration to increase the generalization capabilities of the model. Objective and subjective evaluation results demonstrate the ability of the conditioned synthesis system to accurately transfer the emotions present in a prompt to speech. At the same time, precise tractability of speaker identities as well as overall high speech quality and intelligibility are maintained."
INTERSPEECH2024,emotion,Boosting Cross-Corpus Speech Emotion Recognition using CycleGAN with Contrastive Learning,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Boosting (machine learning),Speech recognition,Artificial intelligence,Emotion recognition,Natural language processing,Pattern recognition (psychology)",,https://www.isca-archive.org//interspeech_2024/wang24ia_interspeech.pdf,"Jincen Wang, Yan Zhao, Cheng Lu, Chuangao Tang, Sunan Li, Yuan Zong, Wenming Zheng","The premise for the success of most classic speech emotion recognition (SER) algorithms is that training and testing samples are independent and identically distributed. However, the premise is not always valid in real life. Thus, in this paper, we propose a novel transfer learning method called contrastive cycle generative adversarial network (C2GAN) to address cross-corpus SER, where training and testing data originates from different corpora. Specifically, we first adapt CycleGAN to generate synthetic data, transforming samples between source and target corpora, to enhance the variability of source data. Then, an emotion-guided contrastive learning module is introduced to jointly optimize original and synthetic data during training, leading to better class-level feature alignment. We conduct experiments on eNTERFACE, CASIA and EmoDB datasets with six different settings for evaluation. Extensive results confirm the excellent performance of C2GAN over other state-of-the-art methods."
INTERSPEECH2024,emotion,Evaluating Transformer-Enhanced Deep Reinforcement Learning for Speech Emotion Recognition,0,Speech and Audio Processing,"Reinforcement learning,Computer science,Transformer,Speech recognition,Emotion recognition,Artificial intelligence,Engineering,Voltage,Electrical engineering",,https://www.isca-archive.org//interspeech_2024/latif24_interspeech.pdf,"Siddique Latif, Raja Jurdak, Björn W. Schuller","Emotion modelling in speech using deep reinforcement learning (RL) has gained attention within the speech-emotion recognition (SER) community. However, prior studies have primarily centred around recurrent neural networks (RNNs) to capture emotional contexts, with limited exploration of the potential offered by more recent transformer architectures. This paper explores a comprehensive evaluation of training a transformer-based model using deep RL and benchmark its efficacy in SER. Specifically, we explore the effectiveness of a pre trained Wav2vec2 (w2v2) model-based classifier within the deep RL setting. We evaluate the proposed deep RL framework using five publicly available datasets and benchmark the results with three recent SER studies using two deep RL methods. Based on the results, we show that the transformer-based RL agent not only demonstrates an improvement in SER accuracy but also shows a reduction in the time taken to begin emotion classification, outpacing the RNNs that have been commonly used to date. Moreover, by leveraging pre-trained transformers, we observe a reduced need for extensive pre-training which has been a norm in prior research."
INTERSPEECH2024,emotion,WHiSER: White House Tapes Speech Emotion Recognition Corpus,0,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","White (mutation),Speech recognition,Computer science,Emotion recognition,Natural language processing,Artificial intelligence,Biochemistry,Chemistry,Gene",,https://www.isca-archive.org//interspeech_2024/naini24_interspeech.pdf,"Abinay Reddy Naini, Lucas Goncalves, Mary A. Kohler, Donita Robinson, Elizabeth Richerson, Carlos Busso","There are several applications for speech-emotion recognition (SER) systems in areas such as security and defense and healthcare. SER systems have achieved high performance when they are trained and tested in similar conditions. However, the performance often drops in more realistic and diverse conditions. Most existing SER datasets are too controlled and do not capture complex scenarios relevant to practical applications. This paper presents the White House tapes speech emotion recognition (WHiSER) corpus, which includes distant speech with real emotions from conversations in the Oval Office in 1972. This dataset is unique because it combines natural emotional expressions with various background noises, making it a perfect tool to test and improve SER models. Its real-world complexity and authenticity make the WHiSER corpus an excellent corpus for advancing emotion recognition technology, offering insights into how human emotions can be accurately recognized in complex environments."
INTERSPEECH2024,emotion,INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition,0,Speech Recognition and Synthesis,"Benchmarking,Computer science,Emotion recognition,Speech recognition,Human–computer interaction,Management,Economics",,https://www.isca-archive.org//interspeech_2024/triantafyllopoulos24b_interspeech.pdf,"Andreas Triantafyllopoulos, Anton Batliner, Simon Rampp, Manuel Milling, Björn Schuller","We revisit the INTERSPEECH 2009 Emotion Challenge -- the first ever speech emotion recognition (SER) challenge -- and evaluate a series of deep learning models that are representative of the major advances in SER research in the time since then. We start by training each model using a fixed set of hyperparameters, and further fine-tune the best-performing models of that initial setup with a grid search. Results are always reported on the official test set with a separate validation set only used for early stopping. Most models score below or close to the official baseline, while they marginally outperform the original challenge winners after hyperparameter tuning. Our work illustrates that, despite recent progress, FAU-AIBO remains a very challenging benchmark. An interesting corollary is that newer methods do not consistently outperform older ones, showing that progress towards `solving' SER is not necessarily monotonic."
INTERSPEECH2024,"emotion,emobox",EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark,2,Emotion and Mood Recognition,"Computer science,Benchmark (surveying),Natural language processing,Speech recognition,Emotion recognition,Artificial intelligence,Speech corpus,Speech synthesis,Geodesy,Geography",,https://www.isca-archive.org//interspeech_2024/ma24b_interspeech.pdf,"Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie Chen, Thomas Hain","Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia. However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult. 2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden. In this paper, we propose EmoBox, an out-of-the box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings. For intra-corpus settings, we carefully designed the data partitioning for different datasets. For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions. Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets. To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales. We hope that our toolkit and benchmark can facilitate the research of SER in the community."
ICASSP2021,emotion,Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention.,65,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Pooling,Emotion recognition,Artificial intelligence,Artificial neural network,Scale (ratio),Task (project management),Deep neural networks,Exploit,Natural language processing,Physics,Computer security,Management,Quantum mechanics,Economics",,,"Sungkyun Chang,Donmoon Lee,Jeongsoo Park,Hyungui Lim,Kyogu Lee,Karam Ko,Yoonchang Han",
ICASSP2022,emotion,Joint Temporal Convolutional Networks and Adversarial Discriminative Domain Adaptation for EEG-Based Cross-Subject Emotion Recognition.,23,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,ECG Monitoring and Analysis","Discriminative model,Computer science,Electroencephalography,Pattern recognition (psychology),Artificial intelligence,Convolutional neural network,Speech recognition,Valence (chemistry),Emotion recognition,Feature extraction,Feature (linguistics),Psychology,Linguistics,Philosophy,Physics,Quantum mechanics,Psychiatry",,,"Shaoyu Zhang,Chen Chen,Xiujuan Zhang,Silong Peng",
ICASSP2022,emotion,TNTC: Two-Stream Network with Transformer-Based Complementarity for Gait-Based Emotion Recognition.,10,"Gait Recognition and Analysis,Human Pose and Action Recognition,Hand Gesture Recognition Systems","Complementarity (molecular biology),Computer science,Artificial intelligence,Transformer,Pattern recognition (psychology),Engineering,Genetics,Voltage,Electrical engineering,Biology",,,"Hyungtae Lee,Heesung Kwon",
ICASSP2021,emotion,Cross-Corpus Speech Emotion Recognition Using Joint Distribution Adaptive Regression.,15,"Emotion and Mood Recognition,Speech and Audio Processing,Face and Expression Recognition","Computer science,Subspace topology,Feature (linguistics),Speech recognition,Joint probability distribution,Artificial intelligence,Regression,Focus (optics),Joint (building),Pattern recognition (psychology),Domain (mathematical analysis),Feature vector,Mathematics,Statistics,Architectural engineering,Mathematical analysis,Philosophy,Linguistics,Physics,Optics,Engineering",,,"Xianchao Zhang,Jie Mu,Han Liu,Xiaotong Zhang",
ICASSP2021,emotion,Subject-Invariant Eeg Representation Learning For Emotion Recognition.,8,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Anomaly Detection Techniques and Applications","Computer science,Domain adaptation,Representation (politics),Confusion,Invariant (physics),Emotion recognition,Artificial intelligence,Generalization,Speech recognition,Electroencephalography,Domain (mathematical analysis),Adaptation (eye),Pattern recognition (psychology),Adversarial system,Machine learning,Psychology,Mathematics,Classifier (UML),Mathematical analysis,Psychiatry,Neuroscience,Politics,Political science,Psychoanalysis,Law,Mathematical physics",,,"Vandad Davoodnia,Saeed Ghorbani,Ali Etemad",
ICASSP2021,emotion,Multimodal Cross- and Self-Attention Network for Speech Emotion Recognition.,46,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Utterance,Modality (human–computer interaction),Benchmark (surveying),Speech recognition,Fuse (electrical),Emotion recognition,Mechanism (biology),Modal,Artificial intelligence,Natural language processing,Philosophy,Geodesy,Engineering,Epistemology,Electrical engineering,Geography,Chemistry,Polymer chemistry",,,"Ying Cheng,Mengyu He,Jiashuo Yu,Rui Feng",
INTERSPEECH2021,emotional,Audio-Visual Recognition of Emotional Engagement of People with Dementia,6,"Emotion and Mood Recognition,Color perception and design,Multisensory perception and integration","Dementia,Audio visual,Speech recognition,Emotion recognition,Computer science,Psychology,Cognitive psychology,Multimedia,Medicine,Disease,Pathology",,https://www.isca-archive.org//interspeech_2021/steinert21_interspeech.pdf,"Lars Steinert, Felix Putze, Dennis Küster, Tanja Schultz","Dementia places an immeasurable burden on affected individuals and caregivers. In addition to general cognitive decline, dementia has a negative impact on communication. Technical activation systems are thus in high demand, as cognitive activation may help to moderate the decline. However, effective activation requires sustained engagement — which, in turn, first needs to be reliably recognized. In this study, we examine emotional engagement recognition for People with Dementia (PwD) using non-intrusive biosignals resulting from speech communication and facial expressions. PwD suffering from mild to severe dementia used a tablet-based activation system over multiple sessions. We demonstrate that they retained their ability to verbally express emotional engagement even at severe stages of the disease. For recognition of emotional engagement, we propose an architecture of Bidirectional Long-Short-Term-Memory Networks that combines video information with up to three speech-based feature sets (eGeMAPS, ComParE’13, DeepSpectrum). Using data of 24 PwD, we show that adding speech improves recognition performance significantly compared to a video-only model. Interestingly, disease-progression did not appear to have a substantial impact on recognition performance in this sample. We further discuss the opportunities and challenges of detecting emotional engagement from speech in PwD."
INTERSPEECH2022,emotion,Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection,1,Emotion and Mood Recognition,"Cadence,Encoding (memory),Modality (human–computer interaction),Computer science,Speech recognition,Artificial intelligence,Physical medicine and rehabilitation,Medicine",,https://www.isca-archive.org//interspeech_2022/dhamyal22_interspeech.pdf,"Hira Dhamyal, Bhiksha Raj, Rita Singh","Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that although the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local 'cadence', which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate `positional encodings' for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence."
INTERSPEECH2022,"emotion,emotions",Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier,11,"Speech and Audio Processing,Neural Networks and Applications,Emotion and Mood Recognition","Speech recognition,Emotion recognition,Perception,Computer science,Classifier (UML),Emotion classification,Artificial intelligence,Psychology,Neuroscience",,https://www.isca-archive.org//interspeech_2022/chou22_interspeech.pdf,"Huang-Cheng Chou, Chi-Chun Lee, Carlos Busso","Previous studies on speech emotion recognition (SER) with categorical emotions have often formulated the task as a single-label classification problem, where the emotions are considered orthogonal to each other. However, previous studies have indicated that emotions can co-occur, especially for more ambiguous emotional sentences (e.g., a mixture of happiness and surprise). Some studies have regarded SER problems as a multi-label task, predicting multiple emotional classes. However, this formulation does not leverage the relation between emotions during training, since emotions are assumed to be independent. This study explores the idea that emotional classes are not necessarily independent and its implications on training SER models. In particular, we calculate the frequency of co-occurring emotions from perceptual evaluations in the train set to generate a matrix with class-dependent penalties, punishing more mistakes between distant emotional classes. We integrate the penalization matrix into three existing label-learning approaches (hard-label, multi-label, and distribution-label learning) using the proposed modified loss. We train SER models using the penalty loss and commonly used cost functions for SER tasks. The evaluation of our proposed penalization matrix on the MSP-Podcast corpus shows important relative improvements in macro F1-score for hard-label learning (17.12%), multi-label learning (12.79%), and distribution-label learning (25.8%)."
INTERSPEECH2022,emotion,Mind the gap: On the value of silence representations to lexical-based speech emotion recognition,2,Emotion and Mood Recognition,"Silence,Speech recognition,Computer science,Value (mathematics),Natural language processing,Linguistics,Artificial intelligence,Art,Philosophy,Aesthetics,Machine learning",,https://www.isca-archive.org//interspeech_2022/perez22_interspeech.pdf,"Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost","Speech timing and non-speech regions (here referred to as ``silence""), often play a critical role in the perception of spoken language. Silence represents an important paralinguistic component in communication. For example, some of its functions include conveying emphasis, dramatization, or even sarcasm. In speech emotion recognition (SER), there has been relatively little work on investigating the utility of silence and no work regarding the effect of silence on linguistics. In this work, we present a novel framework which investigates fusing linguistic and silence representations for emotion recognition in naturalistic speech using the MSP-Podcast dataset. We investigate two methods to represent silence in SER models; the first approach uses utterance-level statistics, while the second learns a silence token embedding within a transformer language model. Our results show that modeling silence does improve SER performance and that modeling silence as a token in a transformer language model significantly improves performance on MSP-Podcast achieving a concordance correlation coefficient of .191 and .453 for activation and valence respectively. In addition, we perform analyses on the attention of silence and find that silence emphasizes the attention of its surrounding words."
INTERSPEECH2022,emotion,Probing speech emotion recognition transformers for linguistic knowledge,14,"Speech Recognition and Synthesis,Topic Modeling,Sentiment Analysis and Opinion Mining","Computer science,Valence (chemistry),Transformer,Natural language processing,Speech recognition,Artificial intelligence,Emotion recognition,Engineering,Physics,Quantum mechanics,Voltage,Electrical engineering",,https://www.isca-archive.org//interspeech_2022/triantafyllopoulos22b_interspeech.pdf,"Andreas Triantafyllopoulos, Johannes Wagner, Hagen Wierstorf, Maximilian Schmitt, Uwe Reichel, Florian Eyben, Felix Burkhardt, Björn W. Schuller","Large, pre-trained neural networks consisting of self-attention layers (transformers) have recently achieved state-of-the-art results on several speech emotion recognition (SER) datasets. These models are typically pre-trained in self-supervised manner with the goal to improve automatic speech recognition performance -- and thus, to understand linguistic information. In this work, we investigate the extent in which this information is exploited during SER fine-tuning. Using a reproducible methodology based on open-source tools, we synthesise prosodically neutral speech utterances while varying the sentiment of the text. Valence predictions of the transformer model are very reactive to positive and negative sentiment content, as well as negations, but not to intensifiers or reducers, while none of those linguistic features impact arousal or dominance. These findings show that transformers can successfully leverage linguistic information to improve their valence predictions, and that linguistic analysis should be included in their testing."
INTERSPEECH2022,emotion,The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation,3,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Conversation,Grayscale,Encoding (memory),Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Emotion detection,Human–computer interaction,Psychology,Communication,Image (mathematics)",,https://www.isca-archive.org//interspeech_2022/lee22e_interspeech.pdf,Joosung Lee,"In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines."
INTERSPEECH2022,emotion,Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms,7,"Speech and Audio Processing,Emotion and Mood Recognition","Focus (optics),Computer science,Speech recognition,Emotion recognition,Calibration,Artificial intelligence,Cognitive psychology,Psychology,Statistics,Physics,Mathematics,Optics",,https://www.isca-archive.org//interspeech_2022/kim22d_interspeech.pdf,"Junghun Kim, Yoojin An, Jihie Kim","Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets."
INTERSPEECH2022,emotion,Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise Weights in Classification Loss,6,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Speech recognition,Artificial intelligence,Natural language processing,Speech corpus,Corpus linguistics,Emotion recognition,Text corpus,Speech synthesis",,https://www.isca-archive.org//interspeech_2022/ahn22_interspeech.pdf,"Youngdo Ahn, Sung Joo Lee, Jong Won Shin","Since each of the currently available emotional speech corpora is rather small to deal with personal or cultural diversity, multiple emotional speech corpora can be jointly used to train a speech emotion recognition (SER) model robust to unseen corpora. Each corpus has different characteristics, including whether acted or spontaneous, in which environment it was recorded, and what lexical contents it contains. Depending on the characteristics, the emotion recognition accuracy and time required to train a model for it are different. If we train the SER model utilizing multiple corpora equally, the classification performance for each training corpus would be different. The performance for unseen corpora may be enhanced if the model is trained to show similar recognition accuracy for each training corpus that covers different characteristics. In this study, we propose to adopt corpus-wise weights in the classification loss, which are functions of the recognition accuracy for each of the training corpus. We also adopt pseudo-emotion labels for the unlabeled speech corpus to further enhance the performance. Experimental results showed that the proposed method outperformed previously proposed approaches in the out-of-corpus SER using three emotional corpora for training and one corpus for evaluation."
ICASSP2022,affective,Enhancing Affective Representations Of Music-Induced Eeg Through Multimodal Supervision And Latent Domain Adaptation.,3,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Neuroscience and Music Perception","Computer science,Electroencephalography,Modalities,Discriminator,Active listening,Feature learning,Artificial intelligence,Speech recognition,Cognition,Modality (human–computer interaction),Pattern recognition (psychology),Psychology,Communication,Telecommunications,Social science,Psychiatry,Detector,Sociology,Neuroscience",,,"Víctor Arroyo,Jose J. Valero-Mas,Jorge Calvo-Zaragoza,Antonio Pertusa",
ICASSP2022,emotion,Multimodal Emotion Recognition with Surgical and Fabric Masks.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Multisensory perception and integration","Speech recognition,Computer science,Emotion recognition,Modalities,Robustness (evolution),Modality (human–computer interaction),Audio visual,Facial expression,Artificial intelligence,Affective computing,Natural language processing,Multimedia,Social science,Biochemistry,Chemistry,Sociology,Gene",,,"Vandana Rajan,Alessio Brutti,Andrea Cavallaro",
ICASSP2022,emotion,Human Emotion Recognition Using Multi-Modal Biological Signals Based On Time Lag-Considered Correlation Maximization.,1,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Advanced Chemical Sensor Technologies","Lag,Gaze,Computer science,Correlation,Modal,Maximization,Canonical correlation,Artificial intelligence,Eye tracking,Time shifting,Pattern recognition (psychology),Mathematics,Transmission (telecommunications),Computer network,Mathematical optimization,Telecommunications,Chemistry,Geometry,Polymer chemistry",,,"Minh Tran,Mohammad Soleymani",
ICASSP2022,emotion,Multi-Modal Emotion Recognition with Self-Guided Modality Calibration.,12,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Humor Studies and Applications","Computer science,Modalities,Modality (human–computer interaction),Artificial intelligence,Modal,Feature (linguistics),Feature learning,Representation (politics),Sentence,Natural language processing,Sentiment analysis,Semantics (computer science),Deep learning,Machine learning,Pattern recognition (psychology),Linguistics,Social science,Chemistry,Philosophy,Sociology,Politics,Political science,Polymer chemistry,Law,Programming language",,,"Jinming Zhao,Ruichen Li,Qin Jin,Xinchao Wang,Haizhou Li",
ICASSP2022,emotion,Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?,24,"Emotion and Mood Recognition,Human Pose and Action Recognition,Face and Expression Recognition","Computer science,Modalities,Modal,Modality (human–computer interaction),Artificial intelligence,Task (project management),Feature (linguistics),Feature extraction,Fuse (electrical),Speech recognition,Pattern recognition (psychology),Machine learning,Social science,Linguistics,Chemistry,Philosophy,Management,Sociology,Polymer chemistry,Electrical engineering,Economics,Engineering",,,"Kaikai Deng,Dong Zhao,Qiaoyue Han,Zihan Zhang,Shuyue Wang,Huadong Ma",
ICASSP2022,emotion,A Pre-Trained Audio-Visual Transformer for Emotion Recognition.,22,"Music and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Computer science,Speech recognition,Robustness (evolution),Transformer,Emotion recognition,Artificial intelligence,Training set,Audio visual,Concordance correlation coefficient,Pattern recognition (psychology),Machine learning,Engineering,Multimedia,Biochemistry,Chemistry,Statistics,Mathematics,Voltage,Electrical engineering,Gene",,,"Ying Wang,Chihui Zhuang,Haihui Ye,Yan Yan,Hanzi Wang",
INTERSPEECH2024,emotion,SER Evals: In-domain and Out-of-domain benchmarking for speech emotion recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis","Benchmarking,Computer science,Speech recognition,Domain (mathematical analysis),Emotion recognition,Natural language processing,Artificial intelligence,Mathematics,Mathematical analysis,Marketing,Business",,https://www.isca-archive.org//interspeech_2024/osman24_interspeech.pdf,"Mohamed Osman, Daniel Z. Kaplan, Tamer Nadeem","Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale  benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction."
INTERSPEECH2021,emotional,An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation,7,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Speech recognition,Quality (philosophy),Voice communication,Voice analysis,Multimedia,Computer network,Philosophy,Epistemology",,https://www.isca-archive.org//interspeech_2021/he21b_interspeech.pdf,"Xiangheng He, Junjie Chen, Georgios Rizos, Björn W. Schuller","Emotional Voice Conversion (EVC) aims to convert the emotional style of a source speech signal to a target style while preserving its content and speaker identity information. Previous emotional conversion studies do not disentangle emotional information from emotion-independent information that should be preserved, thus transforming it all in a monolithic manner and generating audio of low quality, with linguistic distortions. To address this distortion problem, we propose a novel StarGAN framework along with a two-stage training process that separates emotional features from those independent of emotion by using an autoencoder with two encoders as the generator of the Generative Adversarial Network (GAN). The proposed model achieves favourable results in both the objective evaluation and the subjective evaluation in terms of distortion, which reveals that the proposed model can effectively reduce distortion. Furthermore, in data augmentation experiments for end-to-end speech emotion recognition, the proposed StarGAN model achieves an increase of 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN model, which indicates that the proposed model is more valuable for data augmentation."
MM2021,emotion,A Multi-Domain Adaptive Graph Convolutional Network for EEG-based Emotion Recognition.,44,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Electroencephalography,Computer science,Graph,Domain (mathematical analysis),Frequency domain,Exploit,Artificial intelligence,Channel (broadcasting),Pattern recognition (psychology),Speech recognition,Theoretical computer science,Psychology,Mathematics,Computer vision,Neuroscience,Mathematical analysis,Computer network,Computer security",,,"Rui Li,Yiting Wang,Bao-Liang Lu",
MM2023,emotion,Real-time Facial Animation for 3D Stylized Character with Emotion Dynamics.,6,"Face recognition and analysis,Human Motion and Animation,3D Shape Modeling and Analysis","Computer science,Animation,Stylized fact,Computer facial animation,Character animation,Character (mathematics),Computer animation,Pipeline (software),Artificial intelligence,Skeletal animation,Motion capture,Facial expression,Consistency (knowledge bases),Computer vision,Motion (physics),Computer graphics (images),Mathematics,Geometry,Economics,Macroeconomics,Programming language",,,"Ye Pan,Ruisi Zhang,Jingying Wang,Yu Ding,Kenny Mitchell",
MM2021,emotion,"MuSe 2021 Challenge: Multimodal Emotion, Sentiment, Physiological-Emotion, and Stress Detection.",27,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Computer science,Sentiment analysis,Artificial intelligence",,,"Lukas Stappen,Eva-Maria Meßner,Erik Cambria,Guoying Zhao,Björn W. Schuller",
MM2023,emotion,Multimodal Emotion Interaction and Visualization Platform.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Video Analysis and Summarization","Interactivity,Computer science,Modalities,Modality (human–computer interaction),Human–computer interaction,Multimodal interaction,Visualization,Interface (matter),Process (computing),Object (grammar),Multimodality,Multimedia,Artificial intelligence,World Wide Web,Maximum bubble pressure method,Sociology,Parallel computing,Social science,Bubble,Operating system",,,"Zheng Zhang,Songling Chen,Mixiao Hou,Guangming Lu",
MM2023,emotion,Semi-Supervised Multimodal Emotion Recognition with Expression MAE.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Face and Expression Recognition","Computer science,Expression (computer science),Artificial intelligence,Emotion recognition,Speech recognition,Pattern recognition (psychology),Natural language processing,Programming language",,,"Zebang Cheng,Yuxiang Lin,Zhaoru Chen,Xiang Li,Shuyi Mao,Fan Zhang,Daijun Ding,Bowen Zhang,Xiaojiang Peng",
MM2023,emotion,Advancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference.,2,"Music and Audio Processing,Speech Recognition and Synthesis,Speech and Audio Processing","Inference,Computer science,Bayesian inference,Bayesian probability,Speech recognition,Artificial intelligence,Machine learning,Emotion recognition",,,"Dejan Porjazovski,Yaroslav Getman,Tamás Grósz,Mikko Kurimo",
MM2023,emotion,Multi-Layer Acoustic & Linguistic Feature Fusion for ComParE-23 Emotion and Requests Challenge.,0,"Music and Audio Processing,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Pooling,Baseline (sea),Layer (electronics),Feature (linguistics),Set (abstract data type),Artificial intelligence,Standard deviation,Speech recognition,Test set,Natural language processing,Machine learning,Statistics,Linguistics,Mathematics,Oceanography,Chemistry,Philosophy,Organic chemistry,Programming language,Geology",,,"Siddhant R. Viksit,Vinayak Abrol",
NAACL2024,emotion,My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language,0,"Language, Metaphor, and Cognition","Embodied cognition,Heart beat,Computer science,Natural (archaeology),Beat (acoustics),Natural language processing,Speech recognition,Psychology,Artificial intelligence,Acoustics,History,Physics,Medicine,Archaeology,Internal medicine",https://github.com/yyzhuang1991/Embodied-Emotions.,https://aclanthology.org/2024.naacl-long.193.pdf,"Yuan Zhuang,Tianyu Jiang,Ellen Riloff","Humans frequently experience emotions. When emotions arise, they affect not only our mental state but can also change our physical state. For example, we often open our eyes wide when we are surprised, or clap our hands when we feel excited. Physical manifestations of emotions are referred to as embodied emotion in the psychology literature. From an NLP perspective, recognizing descriptions of physical movements or physiological responses associated with emotions is a type of implicit emotion recognition. Our work introduces a new task of recognizing expressions of embodied emotion in natural language. We create a dataset of sentences that contains 7,300 body part mentions with human annotations for embodied emotion. We develop a classification model for this task and present two methods to acquire weakly labeled instances of embodied emotion by extracting emotional manner expressions and by prompting a language model. Our experiments show that the weakly labeled data can train an effective classification model without gold data, and can also improve performance when combined with gold data. Our dataset is publicly available at https://github.com/yyzhuang1991/Embodied-Emotions."
NAACL2024,affect,What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception,0,Explainable Artificial Intelligence (XAI),"Affect (linguistics),Perception,Computer science,Human–computer interaction,Cognitive psychology,Psychology,Communication,Neuroscience",,https://aclanthology.org/2024.naacl-long.168.pdf,"Chaitanya Malaviya,Subin Lee,Dan Roth,Mark Yatskar","Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorrect answers and corresponding rationales in various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition, formats with attributions to the context and in-depth reasoning significantly enhance user-reported understanding and trust of model outputs."
CVPR2023,emotion,Context De-Confounded Emotion Recognition,28,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,EEG and Brain-Computer Interfaces","Spurious relationship,Computer science,Context (archaeology),Machine learning,Artificial intelligence,Causality (physics),Exploit,Task (project management),Causal model,Cognitive psychology,Benchmark (surveying),Context model,Data science,Psychology,Object (grammar),Computer security,Mathematics,Paleontology,Statistics,Physics,Management,Geodesy,Quantum mechanics,Economics,Biology,Geography",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Context_De-Confounded_Emotion_Recognition_CVPR_2023_paper.pdf,"Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai Huang, Zhiyan Dong, Peng Zhai, Lihua Zhang","Context-Aware Emotion Recognition (CAER) is a crucial and challenging task that aims to perceive the emotional states of the target person with contextual information. Recent approaches invariably focus on designing sophisticated architectures or mechanisms to extract seemingly meaningful representations from subjects and contexts. However, a long-overlooked issue is that a context bias in existing datasets leads to a significantly unbalanced distribution of emotional states among different context scenarios. Concretely, the harmful bias is a confounder that misleads existing models to learn spurious correlations based on conventional likelihood estimation, significantly limiting the models' performance. To tackle the issue, this paper provides a causality-based perspective to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a tailored causal graph. Then, we propose a Contextual Causal Intervention Module (CCIM) based on the backdoor adjustment to de-confound the confounder and exploit the true causal effect for model training. CCIM is plug-in and model-agnostic, which improves diverse state-of-the-art approaches by considerable margins. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our CCIM and the significance of causal insight."
INTERSPEECH2021,emotional,Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training,17,"Speech Recognition and Synthesis,Speech and Audio Processing,Voice and Speech Disorders","Prosody,Computer science,Utterance,Speech recognition,Initialization,Speech synthesis,Natural language processing,Style (visual arts),Sequence (biology),Quality (philosophy),Speech corpus,Artificial intelligence,Genetics,Biology,Philosophy,Archaeology,Epistemology,History,Programming language",,https://www.isca-archive.org//interspeech_2021/zhou21b_interspeech.pdf,"Kun Zhou, Berrak Sisman, Haizhou Li","Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation."
INTERSPEECH2023,emotion,"""Select language, modality or put on a mask!"" Experiments with Multimodal Emotion Recognition",1214,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Color perception and design","Affective computing,Multimodality,Modalities,Computer science,Field (mathematics),Affect (linguistics),Gesture,Cognitive computing,Data science,Cognition,Artificial intelligence,World Wide Web,Psychology,Social science,Sociology,Mathematics,Communication,Neuroscience,Pure mathematics",,https://www.isca-archive.org//interspeech_2023/bujnowski23_interspeech.pdf,"Paweł Bujnowski, Bartłomiej Kuźma, Bartłomiej Paziewski, Jacek Rutkowski, Joanna Marhula, Zuzanna Bordzicka, Piotr Andruszkiewicz","We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks."
INTERSPEECH2023,emotion,Video Multimodal Emotion Recognition System for Real World Applications,0,Emotion and Mood Recognition,"Utterance,Computer science,Interface (matter),Emotion recognition,Speech recognition,Process (computing),Human–computer interaction,Multimodal interaction,Artificial intelligence,Bubble,Maximum bubble pressure method,Parallel computing,Operating system",,https://www.isca-archive.org//interspeech_2023/lee23l_interspeech.pdf,"Sun-Kyung Lee, Jong-Hwan Kim","This paper proposes a system capable of recognizing a speaker's utterance-level emotion through multimodal cues in a video. The system seamlessly integrates multiple AI models to first extract and pre-process multimodal information from the raw video input. Next, an end-to-end MER model sequentially predicts the speaker's emotions at the utterance level. Additionally, users can interactively demonstrate the system through the implemented interface."
INTERSPEECH2023,emotion,Stable Speech Emotion Recognition with Head-k-Pooling Loss,2,"Speech and Audio Processing,Speech Recognition and Synthesis","Speech recognition,Pooling,Computer science,Head (geology),Emotion recognition,Natural language processing,Artificial intelligence,Geology,Geomorphology",,https://www.isca-archive.org//interspeech_2023/ding23_interspeech.pdf,"Chaoyue Ding, Jiakui Li, Daoming Zong, Baoxiang Li, Tian-Hao Zhang, Qunyan Zhou","Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability."
INTERSPEECH2023,emotion,"Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",0,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Speech recognition,Episodic memory,Emotion recognition,Domain (mathematical analysis),Cognitive psychology,Artificial intelligence,Psychology,Cognition,Neuroscience,Mathematical analysis,Mathematics",,https://www.isca-archive.org//interspeech_2023/tavernor23_interspeech.pdf,"James Tavernor, Matthew Perez, Emily Mower Provost","Emotion conveys abundant information that can improve the user experience of various automated systems, in addition to communicating information important for managing well-being. Human speech conveys emotion, but speech emotion recognition models do not perform well in unseen environments. This limits the ubiquitous use of speech emotion recognition models. In this paper, we investigate how a model can be adapted to unseen environments without forgetting previously learned environments. We show that memory-based methods maintain performance on previously seen environments while still being able to adapt to new environments. These methods enable continual training of speech emotion recognition models following deployment while retaining previous knowledge, working towards a more general, adaptable, acoustic model."
INTERSPEECH2023,emotion,Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack,1,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Emotion recognition,Inference,Information leakage,Task (project management),Speech recognition,Computer security,Speaker recognition,Artificial intelligence,Engineering,Systems engineering",,https://www.isca-archive.org//interspeech_2023/alsenani23_interspeech.pdf,"Basmah Alsenani, Tanaya Guha, Alessandro Vinciarelli","Increasingly more applications now use deep networks to analyse speaker's affective states. An undesirable side effect is that models trained to perform one task (e.g, emotion from speech) can be attacked to infer other, possibly privacy-sensitive attributes (e.g., gender) of the speaker. The amount of information an attacker can infer through such attacks is called leakage, and this article presents the first systematic study of the interplay between gender leakage and the main characteristics of the attacker model (family, architecture and training condition). To this end, we define various attack scenarios, and perform extensive experiments to analyse privacy risks in Speech Emotion Recognition (SER). Results show that SER models can leak a speaker's gender with an accuracy of 51% to 95% (upper bound) depending on the attack condition. Furthermore, our results provide fresh insights on how to limit the effectiveness of possible attacks and, thereby, to ensure privacy preservation."
INTERSPEECH2023,emotion,A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model,4,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Diffusion,Natural language processing,Artificial intelligence,Physics,Thermodynamics",,https://www.isca-archive.org//interspeech_2023/malik23_interspeech.pdf,"Mohammad Ibrahim Malik, Siddique Latif, Raja Jurdak, Björn W. Schuller","In this paper, we propose to utilise diffusion models for data augmentation in speech emotion recognition (SER). In particular, we present an effective approach to utilise improved denoising diffusion probabilistic models (IDDPM) to generate synthetic emotional data. We condition the IDDPM with the textual embedding from bidirectional encoder representations from transformers (BERT) to generate high-quality synthetic emotional samples in different speakers' voiceswe uploaded the synthetic samples for reviewers to listen.. We implement a series of experiments and show that better quality synthetic data helps improve SER performance. We compare results with generative adversarial networks (GANs) and show that the proposed model generates better-quality synthetic samples that can considerably improve the performance of SER when augmented with synthetic data."
ACL2023,emotion,DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations,21,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Conversation,Dual (grammatical number),Context (archaeology),Graph,Natural language processing,Emotion recognition,Artificial intelligence,Human–computer interaction,Theoretical computer science,Linguistics,Paleontology,Philosophy,Biology",,https://aclanthology.org/2023.acl-long.408.pdf,"Duzhen Zhang,Feilong Chen,Xiuyi Chen","Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT) module to incorporate discourse structural information by analyzing the discourse dependencies between utterances. Additionally, we develop a Speaker-aware GAT (SpkGAT) module to incorporate speaker-aware contextual information by considering the speaker dependencies between utterances. Furthermore, we design an interaction module that facilitates the integration of the DisGAT and SpkGAT modules, enabling the effective interchange of relevant information between the two modules. We extensively evaluate our method on four datasets, and experimental results demonstrate that our proposed DualGATs surpass state-of-the-art baselines on the majority of the datasets."
ACL2021,emotion,MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation,145,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Multimodal Machine Learning Applications","Conversation,Computer science,Convolution (computer science),Artificial intelligence,Natural language processing,Joint (building),Graph,Computational linguistics,Emotion recognition,Speech recognition,Linguistics,Artificial neural network,Theoretical computer science,Engineering,Philosophy,Architectural engineering",,https://aclanthology.org/2021.acl-long.440.pdf,"Jingwen Hu,Yuchen Liu,Jinming Zhao,Qin Jin","Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting."
INTERSPEECH2023,emotion,The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers,2,Speech Recognition and Synthesis,"Calibration,Computer science,Speech recognition,Emotion recognition,Artificial intelligence,Natural language processing,Machine learning,Statistics,Mathematics",,https://www.isca-archive.org//interspeech_2023/chou23_interspeech.pdf,"Huang-Cheng Chou, Lucas Goncalves, Seong-Gyun Leem, Chi-Chun Lee, Carlos Busso","The uncertainty in modeling emotions makes speech emotion recognition (SER) systems less reliable. An intuitive way to increase trust in SER is to reject predictions with low confidence. This approach assumes that an SER system is well calibrated, where highly confident predictions are often right and low confident predictions are often wrong. Hence, it is desirable to calibrate the confidence of SER classifiers. We evaluate the reliability of SER systems by exploring the relationship between confidence and accuracy, using the expected calibration error (ECE) metric. We develop a multi-label variant of the post-hoc temperature scaling (TS) method to calibrate SER systems, while preserving their accuracy. The best method combines an emotion co-occurrence weight penalty function, a class-balanced objective function, and the proposed multi-label TS calibration method. The experiments show the effectiveness of our developed multi-label calibration method in terms of accuracy and ECE."
INTERSPEECH2023,emotion,Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition,0,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining","Computer science,Personalization,Speech recognition,Robustness (evolution),Generalization,Encoder,Valence (chemistry),Emotion recognition,Artificial intelligence,Natural language processing,Mathematics,Mathematical analysis,Biochemistry,Chemistry,Operating system,Physics,Quantum mechanics,World Wide Web,Gene",,https://www.isca-archive.org//interspeech_2023/tran23c_interspeech.pdf,"Minh Tran, Yufeng Yin, Mohammad Soleymani","There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation."
INTERSPEECH2024,emotion,Speech Emotion Recognition with Multi-level Acoustic and Semantic Information Extraction and Interaction,0,"Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Emotion recognition,Speech recognition,Natural language processing,Feature extraction,Artificial intelligence,Human–computer interaction",,https://www.isca-archive.org//interspeech_2024/gao24f_interspeech.pdf,"Yuan Gao, Hao Shi, Chenhui Chu, Tatsuya Kawahara","Speech emotion recognition (SER) systems can learn linguistic information by integrating automatic speech recognition (ASR). However, existing SER systems fall short in explicitly learning semantic emotional information from ASR predictions. Our proposed system addresses this problem by incorporating a semantic feature extractor for explicit emotional information extraction. Furthermore, a cross attention-based information interaction module is proposed to learn the complementary emotional information in the embeddings from both feature extractors. Within the interaction module, a temporal-aware gate fusion network is incorporated to dynamically integrate the embeddings from acoustic and semantic feature extractors and mitigate the impact of ASR errors in SER. Experimental results on IEMOCAP show that our system outperforms the existing SER systems by improving the unweighted accuracy by 3.32%."
INTERSPEECH2024,emotion,An Effective Local Prototypical Mapping Network for Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Human–computer interaction,Natural language processing,Artificial intelligence",,https://www.isca-archive.org//interspeech_2024/xi24_interspeech.pdf,"Yuxuan Xi, Yan Song, Lirong Dai, Haoyu Song, Ian McLoughlin","Speech emotion recognition (SER) systems are generally optimized through utterance-level supervision, but emotion is complex and often varies within an utterance. This paper propose a local prototypical mapping network (LPMN) to model frame-level emotional variance and better exploit within-frame dynamics to improve performance. Specifically, a codebook of prototypes is first constructed to characterize complex frame-level features output from a pre-trained backbone network. An utterance-level embedding is obtained by selecting the most emotion-related mappings via a similarity measure between features and prototypes, motivated by multiple instance learning algorithms. Prototypes can be jointly optimized with quantization loss and CE loss. A prototype selection scheme is further proposed to select emotion-aware prototypes to reduce bias caused by irrelevant factors. Evaluations on IEMOCAP and MER2023 benchmarks demonstrate the effectiveness of LPMN."
INTERSPEECH2024,emotion,Confidence-aware Hypothesis Transfer Networks for Source-Free Cross-Corpus Speech Emotion Recognition,0,Speech Recognition and Synthesis,"Computer science,Speech recognition,Natural language processing,Emotion recognition,Artificial intelligence,Transfer (computing),Parallel computing",,https://www.isca-archive.org//interspeech_2024/wang24ja_interspeech.pdf,"Jincen Wang, Yan Zhao, Cheng Lu, Hailun Lian, Hongli Chang, Yuan Zong, Wenming Zheng","The goal of Source-free cross-corpus speech emotion recognition (SER) is to transfer emotion knowledge from source corpus to target one without access to source data. To address this challenge, we develop a novel method named Confidence-aware Hypothesis Transfer Network (CaHTN) including two modules. To be specific, the first module called hypothesis implicit transfer leverages the frozen source classifier (hypothesis) to force target samples to implicitly align the source hypothesis by information maximization. Besides, a bidirectional confident self-training module is designed to exploit not only the positive pseudo label information but also the negative ones for target feature extraction enhancement. To verify its effectiveness, we design twelve source-free cross-corpus SER tasks and conduct extensive experiments on CASIA, EmoDB, EMOVO and eNTERFACE. Experimental results indicate CaHTN obtains state-of-the-art performance in addressing source-free cross-corpus SER."
INTERSPEECH2024,emotion,Unsupervised Domain Adaptation for Speech Emotion Recognition using K-Nearest Neighbors Voice Conversion,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Speech recognition,Computer science,Domain adaptation,k-nearest neighbors algorithm,Domain (mathematical analysis),Adaptation (eye),Emotion recognition,Artificial intelligence,Pattern recognition (psychology),Psychology,Mathematics,Mathematical analysis,Neuroscience,Classifier (UML)",,https://www.isca-archive.org//interspeech_2024/mote24_interspeech.pdf,"Pravin Mote, Berrak Sisman, Carlos Busso","Abundant speech data for speech emotion recognition (SER) is often unlabeled, rendering it ineffective for model training. Models trained on existing labeled datasets struggle with unlabeled data due to mismatches in data distributions. To avoid the cost of annotating speech data, it is imperative to explore unsupervised adaptation techniques to leverage the potential of unlabeled data. Motivated by this observation, we propose a novel use of voice conversion (VC) for SER, which effectively enhances emotion recognition performance on an unlabeled dataset. Our approach involves leveraging the simplicity and efficacy of the k-nearest neighbor (kNN)-based VC technique to transform speech samples from the unlabeled domain to the labeled domain. In contrast to conventional domain adaptation methods, our approach avoids re-training of a model on transformed unlabeled data. We achieve good results by testing transformed unlabeled samples on a model trained with a different labeled dataset."
MM2023,emotion,Mining High-quality Samples from Raw Data and Majority Voting Method for Multimodal Emotion Recognition.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Color perception and design","Computer science,Artificial intelligence,Majority rule,Voting,Consistency (knowledge bases),Pattern recognition (psychology),Noise (video),Raw data,Set (abstract data type),Data set,Machine learning,Weighted voting,Feature (linguistics),Quality (philosophy),Feature extraction,Image (mathematics),Linguistics,Philosophy,Epistemology,Politics,Political science,Law,Programming language",,,"Qifei Li,Yingming Gao,Ya Li",
MM2023,emotion,Semi-Supervised Multimodal Emotion Recognition with Class-Balanced Pseudo-labeling.,2,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Face and Expression Recognition","Computer science,Emotion recognition,Artificial intelligence,Class (philosophy),Set (abstract data type),Machine learning,Pattern recognition (psychology),Data set,Training set,Speech recognition,Programming language",,,"Haifeng Chen,Chujia Guo,Yan Li,Peng Zhang,Dongmei Jiang",
MM2023,emotion,Multimodal Emotion Recognition in Noisy Environment Based on Progressive Label Revision.,2,"Emotion and Mood Recognition,Face and Expression Recognition,Advanced Computing and Algorithms","Computer science,Robustness (evolution),Artificial intelligence,Pattern recognition (psychology),Feature extraction,Classifier (UML),Modalities,Noise (video),Machine learning,Emotion recognition,Noise measurement,Speech recognition,Noise reduction,Social science,Biochemistry,Chemistry,Sociology,Image (mathematics),Gene",,,"Sunan Li,Hailun Lian,Cheng Lu,Yan Zhao,Chuangao Tang,Yuan Zong,Wenming Zheng",
MM2023,emotion,The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share & Requests.,9,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Computer science,Feature extraction,Multimedia,Feature (linguistics),Emotion recognition,Baseline (sea),Artificial intelligence,Machine learning,Philosophy,Oceanography,Linguistics,Geology",,,"Björn W. Schuller,Anton Batliner,Shahin Amiriparian,Alexander Barnhill,Maurice Gerczuk,Andreas Triantafyllopoulos,Alice E. Baird,Panagiotis Tzirakis,Chris Gagne,Alan S. Cowen,Nikola Lackovic,Marie-José Caraty,Claude Montacié",
MM2023,emotion,Emotion Recognition ToolKit (ERTK): Standardising Tools For Emotion Recognition Research.,3,"Emotion and Mood Recognition,Music and Audio Processing,Speech Recognition and Synthesis","Computer science,Python (programming language),Scripting language,Modular design,Emotion recognition,Software,Interface (matter),Feature (linguistics),Source code,User interface,Natural language processing,Affective computing,Artificial intelligence,Human–computer interaction,Programming language,Linguistics,Philosophy,Bubble,Maximum bubble pressure method,Parallel computing",,,"Aaron Keesing,Yun Sing Koh,Vithya Yogarajan,Michael Witbrock",
MM2023,affective,MRAC'23: 1st International Workshop on Multimodal and Responsible Affective Computing.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Affective computing,Field (mathematics),Human–computer interaction,Deep learning,Multimodal interaction,Emotion recognition,Data science,Artificial intelligence,Mathematics,Pure mathematics",,,"Zheng Lian,Erik Cambria,Guoying Zhao,Björn W. Schuller,Jianhua Tao",
EMNLP2023,"emotional,emotions",From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues,2,"Sentiment Analysis and Opinion Mining,Language, Metaphor, and Cognition,Topic Modeling","Conversation,Computer science,Commonsense knowledge,Code (set theory),Context (archaeology),Comprehension,CLARITY,Commonsense reasoning,Representation (politics),Pipeline (software),Natural language processing,Human–computer interaction,Artificial intelligence,Cognitive science,Psychology,Knowledge representation and reasoning,Communication,Paleontology,Biochemistry,Chemistry,Set (abstract data type),Politics,Political science,Law,Biology,Programming language",,https://aclanthology.org/2023.emnlp-main.598.pdf,"Shivani Kumar,Ramaneswaran S,Md Akhtar,Tanmoy Chakraborty","Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC."
EMNLP2021,emotion,Towards Label-Agnostic Emotion Embeddings,5,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Computer science,Natural language processing,Interoperability,Artificial intelligence,Sentence,Representation (politics),Natural language,Quality (philosophy),Sentiment analysis,World Wide Web,Philosophy,Epistemology,Politics,Political science,Law",,https://aclanthology.org/2021.emnlp-main.728.pdf,"Sven Buechel,Luise Modersohn,Udo Hahn","Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068."
MM2023,"affects,emotions","MuSe 2023 Challenge: Multimodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Personalised Recognition of Affects.",10,"Humor Studies and Applications,Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Personalization,Football,Sentiment analysis,Affective computing,Arousal,Test (biology),Valence (chemistry),Emotion recognition,Multimodality,German,Computer science,Psychology,Artificial intelligence,Social psychology,World Wide Web,Linguistics,Paleontology,Philosophy,Physics,Quantum mechanics,Political science,Law,Biology",,,"Shahin Amiriparian,Lukas Christ,Andreas König,Alan Cowen,Eva-Maria Meßner,Erik Cambria,Björn W. Schuller",
CVPR2022,"affective,emotional",It Is Okay To Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection,14,"Multimodal Machine Learning Applications,Human Pose and Action Recognition,Domain Adaptation and Few-Shot Learning","Computer science,Closed captioning,Affection,Code (set theory),Artificial intelligence,Emotional intelligence,Natural language processing,Image (mathematics),Psychology,Social psychology,Set (abstract data type),Programming language",https://artemisdataset-v2.org.,https://openaccess.thecvf.com/content/CVPR2022/papers/Mohamed_It_Is_Okay_To_Not_Be_Okay_Overcoming_Emotional_Bias_CVPR_2022_paper.pdf,"Youssef Mohamed, Faizan Farooq Khan, Kilichbek Haydarov, Mohamed Elhoseiny","Datasets that capture the connection between vision, language, and affection are limited, causing a lack of understanding of the emotional aspect of human intelligence. As a step in this direction, the ArtEmis dataset was recently introduced as a large-scale dataset of emotional reactions to images along with language explanations of these chosen emotions. We observed a significant emotional bias towards instance-rich emotions, making trained neural speakers less accurate in describing under-represented emotions. We show that collecting new data, in the same way, is not effective in mitigating this emotional bias. To remedy this problem, we propose a contrastive data collection approach to balance ArtEmis with a new complementary dataset such that a pair of similar images have contrasting emotions (one positive and one negative). We collected 260,533 instances using the proposed method, we combine them with ArtEmis, creating a second iteration of the dataset. The new combined dataset, dubbed ArtEmis v2.0, has a balanced distribution of emotions with explanations revealing more fine details in the associated painting. Our experiments show that neural speakers trained on the new dataset improve CIDEr and METEOR evaluation metrics by 20% and 7%, respectively, compared to the biased dataset. Finally, we also show that the performance per emotion of neural speakers is improved across all the emotion categories, significantly on under-represented emotions. The collected dataset and code are available at https://artemisdataset-v2.org."
INTERSPEECH2024,emotion,Reinforcement Learning based Data Augmentation for Noise Robust Speech Emotion Recognition,0,Speech and Audio Processing,"Reinforcement learning,Computer science,Speech recognition,Noise (video),Emotion recognition,Reinforcement,Artificial intelligence,Psychology,Social psychology,Image (mathematics)",,https://www.isca-archive.org//interspeech_2024/ranjan24_interspeech.pdf,"Sumit Ranjan, Rupayan Chakraborty, Sunil Kumar Kopparapu","Speech emotion recognition (SER) is an indispensable component of any human machine interactions, and enables building empathetic voice user interfaces. Ability to accurately recognize emotion in noisy environments is important in practical scenarios when a person is interacting with a machine or an agent as in the case of a voice based call center. In this paper, we propose reinforcement learning (RL) based data augmentation technique to enable building a robust SER system. The reward function used in RL enables picking selective noises spread over different frequency bands for data augmentation. We show that the proposed RL based augmentation technique is superior to a recently proposed random selection based technique for the noise robust SER task. We use IEMOCAP dataset with four emotion classes for validating the proposed technique. Moreover, we test the noise robustness of SER system in both cross-corpus and cross-language scenarios.                   "
INTERSPEECH2021,emotions,Acoustic and Prosodic Correlates of Emotions in Urdu Speech,2,"Speech and Audio Processing,Infant Health and Development,Multisensory perception and integration","Urdu,Speech recognition,Computer science,Linguistics,Natural language processing,Psychology,Philosophy",,https://www.isca-archive.org//interspeech_2021/urooj21_interspeech.pdf,"Saba Urooj, Benazir Mumtaz, Sarmad Hussain, Ehsan ul Haq","Emotional speech corpora exhibit differences in duration, intensity and fundamental frequency. We investigated acoustic as well as prosodic correlates of emotional speech in Urdu. We recorded a corpus of 23 sentences from four speakers of Urdu covering four emotional states. Main results show that: a) sadness exhibits lowest utterance rate, lowest intensity and narrow pitch range, b) anger exhibits highest utterance rate, highest intensity and wider pitch range, and c) happiness exhibits higher utterance rate and wider pitch range as compared to neutral and sadness; but no significant differences are found between the intensity and pitch range of anger and happiness. The analysis also shows differences in terms of pitch or phrase accents and boundary tones."
INTERSPEECH2023,"emomix,emotion,emotional",EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis,11,"Speech and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Mixing (physics),Diffusion,Computer science,Speech recognition,Physics,Thermodynamics,Quantum mechanics",,https://www.isca-archive.org//interspeech_2023/tang23_interspeech.pdf,"Haobin Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao","There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control."
CVPR2023,emotion,"Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation",15,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Topic Modeling","Computer science,Conversation,Pairwise comparison,Artificial intelligence,ENCODE,Task (project management),Graph,Context (archaeology),Multivariate statistics,Machine learning,Theoretical computer science,Psychology,Paleontology,Biochemistry,Chemistry,Management,Communication,Biology,Economics,Gene",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf,"Feiyu Chen, Jie Shao, Shuyuan Zhu, Heng Tao Shen","Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relationships in a loosely-coupled manner, which may harm relationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based ERC models fail to address some general limits of GNNs, including assuming pairwise formulation and erasing high-frequency signals, which may be trivial for many applications but crucial for the ERC task. In this paper, we propose a GNN-based model that explores multivariate relationships and captures the varying importance of emotion discrepancy and commonality by valuing multi-frequency signals. We empower GNNs to better capture the inherent relationships among utterances and deliver more sufficient multimodal and contextual modelling. Experimental results show that our proposed method outperforms previous state-of-the-art works on two popular multimodal ERC datasets."
NAACL2022,emotions,Empathic Machines: Using Intermediate Features as Levers to Emulate Emotions in Text-To-Speech Systems,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Computational linguistics,Speech recognition,Natural language processing,Artificial intelligence,Linguistics,Human–computer interaction,Cognitive science,Psychology,Philosophy",,https://aclanthology.org/2022.naacl-main.26.pdf,"Saiteja Kosgi,Sarath Sivaprasad,Niranjan Pedanekar,Anil Nelakanti,Vineet Gandhi","We present a method to control the emotional prosody of Text to Speech (TTS) systems by using phoneme-level intermediate features (pitch, energy, and duration) as levers. As a key idea, we propose Differential Scaling (DS) to disentangle features relating to affective prosody from those arising due to acoustics conditions and speaker identity. With thorough experimental studies, we show that the proposed method improves over the prior art in accurately emulating the desired emotions while retaining the naturalness of speech. We extend the traditional evaluation of using individual sentences for a more complete evaluation of HCI systems. We present a novel experimental setup by replacing an actor with a TTS system in offline and live conversations. The emotion to be rendered is either predicted or manually assigned. The results show that the proposed method is strongly preferred over the state-of-the-art TTS system and adds the much-coveted “human touch” in machine dialogue. Audio samples from our experiments and the code are available at: "
NAACL2024,emotional,The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth,0,"LGBTQ Health, Identity, and Policy","Queer,Gender studies,Psychology,Political science,Sociology",https://github.com/nitaytech/LGBTeenDataset,https://aclanthology.org/2024.naacl-long.113.pdf,"Shir Lissak,Nitay Calderon,Geva Shenkman,Yaakov Ophir,Eyal Fruchter,Anat Brunstein Klomek,Roi Reichart","Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM’s interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset"
NAACL2024,emotion,TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation,2,"Emotion and Mood Recognition,Speech and dialogue systems","Conversation,Computer science,Emotion recognition,Speech recognition,Fusion,Human–computer interaction,Multimedia,Artificial intelligence,Psychology,Communication,Linguistics,Philosophy",,https://aclanthology.org/2024.naacl-long.5.pdf,"Taeyang Yun,Hyunkuk Lim,Jeonghwan Lee,Min Song","Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue sys- tems to effectively respond to user requests. The emotions in a conversation can be identi- fied by the representations from various modal- ities, such as audio, visual, and text. How- ever, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a lan- guage model acting as the teacher to the non- verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multi- modal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effec- tiveness of our components through additional experiments."
INTERSPEECH2023,emotional,Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks,4,"Education and Learning Interventions,Social Robot Interaction and HRI,Emotion and Mood Recognition","Computer science,Head (geology),Human–computer interaction,Geology,Geomorphology",,https://www.isca-archive.org//interspeech_2023/wang23m_interspeech.pdf,"Jianrong Wang, Yaxin Zhao, Li Liu, Tianyi Xu, Qi Li, Sen Li","Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks. Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available."
INTERSPEECH2024,emotional,Towards Realistic Emotional Voice Conversion using Controllable Emotional Intensity,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Intensity (physics),Optics,Physics",,https://www.isca-archive.org//interspeech_2024/qi24_interspeech.pdf,"Tianhua Qi, Shiyan Wang, Cheng Lu, Yan Zhao, Yuan Zong, Wenming Zheng","Realistic emotional voice conversion (EVC) aims to enhance emotional diversity of converted audios, making the synthesized voices more authentic and natural. To this end, we propose Emotional Intensity-aware Network (EINet), dynamically adjusting intonation and rhythm by incorporating controllable emotional intensity. To better capture nuances in emotional intensity, we go beyond mere distance measurements among acoustic features. Instead, an emotion evaluator is utilized to precisely quantify speaker’s emotional state. By employing an intensity mapper, intensity pseudo-labels are obtained to bridge the gap between emotional speech intensity modeling and run-time conversion. To ensure high speech quality while retaining controllability, an emotion renderer is used for combining linguistic features smoothly with manipulated emotional features at frame level. Furthermore, we employ a duration predictor to facilitate adaptive prediction of rhythm changes condition on specifying intensity value. Experimental results show EINet’s superior performance in naturalness and diversity of emotional expression compared to state-of-the-art EVC methods."
ACL2024,affected,VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model,2,"Web Data Mining and Analysis,Digital and Cyber Forensics,Spam and Phishing Detection","Computer science,Vulnerability (computing),Natural language processing,Language model,Programming language,Artificial intelligence,Computer security",,https://aclanthology.org/2024.acl-long.527.pdf,"Tianyu Chen,Lin Li,ZhuLiuchuan ZhuLiuchuan,Zongyang Li,Xueqing Liu,Guangtai Liang,Qianxiang Wang,Tao Xie","Security practitioners maintain vulnerability reports (e.g., GitHub Advisory) to help developers mitigate security risks. An important task for these databases is automatically extracting structured information mentioned in the report, e.g., the affected software packages, to accelerate the defense of the vulnerability ecosystem.However, it is challenging for existing work on affected package identification to achieve high precision. One reason is that all existing work focuses on relatively smaller models, thus they cannot harness the knowledge and semantic capabilities of large language models.To address this limitation, we propose VulLibGen, the first method to use LLM for affected package identification. In contrast to existing work, VulLibGen proposes the novel idea to directly generate the affected package. To improve the precision, VulLibGen employs supervised fine-tuning (SFT), retrieval augmented generation (RAG) and a local search algorithm. The local search algorithm is a novel post-processing algorithm we introduce for reducing the hallucination of the generated packages. Our evaluation results show that VulLibGen has an average precision of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best average precision in previous work is 0.721. Additionally, VulLibGen has high value to security practice: we submitted 60 <vulnerability, affected package> pairs to GitHub Advisory (covers four ecosystems) and 34 of them have been accepted and merged."
ACL2023,emotional,Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,11,"Speech and dialogue systems,AI in Service Interactions,Topic Modeling","Conversation,Schema (genetic algorithms),Computer science,Empathy,Knowledge graph,Multimethodology,Knowledge management,Psychology,Artificial intelligence,Social psychology,Information retrieval,Mathematics education,Communication",,https://aclanthology.org/2023.acl-long.225.pdf,"Yang Deng,Wenxuan Zhang,Yifei Yuan,Wai Lam","Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimental results on two ESC datasets show the superiority of KEMI in both content-preserving evaluation and mixed initiative related analyses."
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
NIPS2024,affect,What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration,0,Speech and dialogue systems,"Affect (linguistics),Context (archaeology),Modal,Computer science,Psychology,Geology,Communication,Chemistry,Paleontology,Polymer chemistry",,https://papers.nips.cc/paper_files/paper/2024/file/deeb4d6bdb5860fd7faf321dd5486d25-Paper-Conference.pdf,"Libo Qin, Qiguang Chen, Hao Fei, Zhi Chen, Min Li, Wanxiang Che","Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: """
NIPS2024,emotion,$E^3$: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset,81,"Personality Traits and Psychology,Personality Disorders and Psychopathology,Evolutionary Psychology and Human Behavior","Trait,Personality,Psychology,Big Five personality traits,Cognitive psychology,Artificial intelligence,Social psychology,Computer vision,Computer science,Programming language",,https://papers.nips.cc/paper_files/paper/2024/file/d611d5c0251d9680f869c5d2c46c6fcd-Paper-Datasets_and_Benchmarks_Track.pdf,"wang lin, Yueying Feng, WenKang Han, Tao Jin, Zhou Zhao, Fei Wu, Chang Yao, Jingyuan Chen","Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior.  Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically.  To address this gap, this paper presents $E^3$ for Exploring Embodied Emotion, the first massive first-person view video dataset. $E^3$ contains more than $50$ hours of video, capturing $8$ different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we define $4$ core benchmark tasks - emotion recognition, emotion classification, emotion localization, and emotion reasoning - supported by more than $80$k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in first-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that $E^3$ can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents."
EMNLP2023,affect,Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?,0,"Topic Modeling,Natural Language Processing Techniques,Multimodal Machine Learning Applications","Security token,Computer science,Language model,Natural language processing,Affect (linguistics),Character (mathematics),Artificial intelligence,Semantic role labeling,Psychology,Geometry,Computer security,Mathematics,Communication,Sentence",,https://aclanthology.org/2023.emnlp-main.563.pdf,"Ahmed Alajrami,Katerina Margatina,Nikolaos Aletras","Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively."
EMNLP2022,emojis,A Federated Approach to Predicting Emojis in Hindi Tweets,2,"Hate Speech and Cyberbullying Detection,Digital Communication and Language,Internet Traffic Analysis and Secure E-voting","Emoji,Computer science,Hindi,Artificial intelligence,Machine learning,Task (project management),Natural language processing,World Wide Web,Social media,Economics,Management",,https://aclanthology.org/2022.emnlp-main.819.pdf,"Deep Gandhi,Jash Mehta,Nirali Parekh,Karan Waghela,Lynette D’Mello,Zeerak Talat","The use of emojis affords a visual modality to, often private, textual communication.The task of predicting emojis however provides a challenge for machine learning as emoji use tends to cluster into the frequently used and the rarely used emojis.Much of the machine learning research on emoji use has focused on high resource languages and has conceptualised the task of predicting emojis around traditional server-side machine learning approaches.However, traditional machine learning approaches for private communication can introduce privacy concerns, as these approaches require all data to be transmitted to a central storage.In this paper, we seek to address the dual concerns of emphasising high resource languages for emoji prediction and risking the privacy of people’s data.We introduce a new dataset of 118k tweets (augmented from 25k unique tweets) for emoji prediction in Hindi, and propose a modification to the federated learning algorithm, CausalFedGSD, which aims to strike a balance between model performance and user privacy. We show that our approach obtains comparative scores with more complex centralised models while reducing the amount of data required to optimise the models and minimising risks to user privacy."
ACL2021,emotional,Supporting Cognitive and Emotional Empathic Writing of Students,11,"Topic Modeling,Text Readability and Simplification,Intelligent Tutoring Systems and Adaptive Learning","Computer science,Computational linguistics,Joint (building),Cognition,Cognitive science,Natural language processing,Volume (thermodynamics),Linguistics,Artificial intelligence,Psychology,Engineering,Philosophy,Neuroscience,Architectural engineering,Physics,Quantum mechanics",,https://aclanthology.org/2021.acl-long.314.pdf,"Thiemo Wambsganss,Christina Niklaus,Matthias Söllner,Siegfried Handschuh,Jan Marco Leimeister","We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German. We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. The obtained inter-rater agreement of α=0.79 for the components and the multi-π=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement. Moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location. We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use. Finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems."
ACL2022,emotional,M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database,30,Sentiment Analysis and Opinion Mining,"Zhàng,Modal,Computer science,Natural language processing,Volume (thermodynamics),Artificial intelligence,Linguistics,Speech recognition,Information retrieval,History,Philosophy,Archaeology,Chemistry,Physics,Quantum mechanics,Polymer chemistry,China",,https://aclanthology.org/2022.acl-long.391.pdf,"Jinming Zhao,Tenggan Zhang,Jingwen Hu,Yuchen Liu,Qin Jin,Xinchao Wang,Haizhou Li","The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M"
IJCAI2024,emotion,ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause Reasoners through Reasoning Chains,0,"Topic Modeling,Intelligent Tutoring Systems and Adaptive Learning,Sentiment Analysis and Opinion Mining","Generative grammar,Chain (unit),Computer science,Generative model,Natural language processing,Cognitive science,Artificial intelligence,Psychology,Physics,Astronomy",https://github.com/hzp3517/ECR-Chain.,https://www.ijcai.org/proceedings/2024/0695.pdf,"Zhaopei Huang, Jinming Zhao, Qin Jin","Understanding the process of emotion generation is crucial for analyzing the causes behind emotions. Causal Emotion Entailment (CEE), an emotion-understanding task, aims to identify the causal utterances in a conversation that stimulate the emotions expressed in a target utterance. However, current works in CEE mainly focus on modeling semantic and emotional interactions in conversations, neglecting the exploration of the emotion-generation process. This hinders the models from deeply understanding emotions, restricting their ability to produce explainable predictions. In this work, inspired by the emotion generation process of ""stimulus-appraisal-emotion"" in the cognitive appraisal theory, we introduce a step-by-step reasoning method, Emotion-Cause Reasoning Chain (ECR-Chain), to infer the stimulus from the target emotional expressions in conversations. Specifically, we first introduce the ECR-Chain to ChatGPT via few-shot prompting, which significantly improves its performance on the CEE task. We further propose an automated construction process to utilize ChatGPT in building an ECR-Chain set, which can enhance the reasoning abilities of smaller models through supervised training and assist the Vicuna-7B model in achieving state-of-the-art CEE performance. Moreover, our methods can enable these generative language models to effectively perform emotion-cause reasoning in an explainable manner. Our code, data and more details are at https://github.com/hzp3517/ECR-Chain."
IJCAI2022,emotion,Online ECG Emotion Recognition for Unknown Subjects via Hypergraph-Based Transfer Learning,5,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Computer science,Hypergraph,Artificial intelligence,Transfer of learning,Machine learning,Training set,Pattern recognition (psychology),Adaptation (eye),Data mining,Speech recognition,Mathematics,Discrete mathematics,Physics,Optics",,https://www.ijcai.org/proceedings/2022/0509.pdf,"Yalan Ye, Tongjie Pan, Qianhe Meng, Jingjing Li, Li Lu","Electrocardiogram (ECG) signal based cross-subject emotion recognition methods reduce the influence of individual differences using domain adaptation (DA) techniques. These methods generally assume that the entire unlabeled data of unknown target subjects are available in training phase.  However, this assumption does not hold in some practical scenarios where the data of target subjects arrive one by one in an online manner instead of being acquired at a time. Thus, existing DA methods cannot be directly applied in this case since the unknown target data is inaccessible in training phase. To tackle the problem, we propose a novel online cross-subject ECG emotion recognition method leveraging hypergraph-based online transfer learning (HOTL). Specifically, the proposed hypergraph structure is capable of learning the high-order correlation among data, such that the recognition model trained on source subjects can be more effectively generalized to target subjects. Meanwhile, the structure can be easily updated by adding a hyperedge which connects a newly coming sample with the current hypergraph, resulting in further reduce the individual differences in online manner without re-training the model. Consequently, HOTL can effectively deal with the online cross-subject scenario where unknown target ECG data arrive one by one and varying overtime. Extensive experiments conducted on the Amigos dataset validate the superiority of the proposed method."
ECCV2024,emotional,Training A Small Emotional Vision Language Model for Visual Art Comprehension,0,"Visual Attention and Saliency Detection,Aesthetic Perception and Analysis,Multimodal Machine Learning Applications","Computer science,Artificial intelligence,Comprehension,Natural language processing,Cognitive science,Programming language,Psychology","https://github.com/BetterZH/SEVLM-""",https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08451.pdf,"Jing Zhang, Liang Zheng*, Meng Wang, Dan Guo*","""This paper develops small vision language models to understand visual art, which, given an art work, aims to identify its emotion category and explain this prediction with natural language. While small models are computationally efficient, their capacity is much limited compared with large models. To break this trade-off, this paper builds a small emotional vision language model (SEVLM) by emotion modeling and input-output feature alignment. On the one hand, based on valence-arousal-dominance (VAD) knowledge annotated by psychology experts, we introduce and fuse emotional features derived through VAD dictionary and a VAD head to align VAD vectors of predicted emotion explanation and the ground truth. This allows the vision language model to better understand and generate emotional texts, compared with using traditional text embeddings alone. On the other hand, we design a contrastive head to pull close embeddings of the image, its emotion class, and explanation, which aligns model outputs and inputs. On two public affective explanation datasets, we show that the proposed techniques consistently improve the visual art understanding performance of baseline SEVLMs. Importantly, the proposed model can be trained and evaluated on a single RTX 2080 Ti while exhibiting very strong performance: it not only outperforms the state-of-the-art small models but is also competitive compared with LLaVA 7B after fine-tuning and GPT4(V). The code is available at https://github.com/BetterZH/SEVLM-"""
ECCV2024,"emotalk3d,emotional",EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head,1,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Human Pose and Action Recognition","Computer science,Head (geology),Fidelity,Human–computer interaction,Computer graphics (images),Computer vision,Telecommunications,Geology,Geomorphology",https://nju-3dv.github.io/,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07387.pdf,"Qianyun He, Xinya Ji, Yicheng Gong, Yuanxun Lu, Zhengyu Diao, Linjia Huang, Yao Yao, Siyu Zhu, Zhan Ma, Songcen Xu, Xiaofei Wu, Zixiao Zhang, Xun Cao, Hao Zhu*","""We present a novel approach for synthesizing 3D talking heads with controllable emotion, featuring enhanced lip synchronization and rendering quality. Despite significant progress in the field, prior methods still suffer from multi-view consistency and a lack of emotional expressiveness. To address these issues, we collect dataset with calibrated multi-view videos, emotional annotations, and per-frame 3D geometry. By training on the dataset, we propose a ‘Speech-to-Geometry-to-Appearance’ mapping framework that first predicts faithful 3D geometry sequence from the audio features, then the appearance of a 3D talking head represented by 4D Gaussians is synthesized from the predicted geometry. The appearance is further disentangled into canonical and dynamic Gaussians, learned from multi-view videos, and fused to render free-view talking head animation. Moreover, our model enables controllable emotion in the generated talking heads and can be rendered in wide-range views. Our method exhibits improved rendering quality and stability in lip motion generation while capturing dynamic facial details such as wrinkles and subtle expressions. Experiments demonstrate the effectiveness of our approach in generating high-fidelity and emotion-controllable 3D talking heads. The code and dataset are released in https://nju-3dv.github.io/ projects/EmoTalk3D."""
ICASSP2022,emotion,Memobert: Pre-Training Model with Prompt-Based Learning for Multimodal Emotion Recognition.,18,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Multimodal Machine Learning Applications","Computer science,Emotion recognition,Benchmark (surveying),Task (project management),Artificial intelligence,Ambiguity,Multimodal learning,Annotation,Speech recognition,Machine learning,Management,Geodesy,Economics,Programming language,Geography",,,"Chihui Zhuang,Yanjie Liang,Yan Yan,Yang Lu,Hanzi Wang",
ICASSP2022,emotion,An Audio-Saliency Masking Transformer for Audio Emotion Classification in Movies.,1,"Music and Audio Processing,Speech and Audio Processing,Video Analysis and Summarization","Computer science,Speech recognition,Valence (chemistry),Perception,Transformer,Arousal,Classifier (UML),Masking (illustration),Artificial intelligence,Psychology,Engineering,Physics,Quantum mechanics,Voltage,Neuroscience,Electrical engineering,Art,Visual arts",,,"Xiaoqing Liu,Huanqiang Zeng,Yifan Shi,Jianqing Zhu,Kai-Kuang Ma",
ICASSP2021,emotional,Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling.,8,"Speech Recognition and Synthesis,Topic Modeling,Natural Language Processing Techniques","Naturalness,Prosody,Speech recognition,Computer science,Similarity (geometry),Sentence,Embedding,Natural language processing,Tone (literature),Speech synthesis,Artificial intelligence,Linguistics,Philosophy,Physics,Quantum mechanics,Image (mathematics)",,,"Hassan Taherian,DeLiang Wang",
ICASSP2021,emotion,Emotion Controllable Speech Synthesis Using Emotion-Unlabeled Dataset with the Assistance of Cross-Domain Speech Emotion Recognition.,54,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Speech recognition,Speech synthesis,Emotion recognition,Task (project management),Domain (mathematical analysis),Quality (philosophy),Artificial neural network,Emotion classification,Artificial intelligence,Natural language processing,Mathematics,Engineering,Mathematical analysis,Philosophy,Systems engineering,Epistemology",,,"Sanyuan Chen,Yu Wu,Zhuo Chen,Jian Wu,Jinyu Li,Takuya Yoshioka,Chengyi Wang,Shujie Liu,Ming Zhou",
ICASSP2021,emotion,Meta-Learning for Low-Resource Speech Emotion Recognition.,17,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Transfer of learning,Task (project management),Adaptation (eye),Artificial intelligence,Natural language processing,Resource (disambiguation),Multi-task learning,Emotion recognition,Speech recognition,Process (computing),Machine learning,Psychology,Computer network,Management,Neuroscience,Economics,Operating system",,,"Atsushi Ando,Ryo Masumura,Hiroshi Sato,Takafumi Moriya,Takanori Ashihara,Yusuke Ijima,Tomoki Toda",
ICASSP2021,emotion,Progressive Co-Teaching for Ambiguous Speech Emotion Recognition.,7,"Emotion and Mood Recognition,Speech and Audio Processing,Hand Gesture Recognition Systems","Computer science,Emotion recognition,Ambiguity,Task (project management),Speech recognition,Artificial intelligence,Training set,Machine learning,Natural language processing,Management,Economics,Programming language",,,"Panagiotis Tzirakis,Anh Nguyen,Stefanos Zafeiriou,Björn W. Schuller",
ICASSP2021,emotion,Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations.,49,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Speech recognition,Asynchronous communication,Utterance,Robustness (evolution),Embedding,Sentence,Artificial intelligence,Natural language processing,Pattern recognition (psychology),Computer network,Biochemistry,Chemistry,Gene",,,"Amir Shirian,Tanaya Guha",
ICASSP2021,emotion,Speech Emotion Recognition Based on Listener Adaptive Models.,9,"Speech and Audio Processing,Emotion and Mood Recognition,Speech Recognition and Synthesis","Speech recognition,Perception,Emotion recognition,Computer science,Emotion perception,Adaptation (eye),Emotion detection,Artificial intelligence,Psychology,Facial expression,Neuroscience",,,"Xianfeng Wang,Min Wang,Wenbo Qi,Wanqi Su,Xiangqian Wang,Huan Zhou",
ICASSP2021,emotion,Speech Emotion Recognition Using Semantic Information.,20,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Paralanguage,Computer science,Artificial intelligence,Feature (linguistics),Natural language processing,Extractor,Speech recognition,Linguistics,Philosophy,Process engineering,Engineering",,,"Srividya Tirunellai Rajamani,Kumar T. Rajamani,Adria Mallol-Ragolta,Shuo Liu,Björn W. Schuller",
ICASSP2022,emotional,Automatic Depression Detection: an Emotional Audio-Textual Corpus and A Gru/Bilstm-Based Model.,67,"Emotion and Mood Recognition,Mental Health via Writing,Digital Mental Health Interventions","Depression (economics),Generalization,Computer science,Natural language processing,Code (set theory),Artificial intelligence,Speech recognition,Mathematics,Mathematical analysis,Set (abstract data type),Economics,Macroeconomics,Programming language",,,"Vijay Ravi,Jinhan Wang,Jonathan Flint,Abeer Alwan",
ICASSP2021,emotion,Compact Graph Architecture for Speech Emotion Recognition.,40,"Emotion and Mood Recognition,Advanced Graph Neural Networks,Sentiment Analysis and Opinion Mining","Computer science,Graph,Scalability,Convolution (computer science),Theoretical computer science,Speech recognition,Artificial intelligence,Pattern recognition (psychology),Artificial neural network,Database",,,"Changzeng Fu,Chaoran Liu,Carlos Toshinori Ishi,Hiroshi Ishiguro",
ICASSP2021,emotion,A Novel end-to-end Speech Emotion Recognition Network with Stacked Transformer Layers.,32,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Transformer,Utterance,Feature extraction,Pipeline (software),End-to-end principle,Speech recognition,Feature (linguistics),Architecture,Artificial intelligence,Margin (machine learning),Pattern recognition (psychology),Machine learning,Engineering,Voltage,Art,Linguistics,Philosophy,Electrical engineering,Visual arts,Programming language",,,"Lili Guo,Longbiao Wang,Chenglin Xu,Jianwu Dang,Eng Siong Chng,Haizhou Li",
ICASSP2021,emotion,A Novel Attention-Based Gated Recurrent Unit and its Efficacy in Speech Emotion Recognition.,43,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Emotion recognition,Context (archaeology),Speech recognition,Recurrent neural network,Artificial intelligence,Field (mathematics),Baseline (sea),Deep learning,Feature extraction,Pattern recognition (psychology),Artificial neural network,Mathematics,Paleontology,Oceanography,Pure mathematics,Biology,Geology",,,"Aneesh Muppidi,Martin Radfar",
ICASSP2022,emotion,Climate and Weather: Inspecting Depression Detection via Emotion Recognition.,11,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Emotion detection,Computer science,Emotion recognition,Artificial intelligence",,,"Iuliia Nigmatulina,Juan Zuluaga-Gomez,Amrutha Prasad,Seyyed Saeed Sarfjoo,Petr Motlícek",
ICASSP2021,emotion,MAEC: Multi-Instance Learning with an Adversarial Auto-Encoder-Based Classifier for Speech Emotion Recognition.,6,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Adversarial system,Classifier (UML),Artificial intelligence,Salient,Regularization (linguistics),Cross-validation,Encoder,Speech recognition,Natural language processing,Autoencoder,Training set,Pattern recognition (psychology),Machine learning,Deep learning,Operating system",,,"Yuan Gao,Jiaxing Liu,Longbiao Wang,Jianwu Dang",
ICASSP2021,emotion,Representation Learning with Spectro-Temporal-Channel Attention for Speech Emotion Recognition.,25,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Convolutional neural network,Computer science,Representation (politics),Channel (broadcasting),Speech recognition,Artificial intelligence,Feature learning,Pattern recognition (psychology),Emotion recognition,Deep learning,Computer network,Politics,Political science,Law",,,"Mingke Xu,Fan Zhang,Xiaodong Cui,Wei Zhang",
ICASSP2021,emotion,Speech Emotion Recognition Using Quaternion Convolutional Neural Networks.,44,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Spectrogram,Convolutional neural network,Quaternion,Artificial intelligence,Motion capture,Emotion recognition,ENCODE,Pattern recognition (psychology),Motion (physics),Mathematics,Biochemistry,Chemistry,Gene,Geometry",,,"Raghavendra Pappagari,Jesús Villalba,Piotr Zelasko,Laureano Moro-Velázquez,Najim Dehak",
ICASSP2021,emotion,Domain-Adversarial Autoencoder with Attention Based Feature Level Fusion for Speech Emotion Recognition.,15,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Discriminative model,Autoencoder,Computer science,Artificial intelligence,Adversarial system,Feature (linguistics),Encoder,Speech recognition,Domain (mathematical analysis),Machine learning,Linear subspace,Feature learning,Pattern recognition (psychology),Deep learning,Mathematical analysis,Philosophy,Linguistics,Mathematics,Operating system,Geometry",,,"Mao Li,Bo Yang,Joshua Levy,Andreas Stolcke,Viktor Rozgic,Spyros Matsoukas,Constantinos Papayiannis,Daniel Bone,Chao Wang",
ICASSP2021,emotion,Speech Emotion Recognition with Multiscale Area Attention and Data Augmentation.,55,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","Computer science,Spectrogram,Emotion recognition,Granularity,Classifier (UML),Convolutional neural network,Artificial intelligence,Vocal tract,Emotion classification,Pattern recognition (psychology),Speech recognition,Artificial neural network,Deep neural networks,Machine learning,Operating system",,,"Qi Cao,Mixiao Hou,Bingzhi Chen,Zheng Zhang,Guangming Lu",
ICASSP2021,emotion,CopyPaste: An Augmentation Method for Speech Emotion Recognition.,30,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Concatenation (mathematics),Utterance,Emotion recognition,Artificial intelligence,Test data,Noise (video),Training set,Mathematics,Combinatorics,Image (mathematics),Programming language",,,"Jiaxing Liu,Sen Chen,Longbiao Wang,Zhilei Liu,Yahui Fu,Lili Guo,Jianwu Dang",
ICASSP2021,emotion,Contrastive Unsupervised Learning for Speech Emotion Recognition.,42,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Emotion recognition,Speech recognition,Artificial intelligence,Salient,Valence (chemistry),Correlation,Feature learning,Natural language processing,Machine learning,Pattern recognition (psychology),Mathematics,Physics,Geometry,Quantum mechanics",,,"Raghuveer Peri,Srinivas Parthasarathy,Charles Bradshaw,Shiva Sundaram",
ICASSP2021,emotion,Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition.,27,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Discriminative model,Feature (linguistics),Benchmark (surveying),Artificial intelligence,ENCODE,Utterance,Feature extraction,Frame (networking),Encoding (memory),Pattern recognition (psychology),Emotion recognition,Speech recognition,Telecommunications,Philosophy,Linguistics,Biochemistry,Chemistry,Geodesy,Gene,Geography",,,"Rohan Kumar Das,Jichen Yang,Haizhou Li",
ICASSP2021,emotion,Multimodal Emotion Recognition with Capsule Graph Convolutional Based Representation Fusion.,24,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Computer science,Convolutional neural network,Representation (politics),Artificial intelligence,Graph,Emotion recognition,Fusion,Pattern recognition (psychology),Natural language processing,Speech recognition,Computer vision,Theoretical computer science,Linguistics,Philosophy,Politics,Political science,Law",,,"Xu Li,Na Li,Chao Weng,Xunying Liu,Dan Su,Dong Yu,Helen Meng",
ICASSP2021,emotion,Disentanglement for Audio-Visual Emotion Recognition Using Multitask Setup.,10,"Music and Audio Processing,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Multi-task learning,Task (project management),Encoding (memory),Speech recognition,Emotion recognition,Artificial intelligence,Multimodal learning,Machine learning,Pattern recognition (psychology),Management,Economics",,,"Anwei Luo,Enlei Li,Yongliang Liu,Xiangui Kang,Z. Jane Wang",
ICASSP2022,"emoq,emotion,emotional",EMOQ-TTS: Emotion Intensity Quantization for Fine-Grained Controllable Emotional Text-to-Speech.,21,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Utterance,Computer science,Speech recognition,Controllability,Quantization (signal processing),Speech synthesis,Emotional expression,Artificial intelligence,Psychology,Cognitive psychology,Mathematics,Computer vision,Applied mathematics",,,"Shiyao Cui,Xin Cong,Bowen Yu,Tingwen Liu,Yucheng Wang,Jinqiao Shi",
ICASSP2022,emotion,Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition.,7,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Adversarial system,Artificial intelligence,Feature extraction,Classifier (UML),Emotion recognition,Speech recognition,Benchmark (surveying),Focus (optics),Domain (mathematical analysis),Feature engineering,Feature (linguistics),Divergence (linguistics),Test data,Pattern recognition (psychology),Machine learning,Deep learning,Programming language,Mathematical analysis,Linguistics,Philosophy,Physics,Mathematics,Geodesy,Optics,Geography",,,"Seong-Gyun Leem,Daniel Fulford,Jukka-Pekka Onnela,David Gard,Carlos Busso",
ICASSP2022,emotion,Multi-Stage Graph Representation Learning for Dialogue-Level Speech Emotion Recognition.,3,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Speech and dialogue systems","Computer science,Utterance,Classifier (UML),Graph,Convolutional neural network,Speech recognition,Artificial intelligence,Emotion recognition,Natural language processing,Feature learning,Theoretical computer science",,,"Sneha Das,Nicole Nadine Lønfeldt,Anne Katrine Pagsberg,Line H. Clemmensen",
ICASSP2022,emotion,Speech Emotion Recognition with Global-Aware Fusion on Multi-Scale Feature Representation.,37,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Convolutional neural network,Feature (linguistics),Artificial intelligence,Representation (politics),Benchmark (surveying),Pattern recognition (psychology),Feature learning,Feature extraction,Scale (ratio),Emotion recognition,Focus (optics),Speech recognition,Natural language processing,Philosophy,Linguistics,Physics,Geodesy,Optics,Quantum mechanics,Politics,Political science,Law,Geography",,,"Zhengyan Sheng,Zhiqiang Guo,Xin Li,Yunxia Li,Zhenhua Ling",
ACL2024,emotion,"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",1,"Gender, Feminism, and Media,Resilience and Mental Health,Populism, Right-Wing Movements","Attribution,Psychology,Social psychology,Computer science",,https://aclanthology.org/2024.acl-long.415.pdf,"Flor Miriam Plaza-del-Arco,Amanda Cercas Curry,Alba Curry,Gavin Abercrombie,Dirk Hovy","Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men’s anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like ‘When I had a serious argument with a dear person’. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."
ACL2021,emotional,Towards Emotional Support Dialog Systems,117,"Topic Modeling,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Dialog box,Computational linguistics,Computer science,Natural language processing,Joint (building),Artificial intelligence,Linguistics,Library science,World Wide Web,Engineering,Philosophy,Architectural engineering",,https://aclanthology.org/2021.acl-long.269.pdf,"Siyang Liu,Chujie Zheng,Orianna Demasi,Sahand Sabour,Yu Li,Zhou Yu,Yong Jiang,Minlie Huang","Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems."
EMNLP2022,emotion,Textless Speech Emotion Conversion using Discrete &amp; Decomposed Representations,11,"Speech Recognition and Synthesis,Computational and Text Analysis Methods","JADE (particle detector),Speech recognition,Natural language processing,Computer science,Artificial intelligence,Free speech,Speech act,Linguistics,Philosophy,Law,Political science,Physics,Particle physics",https://speechbot.github.io/emotion,https://aclanthology.org/2022.emnlp-main.769.pdf,"Felix Kreuk,Adam Polyak,Jade Copet,Eugene Kharitonov,Tu Anh Nguyen,Morgan Rivière,Wei-Ning Hsu,Abdelrahman Mohamed,Emmanuel Dupoux,Yossi Adi","Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We use a decomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic features, speaker, and emotion. First, we modify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic features based on these units. Finally, the speech waveform is generated by feeding the predicted representations into a neural vocoder. Such a paradigm allows us to go beyond spectral and parametric changes of the signal, and model non-verbal vocalizations, such as laughter insertion, yawning removal, etc. We demonstrate objectively and subjectively that the proposed method is vastly superior to current approaches and even beats text-based systems in terms of perceived emotion and audio quality. We rigorously evaluate all components of such a complex system and conclude with an extensive model analysis and ablation study to better emphasize the architectural choices, strengths and weaknesses of the proposed method. Samples are available under the following link: https://speechbot.github.io/emotion"
EMNLP2024,"emotion,emoknob",EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control,0,Speech Recognition and Synthesis,"Cloning (programming),Computer science,Control (management),Speech recognition,Artificial intelligence,Programming language",,https://aclanthology.org/2024.emnlp-main.466.pdf,"Haozhe Chen,Run Chen,Julia Hirschberg","While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services."
ACL2022,emotions,SRL4E – Semantic Role Labeling for Emotions: A Unified Evaluation Framework,7,"Sentiment Analysis and Opinion Mining,Topic Modeling,Humor Studies and Applications","Computer science,Benchmark (surveying),Task (project management),Field (mathematics),Sentence,Domain (mathematical analysis),Sentiment analysis,Semantic role labeling,Natural language processing,Artificial intelligence,Semantics (computer science),Scheme (mathematics),Information retrieval,Data science,Mathematics,Management,Geodesy,Pure mathematics,Economics,Programming language,Geography,Mathematical analysis",,https://aclanthology.org/2022.acl-long.314.pdf,"Cesare Campagnano,Simone Conia,Roberto Navigli","In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area."
ACL2024,"emotional,emobench",EmoBench: Evaluating the Emotional Intelligence of Large Language Models,0,"Topic Modeling,Machine Learning in Healthcare","Emotional intelligence,Psychology,Computer science,Cognitive psychology,Natural language processing,Linguistics,Social psychology,Philosophy",https://github.com/Sahandfer/EmoBench.,https://aclanthology.org/2024.acl-long.326.pdf,"Sahand Sabour,Siyang Liu,Zheyuan Zhang,June Liu,Jinfeng Zhou,Alvionna Sunaryo,Tatia Lee,Rada Mihalcea,Minlie Huang","Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench."
ICASSP2022,emotion,Representation Learning Through Cross-Modal Conditional Teacher-Student Training For Speech Emotion Recognition.,24,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Valence (chemistry),Speech recognition,Natural language processing,Artificial intelligence,Discriminative model,Correlation,Emotion recognition,Representation (politics),Concordance correlation coefficient,Generalizability theory,Mathematics,Statistics,Physics,Geometry,Quantum mechanics,Politics,Political science,Law",,,"Youxiang Zhu,Bang Tran,Xiaohui Liang,John A. Batsis,Robert M. Roth",
ICASSP2022,emotion,Not All Features are Equal: Selection of Robust Features for Speech Emotion Recognition in Noisy Environments.,10,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","Robustness (evolution),Speech recognition,Computer science,Feature selection,Feature extraction,Artificial intelligence,Pattern recognition (psychology),Noise (video),Biochemistry,Chemistry,Image (mathematics),Gene",,,"Mercedes Vetráb,José Vicente Egas López,Réka Balogh,Nóra Imre,Ildikó Hoffmann,László Tóth,Magdolna Pákáski,János Kálmán,Gábor Gosztolya",
ICASSP2022,emotion,Towards Transferable Speech Emotion Representation: On Loss Functions for Cross-Lingual Latent Representations.,4,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Autoencoder,Computer science,Embedding,Artificial intelligence,Normalization (sociology),Transfer of learning,Pattern recognition (psychology),Natural language processing,Machine learning,Deep learning,Speech recognition,Sociology,Anthropology",,,"Ayimnisagul Ablimit,Catarina Botelho,Alberto Abad,Tanja Schultz,Isabel Trancoso",
ICASSP2022,emotion,Key-Sparse Transformer for Multimodal Speech Emotion Recognition.,37,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Transformer,Speech recognition,Emotion recognition,Key (lock),Artificial intelligence,Human–computer interaction,Engineering,Computer security,Voltage,Electrical engineering",,,"Soumya Dutta,Sriram Ganapathy",
ICASSP2022,emotion,Neural Architecture Search for Speech Emotion Recognition.,15,"Speech Recognition and Synthesis,Music and Audio Processing,Emotion and Mood Recognition","Dropout (neural networks),Computer science,Architecture,Artificial neural network,Artificial intelligence,Deep neural networks,Machine learning,Speech recognition,Art,Visual arts",,,"Edmilson da Silva Morais,Ron Hoory,Weizhong Zhu,Itai Gat,Matheus Damasceno,Hagai Aronowitz",
ICASSP2022,emotion,Multi-Lingual Multi-Task Speech Emotion Recognition Using wav2vec 2.0.,163,"Music and Audio Processing,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Valence (chemistry),Concordance correlation coefficient,Transformer,Speech recognition,Robustness (evolution),Artificial intelligence,Natural language processing,Mathematics,Engineering,Biochemistry,Statistics,Chemistry,Quantum mechanics,Voltage,Electrical engineering,Gene,Physics",,,"Gábor Gosztolya,László Tóth,Veronika Svindt,Judit Bóna,Ildikó Hoffmann",
ICASSP2022,emotion,LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition.,64,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Convolutional neural network,Feature (linguistics),Convolution (computer science),Emotion recognition,Speech recognition,Feature extraction,Acoustic model,Filter (signal processing),Code (set theory),SIGNAL (programming language),Artificial intelligence,Deep learning,Source code,Artificial neural network,Pattern recognition (psychology),Speech processing,Computer vision,Philosophy,Linguistics,Set (abstract data type),Programming language,Operating system",,,"R'mani Haulcy,Katerina Placek,Brian Tracey,Adam P. Vogel,James R. Glass",
ACL2023,"emotion,emotional",Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach,9,"Mental Health via Writing,Stuttering Research and Treatment,Speech and dialogue systems","Conversation,Supporter,Coherence (philosophical gambling strategy),Reinforcement learning,Computer science,Turn-taking,Process (computing),Cognitive psychology,Psychology,Human–computer interaction,Artificial intelligence,Communication,Physics,Archaeology,Quantum mechanics,History,Operating system",,https://aclanthology.org/2023.acl-long.96.pdf,"Jinfeng Zhou,Zhuang Chen,Bo Wang,Minlie Huang","Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., "
ACL2021,emotion,Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction,43,"Multimodal Machine Learning Applications,Topic Modeling,Sentiment Analysis and Opinion Mining","Computer science,Position paper,Association (psychology),Natural language processing,Joint (building),Position (finance),Computational linguistics,Graph,Artificial intelligence,Cognitive science,World Wide Web,Psychology,Engineering,Theoretical computer science,Architectural engineering,Psychotherapist,Finance,Economics",,https://aclanthology.org/2021.acl-long.261.pdf,"Hanqi Yan,Lin Gui,Gabriele Pergola,Yulan He","The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models."
ICASSP2022,emotion,Multimodal Transformer with Learnable Frontend and Self Attention for Emotion Recognition.,15,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Transformer,Encoder,Utterance,Emotion recognition,Modal,Artificial intelligence,Feature extraction,Natural language processing,Classifier (UML),Engineering,Voltage,Chemistry,Polymer chemistry,Electrical engineering,Operating system",,,"Bang Tran,Youxiang Zhu,Xiaohui Liang,James W. Schwoebel,Lindsay A. Warrenburg",
ICASSP2022,emotion,Speech Emotion Recognition Using Self-Supervised Features.,75,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Music and Audio Processing","Computer science,Artificial intelligence,Feature (linguistics),Speech recognition,Field (mathematics),Utterance,Categorical variable,Modular design,Feature extraction,Natural language processing,Pattern recognition (psychology),Machine learning,Philosophy,Linguistics,Mathematics,Pure mathematics,Operating system",,,"Doyeon Kim,Hyewon Han,Hyeon-Kyeong Shin,Soo-Whan Chung,Hong-Goo Kang",
ICASSP2022,emotion,A Commonsense Knowledge Enhanced Network with Retrospective Loss for Emotion Recognition in Spoken Dialog.,4,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Social Robot Interaction and HRI","Dialog box,Computer science,Commonsense knowledge,Natural language processing,Commonsense reasoning,Dialog system,Artificial intelligence,Speech recognition,World Wide Web,Knowledge extraction",,,"Kai Wei,Dillon Knox,Martin Radfar,Thanh Tran,Markus Müller,Grant P. Strimel,Nathan Susanj,Athanasios Mouchtaris,Maurizio Omologo",
ICASSP2022,emotion,Hierarchical and Multi-View Dependency Modelling Network for Conversational Emotion Recognition.,6,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Topic Modeling","Utterance,Dependency (UML),Computer science,Context (archaeology),Security token,Representation (politics),Benchmark (surveying),Natural language processing,Artificial intelligence,Speech recognition,Task (project management),Engineering,Paleontology,Computer security,Geodesy,Politics,Political science,Law,Biology,Geography,Systems engineering",,,"Jing Yang Lee,Kong Aik Lee,Woon-Seng Gan",
ICASSP2022,emotion,MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations.,85,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Modalities,Artificial intelligence,Conversation,Graph,Redundancy (engineering),Fuse (electrical),Context (archaeology),Human–computer interaction,Machine learning,Theoretical computer science,Engineering,Paleontology,Social science,Linguistics,Philosophy,Sociology,Electrical engineering,Biology,Operating system",,,"Muhammad Saad Saeed,Muhammad Haris Khan,Shah Nawaz,Muhammad Haroon Yousaf,Alessio Del Bue",
ICASSP2022,emotion,"Modeling Intention, Emotion and External World in Dialogue Systems.",8,"Sentiment Analysis and Opinion Mining,Topic Modeling,Mental Health via Writing","Relation (database),Action (physics),Computer science,Task (project management),Style (visual arts),Process (computing),Cognitive psychology,Human–computer interaction,Psychology,Data mining,Engineering,History,Physics,Systems engineering,Archaeology,Quantum mechanics,Operating system",,,"K. C. Kishan,Zhenning Tan,Long Chen,Minho Jin,Eunjung Han,Andreas Stolcke,Chul Lee",
ICASSP2021,affect,How Phonotactics Affect Multilingual and Zero-Shot ASR Performance.,12,"Speech Recognition and Synthesis,Music and Audio Processing,Topic Modeling","Phonotactics,Computer science,Encoder,Natural language processing,Language model,Training set,Artificial intelligence,Speech recognition,Transfer of learning,Linguistics,Phonology,Philosophy,Operating system",,,"Qiang Gao,Haiwei Wu,Yanqing Sun,Yitao Duan",
ICASSP2021,emotions,Deepemocluster: a Semi-Supervised Framework for Latent Cluster Representation of Speech Emotions.,13,"Emotion and Mood Recognition,Music and Audio Processing,Sentiment Analysis and Opinion Mining","Autoencoder,Computer science,Generalization,Artificial intelligence,Spectrogram,Representation (politics),Machine learning,Set (abstract data type),Latent variable,Encoder,Feature learning,Unsupervised learning,Semi-supervised learning,Pattern recognition (psychology),Artificial neural network,Mathematics,Mathematical analysis,Politics,Political science,Law,Programming language,Operating system",,,"Sri Harsha Dumpala,Sheri Rempel,Katerina Dikaios,Mehri Sajjadian,Rudolf Uher,Sageev Oore",
ICASSP2021,emotion,The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case.,13,"Music and Audio Processing,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Transfer of learning,Task (project management),Similarity (geometry),Speech recognition,Deep learning,Artificial intelligence,Artificial neural network,Adaptation (eye),Deep neural networks,Natural language processing,Knowledge transfer,Image (mathematics),Psychology,Knowledge management,Management,Neuroscience,Economics",,,"Brian Stasak,Zhaocheng Huang,Dale Joachim,Julien Epps",
ICASSP2021,emotion,Pause-Encoded Language Models for Recognition of Alzheimer's Disease and Emotion.,143,"Voice and Speech Disorders,Dysphagia Assessment and Management,Emotion and Mood Recognition","Paralanguage,Computer science,Feature (linguistics),Feature extraction,Pattern recognition (psychology),Artificial intelligence,Speech recognition,Set (abstract data type),Communication,Linguistics,Philosophy,Sociology,Programming language",,,"Ina Kodrasi,Michaela Pernon,Marina Laganaro,Hervé Bourlard",
ICASSP2022,emotion,Speaker Normalization for Self-Supervised Speech Emotion Recognition.,43,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Normalization (sociology),Speech recognition,Speaker recognition,Emotion recognition,Exploit,Artificial intelligence,Feature learning,Feature (linguistics),Task (project management),Feature extraction,Pattern recognition (psychology),Linguistics,Philosophy,Computer security,Management,Sociology,Anthropology,Economics",,,"Yuanchao Li,Peter Bell,Catherine Lai",
ICASSP2022,emotion,Sentiment-Aware Automatic Speech Recognition Pre-Training for Enhanced Speech Emotion Recognition.,15,"Emotion and Mood Recognition,Music and Audio Processing,Sentiment Analysis and Opinion Mining","Computer science,Speech recognition,Emotion recognition,Concordance correlation coefficient,Valence (chemistry),Artificial intelligence,Acoustic model,Sentiment analysis,Task (project management),Training set,Natural language processing,Speech processing,Pattern recognition (psychology),Statistics,Physics,Mathematics,Management,Quantum mechanics,Economics",,,"Heqing Zou,Yuke Si,Chen Chen,Deepu Rajan,Eng Siong Chng",
ICASSP2022,emotion,Confidence Estimation for Speech Emotion Recognition Based on the Relationship Between Emotion Categories and Primitives.,2,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Computer science,Annotation,Artificial intelligence,Centroid,Speech recognition,Emotion recognition,Pattern recognition (psychology),Metric (unit),Machine learning,Economics,Operations management",,,"Zhengjun Yue,Erfan Loweimi,Zoran Cvetkovic,Heidi Christensen,Jon Barker",
EMNLP2022,emotions,Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts,7,"Sentiment Analysis and Opinion Mining,Misinformation and Its Impacts,Topic Modeling","Automatic summarization,Social media,Affect (linguistics),Emotion detection,Sentiment analysis,Computer science,Coronavirus disease 2019 (COVID-19),Affective computing,Psychology,Emotion classification,Cognitive psychology,Emotion recognition,Natural language processing,Artificial intelligence,World Wide Web,Communication,Medicine,Disease,Pathology,Infectious disease (medical specialty)",,https://aclanthology.org/2022.emnlp-main.642.pdf,"Hongli Zhan,Tiberiu Sosea,Cornelia Caragea,Junyi Jessy Li","Crises such as the COVID-19 pandemic continuously threaten our world and emotionally affect billions of people worldwide in distinct ways. Understanding the triggers leading to people’s emotions is of crucial importance. Social media posts can be a good source of such analysis, yet these texts tend to be charged with multiple emotions, with triggers scattering across multiple sentences. This paper takes a novel angle, namely, emotion detection and trigger summarization, aiming to both detect perceived emotions in text, and summarize events and their appraisals that trigger each emotion. To support this goal, we introduce CovidET (Emotions and their Triggers during Covid-19), a dataset of ~1,900 English Reddit posts related to COVID-19, which contains manual annotations of perceived emotions and abstractive summaries of their triggers described in the post. We develop strong baselines to jointly detect emotions and summarize emotion triggers. Our analyses show that CovidET presents new challenges in emotion-specific summarization, as well as multi-emotion detection in long social media posts."
EMNLP2023,affect,Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models,11,"Topic Modeling,Expert finding and Q&A systems,Natural Language Processing Techniques","Certainty,Affect (linguistics),Overconfidence effect,Epistemology,Psychology,Computer science,Epistemic modality,Typology,Linguistics,Social psychology,Philosophy,Sociology,Anthropology",,https://aclanthology.org/2023.emnlp-main.335.pdf,"Kaitlyn Zhou,Dan Jurafsky,Tatsunori Hashimoto","The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like “I’m sure it’s”, “I think it’s”, or “Wikipedia says it’s” affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty."
CVPR2023,emotion,Decoupled Multimodal Distilling for Emotion Recognition,53,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Modality (human–computer interaction),Crossmodal,Modalities,Visualization,Artificial intelligence,Graph,Human–computer interaction,Distillation,Machine learning,Perception,Natural language processing,Pattern recognition (psychology),Visual perception,Theoretical computer science,Social science,Chemistry,Organic chemistry,Neuroscience,Sociology,Biology",https://github.com/mdswyz/DMD.,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.pdf,"Yong Li, Yuanzhi Wang, Zhen Cui","Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acoustic modalities. Despite the impressive performance of previous MER approaches, the inherent multimodal heterogeneities still haunt and the contribution of different modalities varies significantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each modality. Specially, the representation of each modality is decoupled into two parts, i.e., modality-irrelevant/-exclusive spaces, in a self-regression manner. DMD utilizes a graph distillation unit (GD-Unit) for each decoupled part so that each GD can be performed in a more specialized and effective manner. A GD-Unit consists of a dynamic graph where each vertice represents a modality and each edge indicates a dynamic knowledge distillation. Such GD paradigm provides a flexible knowledge transfer manner where the distillation weights can be automatically learned, thus enabling diverse crossmodal knowledge transfer patterns. Experimental results show DMD consistently obtains superior performance than state-of-the-art MER methods. Visualization results show the graph edges in DMD exhibit meaningful distributional patterns w.r.t. the modality-irrelevant/-exclusive feature spaces. Codes are released at https://github.com/mdswyz/DMD."
CVPR2023,emotion,Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network,14,"Emotion and Mood Recognition,Human Pose and Action Recognition,Video Analysis and Summarization","Computer science,Leverage (statistics),ENCODE,Modal,Focus (optics),Code (set theory),Key (lock),Artificial intelligence,Context (archaeology),Emotion recognition,Machine learning,Paleontology,Biochemistry,Chemistry,Physics,Computer security,Set (abstract data type),Biology,Polymer chemistry,Optics,Gene,Programming language",https://github.com/nku-zhichengzhang/WECL.,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf,"Zhicheng Zhang, Lijuan Wang, Jufeng Yang","Automatically predicting the emotions of user-generated videos (UGVs) receives increasing interest recently. However, existing methods mainly focus on a few key visual frames, which may limit their capacity to encode the context that depicts the intended emotions. To tackle that, in this paper, we propose a cross-modal temporal erasing network that locates not only keyframes but also context and audio-related information in a weakly-supervised manner. In specific, we first leverage the intra- and inter-modal relationship among different segments to accurately select keyframes. Then, we iteratively erase keyframes to encourage the model to concentrate on the contexts that include complementary information. Extensive experiments on three challenging video emotion benchmarks demonstrate that our method performs favorably against state-of-the-art approaches. The code is released on https://github.com/nku-zhichengzhang/WECL."
EMNLP2022,emotion,Calibrating Student Models for Emotion-related Tasks,1,"Topic Modeling,Multimodal Machine Learning Applications,Sentiment Analysis and Opinion Mining","Computer science,Artificial intelligence,Calibration,Regularization (linguistics),Machine learning,Distillation,Domain (mathematical analysis),Perspective (graphical),Empathy,Natural language processing,Statistics,Mathematics,Psychology,Mathematical analysis,Chemistry,Organic chemistry,Psychiatry",,https://aclanthology.org/2022.emnlp-main.629.pdf,"Mahshid Hosseini,Cornelia Caragea","Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique on the distillation objective and propose to use a simple yet effective mixup method informed by training dynamics for calibrating the student models. Underpinned by the regularization impact of the mixup process by providing better training signals to the student models using training dynamics, our proposed mixup strategy gradually enhances the student model’s calibration while effectively improving its performance. We evaluate the calibration of pre-trained language models through knowledge distillation over three tasks of emotion detection, sentiment analysis, and empathy detection. By conducting extensive experiments on different datasets, with both in-domain and out-of-domain test sets, we demonstrate that student models distilled from teacher models trained using our proposed mixup method obtained the lowest Expected Calibration Errors (ECEs) and best performance on both in-domain and out-of-domain test sets."
EMNLP2022,emotion,ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on Diversity over Language and Culture,4,Natural Language Processing Techniques,"Zhàng,Emphasis (telecommunications),Computer science,Diversity (politics),Linguistics,Natural language processing,Sociology,History,China,Philosophy,Anthropology,Telecommunications,Archaeology",,https://aclanthology.org/2022.emnlp-main.600.pdf,"Youssef Mohamed,Mohamed Abdelfattah,Shyma Alhuwaider,Feifan Li,Xiangliang Zhang,Kenneth Church,Mohamed Elhoseiny","This paper introduces ArtELingo, a new benchmark and dataset, designed to encourage work on diversity across languages and cultures. Following ArtEmis, a collection of 80k artworks from WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds another 0.79M annotations in Arabic and Chinese, plus 4.8K in Spanish to evaluate “cultural-transfer” performance. 51K artworks have 5 annotations or more in 3 languages. This diversity makes it possible to study similarities and differences across languages and cultures. Further, we investigate captioning tasks, and find diversity improves the performance of baseline models. ArtELingo is publicly available at ‘www.artelingo.org‘ with standard splits and baseline models. We hope our work will help ease future research on multilinguality and culturally-aware AI."
ECCV2024,emotion,Upper-body Hierarchical Graph for Skeleton Based Emotion Recognition in Assistive Driving,0,"Human Pose and Action Recognition,Video Surveillance and Tracking Methods,Emotion and Mood Recognition","Computer science,Skeleton (computer programming),Graph,Artificial intelligence,Computer vision,Emotion recognition,Pattern recognition (psychology),Theoretical computer science,Programming language","https://github.com/jerry-wjh/UbH-GCN.""",https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03697.pdf,"Jiehui Wu, Jiansheng Chen*, Qifeng Luo, Siqi Liu, Youze Xue, Huimin Ma","""Emotion recognition plays a crucial role in enhancing the safety and enjoyment of assistive driving experiences. By enabling intelligent systems to perceive and understand human emotions, we can significantly improve human-machine interactions. Current research in emotion recognition emphasizes facial expressions, speech and physiological signals, often overlooking body movement’s expressive potential. Existing most methods, reliant on full-body poses and graph convolutional networks with predetermined adjacency matrices, face challenges in driving scenarios, including limited visibility, restricted movement and imbalanced data distribution, which affect model generalization and accuracy. To overcome these limitations, we introduce an innovative emotion recognition method tailored for assistive driving. Our method leverages upper-body skeleton sequences, overcoming the constraints of full-body pose capture in driving scenario. Our architecture employs an upper-body hierarchical graph (UbH-Graph) to dynamically capture upper-body movement and emotion relationships. We uniquely incorporate class-specific variations during training, balancing feature distribution and enhancing emotion recognition. Our method outperforms existing multimodal approaches on the assistive driving dataset and demonstrates robustness and adaptability on the daily action dataset. Code is available at https://github.com/jerry-wjh/UbH-GCN."""
ECCV2024,affective,Facial Affective Behavior Analysis with Instruction Tuning,0,Consumer Perception and Purchasing Behavior,"Computer science,Psychology,Cognitive psychology,Human–computer interaction","https://johnx69.github.io/FABA/.""",https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02709.pdf,"Yifan Li*, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong","""Facial affective behavior analysis (FABA) is crucial for understanding human mental states from images. However, traditional approaches primarily deploy models to discriminate among discrete emotion categories, and lack the fine granularity and reasoning capability for complex facial behaviors. The advent of Multi-modal Large Language Models (MLLMs) has been proven successful in general visual understanding tasks. However, directly harnessing MLLMs for FABA is challenging due to the scarcity of datasets and benchmarks, neglecting facial prior knowledge, and low training efficiency. To address these challenges, we introduce (i ) an instruction-following dataset for two FABA tasks, , facial emotion and action unit recognition, (ii ) a benchmark FABA-Bench with a new metric considering both recognition and generation ability, and (iii ) a new MLLM EmoLA as a strong baseline to the community. Our initiative on the dataset and benchmarks reveal the nature and rationale of facial affective behaviors, , fine-grained facial movement, interpretability, and reasoning. Moreover, to build an effective and efficient FABA MLLM, we introduce a facial prior expert module with face structure knowledge and a low-rank adaptation module into pre-trained MLLM. We conduct extensive experiments on FABA-Bench and four commonly-used FABA datasets. The results demonstrate that the proposed facial prior expert can boost the performance and EmoLA achieves the best results on our FABA-Bench. On commonly-used FABA datasets, EmoLA is competitive rivaling task-specific state-of-the-art models. The dataset and codes are available: https://johnx69.github.io/FABA/."""
ACL2021,emotion,Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities,93,"Emotion and Mood Recognition,Human Pose and Action Recognition,Generative Adversarial Networks and Image Synthesis","Modalities,Modality (human–computer interaction),Computer science,Joint (building),Volume (thermodynamics),Association (psychology),Natural language processing,Artificial intelligence,Cognitive science,Linguistics,Psychology,Engineering,Sociology,Philosophy,Social science,Architectural engineering,Physics,Quantum mechanics,Psychotherapist",,https://aclanthology.org/2021.acl-long.203.pdf,"Jinming Zhao,Ruichen Li,Qin Jin","Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions. Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at "
ACL2021,affects,Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?,8,"Topic Modeling,Speech and dialogue systems,Sentiment Analysis and Opinion Mining","Granularity,Computer science,Context (archaeology),Volume (thermodynamics),Computational linguistics,Joint (building),Tracking (education),State (computer science),Natural language processing,Data science,Artificial intelligence,Linguistics,Information retrieval,Sociology,History,Programming language,Engineering,Philosophy,Architectural engineering,Pedagogy,Physics,Archaeology,Quantum mechanics",,https://aclanthology.org/2021.acl-long.193.pdf,"Puhai Yang,Heyan Huang,Xian-Ling Mao","Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. First, we explore how greatly different granularities affect dialogue state tracking. Then, we further discuss how to combine multiple granularities for dialogue state tracking. Finally, we apply the findings about context granularity to few-shot learning scenario. Besides, we have publicly released all codes."
ACL2023,emotion,Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction,3,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Robustness (evolution),Computer science,Graph,Relationship extraction,Artificial intelligence,Decision boundary,Representation (politics),Machine learning,Labeled data,Data mining,Theoretical computer science,Support vector machine,Information extraction,Biochemistry,Chemistry,Politics,Political science,Law,Gene",,https://aclanthology.org/2023.acl-long.62.pdf,"Huawen Feng,Junlong Liu,Junhao Zheng,Haibin Chen,Xichen Shang,Qianli Ma","Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using the same graph structure. In this paper, we propose a **J**oint **C**onstrained Learning framework with **B**oundary-adjusting for Emotion-Cause Pair Extraction (**JCB**). Specifically, through constrained learning, we summarize the prior rules existing in the data and force the model to take them into consideration in optimization, which helps the model learn a better representation from unbalanced data. Furthermore, we adjust the decision boundary of classifiers according to the relations between subtasks, which have always been ignored. No longer working independently as in the previous framework, the classifiers corresponding to three subtasks cooperate under the relation constraints. Experimental results show that **JCB** obtains competitive results compared with state-of-the-art methods and prove its robustness on unbalanced data."
ACL2023,emotion,Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition,10,Emotion and Mood Recognition,"Ping (video games),Modal,Modality (human–computer interaction),Computer science,Artificial intelligence,Independence (probability theory),Natural language processing,Linguistics,Speech recognition,Mathematics,Philosophy,Statistics,Computer security,Chemistry,Polymer chemistry",,https://aclanthology.org/2023.acl-long.39v2.pdf,"Jun Sun,Shoukang Han,Yu-Ping Ruan,Xiaoning Zhang,Shu-Kai Zheng,Yulong Liu,Yuxin Huang,Taihao Li","Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset with Modality-wise Annotions, abbreviated as CHERMA, provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. The model consists of uni-modal transformer modules that learn representations for each modality, and a multi-modal transformer module that fuses all modalities. All the modules are supervised by their corresponding labels separately, and the forward information flow is uni-directionally from the uni-modal modules to the multi-modal module. The supervision strategy and the model architecture guarantee each individual modality learns its representation independently, and meanwhile the multi-modal module aggregates all information. Extensive empirical results demonstrate that our proposed scheme outperforms state-of-the-art alternatives, corroborating the importance of modality independence in multi-modal emotion recognition. The dataset and codes are availabel at "
ICLR2024,emo,EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING,0,"Topic Modeling,Natural Language Processing Techniques,Speech Recognition and Synthesis","Language model,Computer science,Earth mover's distance,Autoregressive model,Artificial intelligence,Principle of maximum entropy,Metric (unit),Entropy (arrow of time),Prior probability,Machine learning,Bayesian probability,Econometrics,Mathematics,Engineering,Operations management,Physics,Quantum mechanics",,https://openreview.net/pdf/6b921e5f84e5257651d95fd8540cd1d7cd603186.pdf,"Siyu Ren,Zhiyong Wu,Kenny Q. Zhu","Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language models trained using EMO and MLE. We find that EMO demonstrates a consistently better language modeling performance than MLE across domains. Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences. This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models."
ICLR2024,emotion,VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition,-1,,,,https://openreview.net/pdf/8a045a606abf89ad24e81dea7ff199c2cb5fe88a.pdf,"Chenyu Liu,XINLIANG ZHOU,Zhengri Zhu,Liming Zhai,Ziyu Jia,Yang Liu","The research on human emotion under electroencephalogram (EEG) is an emerging field in which cross-subject emotion recognition (ER) is a promising but challenging task. Many approaches attempt to find emotionally relevant domain-invariant features using domain adaptation (DA) to improve the accuracy of cross-subject ER. However, two problems still exist with these methods. First, only single-modal data (EEG) is utilized, ignoring the complementarity between multi-modal physiological signals. Second, these methods aim to completely match the signal features between different domains, which is difficult due to the extreme individual differences of EEG. To solve these problems, we introduce the complementarity of multi-modal physiological signals and propose a new method for cross-subject ER that does not align the distribution of signal features but rather the distribution of spatio-temporal relationships between features. We design a Variational Bayesian Heterogeneous Graph Neural Network (VBH-GNN) with Relationship Distribution Adaptation (RDA). The RDA first aligns the domains by expressing the model space as a posterior distribution of a heterogeneous graph for a given source domain. Then, the RDA transforms the heterogeneous graph into an emotion-specific graph to further align the domains for the downstream ER task. Extensive experiments on two public datasets, DEAP and Dreamer, show that our VBH-GNN outperforms state-of-the-art methods in cross-subject scenarios."
EMNLP2021,emotion,Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks,10,"Topic Modeling,Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Conversation,Shot (pellet),Computer science,Speech recognition,Emotion recognition,Psychology,Communication,Natural language processing,Chemistry,Organic chemistry",,https://aclanthology.org/2021.emnlp-main.549.pdf,"Gaël Guibon,Matthieu Labeau,Hélène Flamein,Luce Lefeuvre,Chloé Clavel","Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones."
EMNLP2024,emotion,Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts,0,"Technology and Data Analysis,Diverse Approaches in Healthcare and Education Studies,Education and Learning Interventions","Key (lock),Computer science,Human–computer interaction,Computer security",,https://aclanthology.org/2024.emnlp-main.331.pdf,"Jaewook Lee,Yeajin Jang,Hongjin Kim,Woojin Lee,Harksoo Kim","Emotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance."
ICASSP2022,emotion,AuxFormer: Robust Approach to Audiovisual Emotion Recognition.,18,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Modalities,Computer science,Leverage (statistics),Robustness (evolution),Transformer,Artificial neural network,Artificial intelligence,Modality (human–computer interaction),Fuse (electrical),Architecture,Macro,Machine learning,Speech recognition,Engineering,Art,Social science,Biochemistry,Chemistry,Voltage,Sociology,Electrical engineering,Visual arts,Gene,Programming language",,,"Zhengjun Yue,Erfan Loweimi,Zoran Cvetkovic",
ICASSP2022,emotion,Fusing ASR Outputs in Joint Training for Speech Emotion Recognition.,33,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Joint (building),Pipeline (software),Word (group theory),Fuse (electrical),Word error rate,Ground truth,Emotion recognition,Natural language processing,Artificial intelligence,Linguistics,Engineering,Architectural engineering,Philosophy,Electrical engineering,Programming language",,,"Mohammad Soleymanpour,Michael T. Johnson,Rahim Soleymanpour,Jeffrey Berry",
ICASSP2022,emotion,Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information.,93,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Speech recognition,Emotion recognition,Computer science",,,"Sondes Abderrazek,Corinne Fredouille,Alain Ghio,Muriel Lalain,Christine Meunier,Virginie Woisard",
ICASSP2021,emotion,Generating Empathetic Responses by Injecting Anticipated Emotion.,15,"Topic Modeling,Humor Studies and Applications,Artificial Intelligence in Games","Psychology,Computer science,Social psychology",,,"Jun Bai,Wenge Rong,Feiyu Xia,Yanmeng Wang,Yuanxin Ouyang,Zhang Xiong",
ICASSP2022,emotion,SERAB: A Multi-Lingual Benchmark for Speech Emotion Recognition.,22,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Leverage (statistics),Benchmarking,Benchmark (surveying),Utterance,Generalization,Artificial intelligence,Speech recognition,Feature (linguistics),Machine learning,Process (computing),Artificial neural network,Emotion recognition,Natural language processing,Mathematical analysis,Linguistics,Philosophy,Mathematics,Geodesy,Marketing,Business,Geography,Operating system",,,"Huang-Cheng Chou,Wei-Cheng Lin,Chi-Chun Lee,Carlos Busso",
ICASSP2022,emotion,Enhancing Privacy Through Domain Adaptive Noise Injection For Speech Emotion Recognition.,12,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Inference,Noise (video),Machine learning,Speech recognition,Artificial intelligence,Representation (politics),Data modeling,Decision tree,Database,Politics,Political science,Law,Image (mathematics)",,,"Andrew Koh,Fuzhao Xue,Chng Eng Siong",
ICASSP2022,emotion,Selective Multi-Task Learning For Speech Emotion Recognition Using Corpora Of Different Styles.,12,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Computer science,Task (project management),Emotion recognition,Style (visual arts),Speech recognition,Natural language processing,Artificial intelligence,Expression (computer science),Labeled data,Management,Archaeology,Economics,History,Programming language",,,"Puyuan Peng,David Harwath",
ICASSP2022,emotion,Exploiting Annotators' Typed Description of Emotion Perception to Maximize Utilization of Ratings for Speech Emotion Recognition.,18,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Perception,Natural language processing,Emotion recognition,Representation (politics),Sentiment analysis,Artificial intelligence,Ground truth,Class (philosophy),Emotion detection,Emotion perception,Psychology,Neuroscience,Politics,Political science,Law,Facial expression",,,"Jijun Shi,Shanshe Wang,Ronggang Wang,Siwei Ma",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ICCV2023,"emotion,emotional",Emotional Listener Portrait: Neural Listener Head Generation with Emotion,0,"Face recognition and analysis,Speech and Audio Processing,Generative Adversarial Networks and Image Synthesis","Conversation,Head (geology),Motion (physics),Facial expression,Computer science,Natural (archaeology),Dynamics (music),Speech recognition,Portrait,Artificial intelligence,Psychology,Communication,Natural language processing,Art,Art history,Pedagogy,Archaeology,Geomorphology,History,Geology",,https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.pdf,"Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, Chenliang Xu","Listener head generation centers on generating non-verbal behaviors (e.g., smile) of a listener in reference to the information delivered by a speaker. A significant challenge when generating such responses is the non-deterministic nature of fine-grained facial expressions during a conversation, which varies depending on the emotions and attitudes of both the speaker and the listener. To tackle this problem, we propose the Emotional Listener Portrait (ELP), which treats each fine-grained facial motion as a composition of several discrete motion-codewords and explicitly models the probability distribution of the motions under different emotional contexts in conversation. Benefiting from the ""explicit"" and ""discrete"" design, our ELP model can not only automatically generate natural and diverse responses toward a given speaker via sampling from the learned distribution but also generate controllable responses with a predetermined attitude. Under several quantitative metrics, our ELP exhibits significant improvements compared to previous methods."
ICCV2023,"affective,emotions",Affective Image Filter: Reflecting Emotions from Text to Images,4,"Generative Adversarial Networks and Image Synthesis,Image Enhancement Techniques,Aesthetic Perception and Analysis","Computer science,Visualization,Artificial intelligence,Filter (signal processing),Transformer,Image (mathematics),Architecture,Natural language processing,Computer vision,Human–computer interaction,Physics,Quantum mechanics,Voltage,Art,Visual arts",,https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.pdf,"Shuchen Weng, Peixuan Zhang, Zheng Chang, Xinlong Wang, Si Li, Boxin Shi","Understanding the emotions in text and presenting them visually is a very challenging problem that requires a deep understanding of natural language and high-quality image synthesis simultaneously. In this work, we propose Affective Image Filter (AIF), a novel model that is able to understand the visually-abstract emotions from the text and reflect them to visually-concrete images with appropriate colors and textures. We build our model based on the multi-modal transformer architecture, which unifies both images and texts into tokens and encodes the emotional prior knowledge. Various loss functions are proposed to understand complex emotions and produce appropriate visualization. In addition, we collect and contribute a new dataset with abundant aesthetic images and emotional texts for training and evaluating the AIF model. We carefully design four quantitative metrics and conduct a user study to comprehensively evaluate the performance, which demonstrates our AIF model outperforms state-of-the-art methods and could evoke specific emotional responses from human observers."
EMNLP2024,affect,How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?,0,"EFL/ESL Teaching and Learning,Second Language Acquisition and Learning,Educational Strategies and Epistemologies","Affect (linguistics),Computer science,Context (archaeology),Information retrieval,Natural language processing,World Wide Web,Data science,Psychology,History,Communication,Archaeology",https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever.,https://aclanthology.org/2024.emnlp-main.305.pdf,"Yang Luo,Zangwei Zheng,Zirui Zhu,Yang You","The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly multimodal in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. However, this effectiveness hinges on the appropriate selection of in-context examples, a process currently biased towards visual data, overlooking textual information. More importantly, the area of supervised retrievers for retrieval of multimodal in-context learning, crucial for optimal in-context example selection, continues to be investigated. Our study provides an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Based on the above finding, we introduce a novel supervised MLLM prompt retriever MSIER that leverages a trained retriever based on MLLM’s confidence to select examples, which enhances multimodal in-context learning efficiency. This approach is validated through extensive testing across three different tasks, demonstrating the method’s effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method’s training and explore the transferability of the supervised prompt retriever. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data. The public code is available at https://github.com/NUS-HPC-AI-Lab/Multimodal-ICL-Retriever."
EMNLP2022,affect,Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?,4,"Natural Language Processing Techniques,Topic Modeling,Speech and dialogue systems","Computer science,Linguistics,Variation (astronomy),Zero (linguistics),Natural language processing,Artificial intelligence,Context (archaeology),Transfer (computing),Rule-based machine translation,Affect (linguistics),History,Physics,Philosophy,Archaeology,Parallel computing,Astrophysics",,https://aclanthology.org/2022.emnlp-main.552.pdf,"Ningyu Xu,Tao Gui,Ruotian Ma,Qi Zhang,Jingting Ye,Menghan Zhang,Xuanjing Huang","Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer."
ECCV2024,emotional,EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis,1,"Handwritten Text Recognition Techniques,Face recognition and analysis,Generative Adversarial Networks and Image Synthesis","Computer science,Head (geology),Geology,Geomorphology","https://tanshuai0219.github.io/EDTalk/""",https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01019.pdf,"Shuai Tan*, Bin Ji, Mengxiao Bi, ye pan*","""Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal inputs—both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. The code and pretrained models are released at: https://tanshuai0219.github.io/EDTalk/"""
IJCAI2024,emote,EMOTE: An Explainable Architecture for Modelling the Other through Empathy,1,Ethics and Social Impacts of AI,"Empathy,Architecture,Computer science,Psychology,Art,Social psychology,Visual arts",,https://www.ijcai.org/proceedings/2024/0539.pdf,"Manisha Senadeera, Thommen Karimpanal George, Stephan Jacobs, Sunil Gupta, Santu Rana","                        Empathy allows us to assume others are like us and have goals analogous to our own. This can also at times be applied to multi-agent games - e.g. Agent 1's attraction to green balls is analogous to Agent 2's attraction to red balls.  Drawing inspiration from empathy, we propose EMOTE, a simple and explainable inverse reinforcement learning (IRL) approach designed to model another agent's action-value function and from it, infer a unique reward function. This is done by referencing the learning agent's own action value function, removing the need to maintain independent action-value estimates for the modelled agents whilst simultaneously addressing the ill-posed nature of IRL by inferring a unique reward function. We experiment on minigrid environments showing EMOTE: (a) produces more consistent reward estimates relative to other IRL baselines (b) is robust in scenarios with composite reward and action-value functions (c) produces human-interpretable states, helping to explain how the agent views other agents.                     "
IJCAI2022,emotion,Visual Emotion Representation Learning via Emotion-Aware Pre-training,2,"Multimodal Machine Learning Applications,Domain Adaptation and Few-Shot Learning,Text and Document Classification Technologies","Computer science,Ambiguity,Emotion recognition,Representation (politics),Emotion classification,Artificial intelligence,Perception,Feature learning,Deep learning,Object (grammar),Psychology,Neuroscience,Politics,Political science,Law,Programming language",,https://www.ijcai.org/proceedings/2022/0234.pdf,"Yue Zhang, Wanying Ding, Ran Xu, Xiaohua Hu","Despite recent progress in deep learning, visual emotion recognition remains a challenging problem due to ambiguity of emotion perception, diverse concepts related to visual emotion and lack of large-scale annotated dataset. In this paper, we present a large-scale multimodal pre-training method to learn visual emotion representation by aligning emotion, object, attribute triplet with a contrastive loss. We conduct our pre-training on a large web dataset with noisy tags and fine-tune on visual emotion classification datasets. Our method achieves state-of-the-art performance for visual emotion classification."
CVPR2022,emotion,"Neural Emotion Director: Speech-Preserving Semantic Control of Facial Expressions in ""In-the-Wild"" Videos",19,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Advanced Image Processing Techniques","Computer science,Facial expression,Rendering (computer graphics),Artificial intelligence,Parametric statistics,Face (sociological concept),Deep neural networks,Deep learning,Computer vision,Speech recognition,Statistics,Mathematics,Social science,Sociology",,https://openaccess.thecvf.com/content/CVPR2022/papers/Papantoniou_Neural_Emotion_Director_Speech-Preserving_Semantic_Control_of_Facial_Expressions_in_CVPR_2022_paper.pdf,"Foivos Paraperas Papantoniou, Panagiotis P. Filntisis, Petros Maragos, Anastasios Roussos","In this paper, we introduce a novel deep learning method for photo-realistic manipulation of the emotional state of actors in ""in-the-wild"" videos. The proposed method is based on a parametric 3D face representation of the actor in the input scene that offers a reliable disentanglement of the facial identity from the head pose and facial expressions. It then uses a novel deep domain translation framework that alters the facial expressions in a consistent and plausible manner, taking into account their dynamics. Finally, the altered facial expressions are used to photo-realistically manipulate the facial region in the input scene based on an especially-designed neural face renderer. To the best of our knowledge, our method is the first to be capable of controlling the actor's facial expressions by even using as a sole input the semantic labels of the manipulated emotions, while at the same time preserving the speech-related lip movements. We conduct extensive qualitative and quantitative evaluations and comparisons, which demonstrate the effectiveness of our approach and the especially promising results that we obtain. Our method opens a plethora of new possibilities for useful applications of neural rendering technologies, ranging from movie post-production and video games to photo-realistic affective avatars."
ACL2021,emotion,Distributed Representations of Emotion Categories in Emotion Space,12,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Joint (building),Computer science,Space (punctuation),Natural language processing,Computational linguistics,Volume (thermodynamics),Cognitive science,Association (psychology),Linguistics,Artificial intelligence,Psychology,Epistemology,Engineering,Philosophy,Physics,Operating system,Architectural engineering,Quantum mechanics",,https://aclanthology.org/2021.acl-long.184.pdf,"Xiangyu Wang,Chengqing Zong","Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space."
ACL2021,emotion,Directed Acyclic Graph Network for Conversational Emotion Recognition,183,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Computer science,Directed acyclic graph,Natural language processing,Joint (building),Computational linguistics,Emotion recognition,Association (psychology),Artificial intelligence,Graph,Speech recognition,Theoretical computer science,Psychology,Algorithm,Engineering,Architectural engineering,Psychotherapist",,https://aclanthology.org/2021.acl-long.123.pdf,"Weizhou Shen,Siyue Wu,Yunyi Yang,Xiaojun Quan","The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC."
IJCAI2022,emotional,Automatic Recognition of Emotional Subgroups in Images,1,"Face and Expression Recognition,Emotion and Mood Recognition,Anomaly Detection Techniques and Applications","Artificial intelligence,Computer science,Pattern recognition (psychology),Cluster analysis,Gaze,Set (abstract data type),Face (sociological concept),Hierarchical clustering,Image (mathematics),Computer vision,Social science,Sociology,Programming language",,https://www.ijcai.org/proceedings/2022/0190.pdf,"Emmeke Veltmeijer, Charlotte Gerritsen, Koen Hindriks","Both social group detection and group emotion recognition in images are growing fields of interest, but never before have they been combined. In this work we aim to detect emotional subgroups in images, which can be of great importance for crowd surveillance or event analysis. To this end, human annotators are instructed to label a set of 171 images, and their recognition strategies are analysed. Three main strategies for labeling images are identified, with each strategy assigning either 1) more weight to emotions (emotion-based fusion), 2) more weight to spatial structures (group-based fusion), or 3) equal weight to both (summation strategy). Based on these strategies, algorithms are developed to automatically recognize emotional subgroups. In particular, K-means and hierarchical clustering are used with location and emotion features derived from a fine-tuned VGG network. Additionally, we experiment with face size and gaze direction as extra input features. The best performance comes from hierarchical clustering with emotion, location and gaze direction as input."
IJCAI2022,emotion,Emotion-Controllable Generalized Talking Face Generation,22,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Advanced Image Processing Techniques","Computer science,Artificial intelligence,Face (sociological concept),Texture (cosmology),Representation (politics),Convolutional neural network,Computer vision,Optical flow,Motion (physics),Feature (linguistics),Landmark,Scope (computer science),Pattern recognition (psychology),Speech recognition,Image (mathematics),Social science,Linguistics,Philosophy,Sociology,Politics,Political science,Law,Programming language",,https://www.ijcai.org/proceedings/2022/0184.pdf,"Sanjana Sinha, Sandika Biswas, Ravindra Yadav, Brojeshwar Bhowmick","Despite the significant progress in recent years, very few of the AI-based talking face generation methods attempt to render natural emotions. Moreover, the scope of the methods is majorly limited to the characteristics of the training dataset, hence they fail to generalize to arbitrary unseen faces. In this paper, we propose a one-shot facial geometry-aware emotional talking face generation method that can generalize to arbitrary faces. We propose a graph convolutional neural network that uses speech content feature, along with an independent emotion input to generate emotion and speech-induced motion on facial geometry-aware landmark representation.  This representation is further used in our optical flow-guided texture generation network for producing the texture. We propose a two-branch texture generation network, with motion and texture branches designed to consider the motion and texture content independently. Compared to the previous emotion talking face methods, our method can adapt to arbitrary faces captured in-the-wild by fine-tuning with only a single image of the target identity in neutral emotion."
IJCAI2024,emotion,Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition,0,"Advanced Memory and Neural Computing,Gaze Tracking and Assistive Technology,EEG and Brain-Computer Interfaces","Computer science,Frame (networking),Artificial intelligence,Artificial neural network,Distillation,Elegance,Block (permutation group theory),Event (particle physics),Scheme (mathematics),Human–computer interaction,Machine learning,Computer vision,Speech recognition,Telecommunications,Mathematical analysis,Philosophy,Chemistry,Physics,Geometry,Mathematics,Organic chemistry,Quantum mechanics,Aesthetics",,https://www.ijcai.org/proceedings/2024/0350.pdf,"Yang Wang, Haiyang Mei, Qirui Bao, Ziqi Wei, Mike Zheng Shou, Haizhou Li, Bo Dong, Xin Yang","We introduce a novel multimodality synergistic knowledge distillation scheme tailored for efficient single-eye motion recognition tasks. This method allows a lightweight, unimodal student spiking neural network (SNN) to extract rich knowledge from an event-frame multimodal teacher network. The core strength of this approach is its ability to utilize the ample, coarser temporal cues found in conventional frames for effective emotion recognition. Consequently, our method adeptly interprets both temporal and spatial information from the conventional frame domain, eliminating the need for specialized sensing devices, e.g., event-based camera. The effectiveness of our approach is thoroughly demonstrated using both existing and our compiled single-eye emotion recognition datasets, achieving unparalleled performance in accuracy and efficiency over existing state-of-the-art methods."
IJCAI2024,emotion,Multi-level Disentangling Network for Cross-Subject Emotion Recognition Based on Multimodal Physiological Signals,0,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces","Computer science,Subject (documents),Emotion recognition,Speech recognition,Artificial intelligence,Pattern recognition (psychology),World Wide Web",,https://www.ijcai.org/proceedings/2024/0340.pdf,"Ziyu Jia, Fengming Zhao, Yuzhe Guo, Hairong Chen, Tianzi Jiang","Emotion recognition based on multimodal physiological signals is attracting more and more attention. However, how to deal with the consistency and heterogeneity of multimodal physiological signals, as well as individual differences across subjects, pose two significant challenges. In this paper, we propose a Multi-level Disentangling Network named MDNet for cross-subject emotion recognition based on multimodal physiological signals. Specifically, MDNet consists of a modality-level disentangling module and a subject-level disentangling module. The modality-level disentangling module projects multimodal physiological signals into modality-invariant subspace and modality-specific subspace, capturing modality-invariant features and modality-specific features. The subject-level disentangling module separates subject-shared features and subject-private features among different subjects from multimodal data, which facilitates cross-subject emotion recognition. Experiments on two multimodal emotion datasets demonstrate that MDNet outperforms other state-of-the-art baselines."
EMNLP2022,emotion,UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition,82,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Sentiment analysis,Computer science,Benchmark (surveying),Modalities,Affective computing,Consistency (knowledge bases),Perspective (graphical),Exploit,Artificial intelligence,Modality (human–computer interaction),Conversation,Natural language processing,Multimodality,Psychology,World Wide Web,Social science,Computer security,Geodesy,Communication,Sociology,Geography",,https://aclanthology.org/2022.emnlp-main.534.pdf,"Guimin Hu,Ting-En Lin,Yi Zhao,Guangming Lu,Yuchuan Wu,Yongbin Li","Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods."
EMNLP2023,emotion,Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts,2,"Hate Speech and Cyberbullying Detection,Sentiment Analysis and Opinion Mining,Spam and Phishing Detection","Computer science,Artificial intelligence,Distress,Identification (biology),Interpretability,Natural language processing,Social media,Sentiment analysis,Machine learning,Information retrieval,Psychology,World Wide Web,Botany,Psychotherapist,Biology",,https://aclanthology.org/2023.emnlp-main.275.pdf,"Gopendra Singh,Soumitra Ghosh,Atul Verma,Chetna Painkra,Asif Ekbal","Due to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions. In this work, we present a novel problem of "
ICCV2023,emotional,Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation,12,"Generative Adversarial Networks and Image Synthesis,Speech and Audio Processing,Human Pose and Action Recognition","Computer science,Adaptation (eye),Inefficiency,Generalization,Human–computer interaction,Transformer,Multimedia,Artificial intelligence,Speech recognition,Psychology,Mathematical analysis,Physics,Mathematics,Quantum mechanics,Voltage,Neuroscience,Economics,Microeconomics",https://yuangan.github.io/eat/,https://openaccess.thecvf.com/content/ICCV2023/papers/Gan_Efficient_Emotional_Adaptation_for_Audio-Driven_Talking-Head_Generation_ICCV_2023_paper.pdf,"Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang","Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/"
ACL2021,emotion,Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection,104,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Computer science,Transformer,Joint (building),Computational linguistics,Natural language processing,Linguistics,Artificial intelligence,Engineering,Philosophy,Architectural engineering,Voltage,Electrical engineering",,https://aclanthology.org/2021.acl-long.125.pdf,"Lixing Zhu,Gabriele Pergola,Lin Gui,Deyu Zhou,Yulan He","Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories."
ACL2021,emoji,Assessing Emoji Use in Modern Text Processing Tools,7,"Digital Communication and Language,Sentiment Analysis and Opinion Mining,Hate Speech and Cyberbullying Detection","Emoji,Computational linguistics,Computer science,Natural language processing,Joint (building),Volume (thermodynamics),Artificial intelligence,Linguistics,Programming language,World Wide Web,Engineering,Social media,Philosophy,Architectural engineering,Physics,Quantum mechanics",,https://aclanthology.org/2021.acl-long.110v2.pdf,"Abu Awal Md Shoeb,Gerard de Melo","Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors. This also leads to an increased need for systems and tools to operate on text containing emojis. In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them. In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis. Our findings show that many systems still have notable shortcomings when operating on text containing emojis."
EMNLP2024,affect,How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?,0,Artificial Intelligence in Healthcare and Education,"Affect (linguistics),Perception,Psychology,Computer science,Knowledge management,Communication,Neuroscience",,https://aclanthology.org/2024.emnlp-main.279.pdf,"Zhuoyan Li,Chen Liang,Jing Peng,Ming Yin","Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people’s perceptions of the writing on various aspects, including their evaluation on the quality of the writing, and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals’ quality evaluations of the same writing. Indeed, factors such as an individual’s writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI’s content generation assistance among the top-ranked writings."
EMNLP2024,emotion,Visual Prompting in LLMs for Enhancing Emotion Recognition,0,Emotion and Mood Recognition,"Computer science,Emotion recognition,Cognitive psychology,Psychology,Human–computer interaction,Multimedia,Artificial intelligence",,https://aclanthology.org/2024.emnlp-main.257.pdf,"Qixuan Zhang,Zhifeng Wang,Dylan Zhang,Wenjia Niu,Sabrina Caldwell,Tom Gedeon,Yang Liu,Zhenyue Qin","Vision Large Language Models (VLLMs) are transforming the intersection of computer vision and natural language processing; however, the potential of using visual prompts for emotion recognition in these models remains largely unexplored and untapped. Traditional methods in VLLMs struggle with spatial localization and often discard valuable global context. We propose a novel Set-of-Vision prompting (SoV) approach that enhances zero-shot emotion recognition by using spatial information, such as bounding boxes and facial landmarks, to mark targets precisely. SoV improves accuracy in face count and emotion categorization while preserving the enriched image context. Through comprehensive experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model’s ability to comprehend facial expressions in natural environments. Our findings demonstrate the effectiveness of integrating spatial visual prompts into VLLMs for improving emotion recognition performance."
EMNLP2021,emotion,Dimensional Emotion Detection from Categorical Emotion,16,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Emotion and Mood Recognition","Categorical variable,Computer science,Natural language processing,Valence (chemistry),Arousal,Sentence,Artificial intelligence,Speech recognition,Correlation,Dominance (genetics),Psychology,Machine learning,Mathematics,Social psychology,Biochemistry,Chemistry,Physics,Geometry,Quantum mechanics,Gene",,https://aclanthology.org/2021.emnlp-main.358.pdf,"Sungjoon Park,Jiseon Kim,Seonghyeon Ye,Jaeyeol Jeon,Hee Young Park,Alice Oh","We present a model to predict fine-grained emotions along the continuous dimensions of valence, arousal, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover’s Distance) loss between the predicted VAD score distribution and the categorical emotion distributions sorted along VAD, and it can simultaneously classify the emotion categories and predict the VAD scores for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations."
EMNLP2023,emotions,Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning,0,"Sentiment Analysis and Opinion Mining,Topic Modeling,Natural Language Processing Techniques","Computer science,Artificial intelligence,Sarcasm,Natural language processing,Cluster analysis,Sentiment analysis,Language model,Representation (politics),Feature learning,Machine learning,Linguistics,Irony,Philosophy,Politics,Political science,Law",,https://aclanthology.org/2023.emnlp-main.222.pdf,"Sapan Shah,Sreedhar Reddy,Pushpak Bhattacharyya","We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting."
ACL2024,emotion,Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation,0,"Opinion Dynamics and Social Influence,Misinformation and Its Impacts,Anomaly Detection Techniques and Applications","Ambiguity,Computer science,Estimation,Domain (mathematical analysis),Artificial intelligence,Mathematics,Engineering,Systems engineering,Mathematical analysis,Programming language",,https://aclanthology.org/2024.acl-long.114.pdf,"Wen Wu,Bo Li,Chao Zhang,Chung-Cheng Chiu,Qiujia Li,Junwen Bai,Tara Sainath,Phil Woodland","The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation."
ACL2024,emotion,Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition,1,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Sentiment analysis,Modalities,Computer science,Emotion recognition,Artificial intelligence,Multimodal therapy,Speech recognition,Natural language processing,Psychology,Psychotherapist,Social science,Sociology",https://github.com/zrguo/MPLMM.,https://aclanthology.org/2024.acl-long.94.pdf,"Zirun Guo,Tao Jin,Zhou Zhao","The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model’s performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intra- and inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at https://github.com/zrguo/MPLMM."
CVPR2024,"emogen,emotional",EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models,2,"Generative Adversarial Networks and Image Synthesis,Video Analysis and Summarization,Image Retrieval and Classification Techniques","Computer science,Image (mathematics),Artificial intelligence,Content (measure theory),Diffusion,Computer vision,Mathematics,Physics,Mathematical analysis,Thermodynamics",https://vcc.tech/research/2024/EmoGen.,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf,"Jingyuan Yang, Jiawei Feng, Hui Huang",Recent years have witnessed remarkable progress in image generation task where users can create visually astonishing images with high-quality. However exsiting text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). Several efforts have been made to modify image emotions with color and style adjustments facing limitations in effectively conveying emotions with fixed image contents. In this work we introduce Emotional Image Content Generation (EIGC) a new task to generate semantic-clear and emotion-faithful images given emotion categories. Specifically we propose an emotion space and construct a mapping network to align it with powerful Contrastive Language-Image Pre-training (CLIP) space providing a concrete interpretation of abstract emotions. Attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. Our method outperforms the state-the-art text-to-image approaches both quantitatively and qualitatively where we derive three custom metrics i.e.emotion accuracy semantic clarity and semantic diversity. In addition to generation our method can help emotion understanding and inspire emotional art design. Project page: https://vcc.tech/research/2024/EmoGen.
CVPR2024,affective,MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation,0,"Music and Audio Processing,Human Pose and Action Recognition,Generative Adversarial Networks and Image Synthesis","Representation (politics),Computer science,Distillation,Artificial intelligence,Distribution (mathematics),Mathematics,Chemistry,Chromatography,Mathematical analysis,Politics,Political science,Law",,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MART_Masked_Affective_RepresenTation_Learning_via_Masked_Temporal_Distribution_Distillation_CVPR_2024_paper.pdf,"Zhicheng Zhang, Pancheng Zhao, Eunil Park, Jufeng Yang",Limited training data is a long-standing problem for video emotion analysis (VEA). Existing works leverage the power of large-scale image datasets for transferring while failing to extract the temporal correlation of affective cues in the video. Inspired by psychology research and empirical theory we verify that the degree of emotion may vary in different segments of the video thus introducing the sentiment complementary and emotion intrinsic among temporal segments. We propose an MAE-style method for learning robust affective representation of videos via masking termed MART. First we extract the affective cues of the lexicon and verify the extracted one by computing its matching score with video content in terms of sentiment and emotion scores alongside the temporal dimension. Then with the verified cues we propose masked affective modeling to recover temporal emotion distribution. We present temporal affective complementary learning that pulls the complementary part and pushes the intrinsic one of masked multimodal features where the constraint is set with cross-modal attention among features to mask the video and recover the degree of emotion among segments. Extensive experiments on five benchmarks show the superiority of our method in video sentiment analysis video emotion recognition multimodal sentiment analysis and multimodal emotion recognition.
ACL2022,emotional,MISC: A Mixed Strategy-Aware Model integrating COMET for Emotional Support Conversation,49,"Mental Health via Writing,Emotion and Mood Recognition,Machine Learning in Healthcare","Conversation,Empathy,Computer science,Benchmark (surveying),Focus (optics),Emotional distress,Human–computer interaction,Emotion detection,Artificial intelligence,Emotion recognition,Psychology,Social psychology,Communication,Anxiety,Physics,Geodesy,Psychiatry,Optics,Geography",,https://aclanthology.org/2022.acl-long.25.pdf,"Quan Tu,Yanran Li,Jianwei Cui,Bin Wang,Ji-Rong Wen,Rui Yan","Applying existing methods to emotional support conversation—which provides valuable assistance to people who are in need—has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user’s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user’s distress. To address the problems, we propose a novel model "
ACL2024,affected,How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,6,"Natural Language Processing Techniques,Topic Modeling","Computer science,Composition (language),Artificial intelligence,Natural language processing,Language model,Data modeling,Linguistics,Database,Philosophy",,https://aclanthology.org/2024.acl-long.12.pdf,"Guanting Dong,Hongyi Yuan,Keming Lu,Chengpeng Li,Mingfeng Xue,Dayiheng Liu,Wei Wang,Zheng Yuan,Chang Zhou,Jingren Zhou","Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, codegeneration, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns."
CVPR2022,emotion,MDAN: Multi-Level Dependent Attention Network for Visual Emotion Analysis,32,"Advanced Computing and Algorithms,Visual Attention and Saliency Detection,Sentiment Analysis and Opinion Mining","Leverage (statistics),Computer science,Granularity,Cognitive psychology,Emotion classification,Hierarchy,Contrast (vision),Artificial intelligence,Psychology,Economics,Market economy,Operating system",,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_MDAN_Multi-Level_Dependent_Attention_Network_for_Visual_Emotion_Analysis_CVPR_2022_paper.pdf,"Liwen Xu, Zhengtao Wang, Bin Wu, Simon Lui","Visual Emotion Analysis (VEA) is attracting increasing attention. One of the biggest challenges of VEA is to bridge the affective gap between visual clues in a picture and the emotion expressed by the picture. As the granularity of emotions increases, the affective gap increases as well. Existing deep approaches try to bridge the gap by directly learning discrimination among emotions globally in one shot without considering the hierarchical relationship among emotions at different affective levels and the affective level of emotions to be classified. In this paper, we present the Multi-level Dependent Attention Network (MDAN) with two branches, to leverage the emotion hierarchy and the correlation between different affective levels and semantic levels. The bottom-up branch directly learns emotions at the highest affective level and strictly follows the emotion hierarchy while predicting emotions at lower affective levels. In contrast, the top-down branch attempt to disentangle the affective gap by one-to-one mapping between semantic levels and affective levels, namely, Affective Semantic Mapping. At each semantic level, a local classifier learns discrimination among emotions at the corresponding affective level. Finally, We integrate global learning and local learning into a unified deep framework and optimize the network simultaneously. Moreover, to properly extract and leverage channel dependencies and spatial attention while disentangling the affective gap, we carefully designed two attention modules: the Multi-head Cross Channel Attention module and the Level-dependent Class Activation Map module. Finally, the proposed deep framework obtains new state-of-the-art performance on six VEA benchmarks, where it outperforms existing state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo dataset at 25 classes classification accuracy."
ICASSP2021,emotional,Depression Detection by Analysing Eye Movements on Emotional Images.,11,"Gaze Tracking and Assistive Technology,EEG and Brain-Computer Interfaces,Emotion and Mood Recognition","Disengagement theory,Eye movement,Computer science,Artificial intelligence,Eye tracking,Support vector machine,Stimulus (psychology),Classifier (UML),Cognitive psychology,Cognition,Computer vision,Psychology,Pattern recognition (psychology),Gerontology,Medicine,Neuroscience",,,"Mingfeng Hao,Mutallip Mamut,Nurbiya Yadikar,Alimjan Aysa,Kurban Ubul",
ICASSP2022,emotional,Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation.,4,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Encoder,Embedding,Computer science,Constraint (computer-aided design),Speech recognition,Encoding (memory),Autoencoder,Cohesion (chemistry),Natural language processing,Artificial intelligence,Deep learning,Mathematics,Chemistry,Geometry,Organic chemistry,Operating system",,,"Dongyang Dai,Yuanzhe Chen,Li Chen,Ming Tu,Lu Liu,Rui Xia,Qiao Tian,Yuping Wang,Yuxuan Wang",
ICASSP2022,"emotion,emotionflow",Emotionflow: Capture the Dialogue Level Emotion Transitions.,27,"Sentiment Analysis and Opinion Mining,Topic Modeling,Speech and dialogue systems","Conversation,Utterance,Computer science,Context (archaeology),Conditional random field,Field (mathematics),Key (lock),Emotion recognition,Natural language processing,Human–computer interaction,Speech recognition,Psychology,Communication,Paleontology,Mathematics,Computer security,Pure mathematics,Biology",,,"Jiancheng Gui,Yikai Li,Kai Chen,Joanna Siebert,Qingcai Chen",
ICASSP2022,emotion,A Novel Sequential Monte Carlo Framework for Predicting Ambiguous Emotion States.,3,"Emotion and Mood Recognition,Anomaly Detection Techniques and Applications,Sentiment Analysis and Opinion Mining","Ambiguity,Computer science,Monte Carlo method,Similarity (geometry),Artificial intelligence,Degree (music),Machine learning,Data mining,Statistics,Mathematics,Physics,Acoustics,Image (mathematics),Programming language",,,"Chanho Park,Rehan Ahmad,Thomas Hain",
ICASSP2022,emotion,Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach.,29,"Music and Audio Processing,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Natural language processing,Emotion recognition,Emotion detection,Artificial intelligence,Speech recognition",,,"Yuval Haitman,Joseph M. Francos,Louis L. Scharf",
ICASSP2024,emotion,SERC-GCN: Speech Emotion Recognition In Conversation Using Graph Convolutional Networks.,1,Emotion and Mood Recognition,"Conversation,Computer science,Speech recognition,Graph,Artificial intelligence,Natural language processing,Emotion recognition,Theoretical computer science,Psychology,Communication",,,"Deeksha Chandola,Enas Altarawneh,Michael Jenkin,Manos Papagelis",
ICASSP2023,emotion,Fast Yet Effective Speech Emotion Recognition with Self-Distillation.,2,"Speech and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Computer science,Speech recognition,Distillation,Emotion recognition,Chemistry,Organic chemistry",,,"Zhao Ren,Thanh Tam Nguyen,Yi Chang,Björn W. Schuller",
ICASSP2023,emotion,Zero-Shot Speech Emotion Recognition Using Generative Learning with Reconstructed Prototypes.,1,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Paralanguage,Computer science,Generative grammar,Zero (linguistics),Shot (pellet),Speech recognition,Artificial intelligence,Emotion recognition,Generative model,Pattern recognition (psychology),Communication,Psychology,Linguistics,Philosophy,Chemistry,Organic chemistry",,,"Xinzhou Xu,Jun Deng,Zixing Zhang,Zhen Yang,Björn W. Schuller",
EMNLP2024,emotion,AlignCap: Aligning Speech Emotion Captioning to Human Preferences,0,"Subtitles and Audiovisual Media,Multimodal Machine Learning Applications,Speech and dialogue systems","Closed captioning,Computer science,Speech recognition,Natural language processing,Linguistics,Psychology,Artificial intelligence,Image (mathematics),Philosophy",,https://aclanthology.org/2024.emnlp-main.224.pdf,"Ziqi Liang,Haoxiang Shi,Hanhui Chen","Speech Emotion Captioning (SEC) has gradually become an active research task. The emotional content conveyed through human speech are often complex, and classifying them into fixed categories may not be enough to fully capture speech emotions. Describing speech emotions through natural language may be a more effective approach. However, existing SEC methods often produce hallucinations and lose generalization on unseen speech. To overcome these problems, we propose AlignCap, which Aligning Speech Emotion Captioning to Human Preferences based on large language model (LLM) with two properties: 1) Speech-Text Alignment, which minimizing the divergence between the LLM’s response prediction distributions for speech and text inputs using knowledge distillation (KD) Regularization. 2) Human Preference Alignment, where we design Preference Optimization (PO) Regularization to eliminate factuality and faithfulness hallucinations. We also extract emotional clues as a prompt for enriching fine-grained information under KD-Regularization. Experiments demonstrate that AlignCap presents stronger performance to other state-of-the-art methods on Zero-shot SEC task."
EMNLP2021,emotion,Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy,8,"Topic Modeling,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Conversation,Utterance,Turn-taking,Inference,Computer science,Feeling,Task (project management),Focus (optics),Benchmark (surveying),Artificial intelligence,Cognitive psychology,Human–computer interaction,Natural language processing,Psychology,Social psychology,Communication,Physics,Management,Optics,Geodesy,Economics,Geography",,https://aclanthology.org/2021.emnlp-main.320.pdf,"Dayu Li,Xiaodan Zhu,Yang Li,Suge Wang,Deyu Li,Jian Liao,Jianxing Zheng","Emotion inference in multi-turn conversations aims to predict the participant’s emotion in the next upcoming turn without knowing the participant’s response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the future feelings of participants, due to the lack of utterance information from the future. Moreover, it is crucial for emotion inference to capture the characteristics of emotional propagation in conversations, such as persistence and contagiousness. In this study, we focus on investigating the task of emotion inference in multi-turn conversations by modeling the propagation of emotional states among participants in the conversation history, and propose an addressee-aware module to automatically learn whether the participant keeps the historical emotional state or is affected by others in the next upcoming turn. In addition, we propose an ensemble strategy to further enhance the model performance. Empirical studies on three different benchmark conversation datasets demonstrate the effectiveness of the proposed model over several strong baselines."
EMNLP2023,emotion,Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers,1,"Mental Health Research Topics,Mental Health via Writing,Digital Mental Health Interventions","Biosocial theory,Valence (chemistry),Mental health,Psychology,Psychopathology,Clinical psychology,Dynamics (music),Emotional valence,Utterance,Psychological intervention,Cognitive psychology,Psychiatry,Social psychology,Computer science,Personality,Artificial intelligence,Cognition,Pedagogy,Physics,Quantum mechanics",,https://aclanthology.org/2023.emnlp-main.188.pdf,"Daniela Teodorescu,Tiffany Cheng,Alona Fyshe,Saif Mohammad","Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time—emotion dynamics—are indicators of one’s mental health. One’s patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one’s everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user’s self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and recovery rates of valence also exhibited significant differences from the control. This work provides important early evidence for how linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses and aid in the understanding, diagnosis, and management of mental health disorders."
EMNLP2024,emotion,Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification,0,Sentiment Analysis and Opinion Mining,"Computer science,Natural language processing,Representation (politics),Artificial intelligence,Information retrieval,Law,Politics,Political science",,https://aclanthology.org/2024.emnlp-main.162.pdf,"Pinyi Zhang,Jingyang Chen,Junchen Shen,Zijie Zhai,Ping Li,Jie Zhang,Kai Zhang","Emotion classification has wide applications in education, robotics, virtual reality, etc. However, identifying subtle differences between fine-grained emotion categories remains challenging. Current methods typically aggregate numerous token embeddings of a sentence into a single vector, which, while being an efficient compressor, may not fully capture complex semantic and temporal distributions. To solve this problem, we propose SEmantic ANchor Graph Neural Networks (SEAN-GNN) for fine-grained emotion classification. It learns a group of representative, multi-faceted semantic anchors in the token embedding space: using these anchors as a global reference, any sentence can be projected onto them to form a “semantic-anchor graph”, with node attributes and edge weights quantifying the semantic and temporal information respectively. The graph structure is well aligned across sentences and, importantly, allows for generating comprehensive emotion representations regarding "
ICLR2024,affect,Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks,0,"Neural Networks and Applications,Advanced Memory and Neural Computing,Domain Adaptation and Few-Shot Learning","Recurrent neural network,Computer science,Forgetting,Task (project management),Sequence learning,Artificial intelligence,Chunking (psychology),Artificial neural network,Stability (learning theory),Machine learning,Cognitive psychology,Psychology,Management,Economics",,https://openreview.net/pdf/34b3ae392ce5f340bdfc472d5624c2093b8f999e.pdf,"Sina Khajehabdollahi,Roxana Zeraati,Emmanouil Giannakakis,Tim Jakob Schäfer,Georg Martius,Anna Levina","Recurrent neural networks (RNNs) in the brain and \emph{in silico} excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale, $\tau_\textrm{\small{net}}$).  However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$, by simultaneously optimizing recurrent weights and $\tau$s. We find that RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and can solve large-$N$ tasks, but suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\tau$ constant and develop longer timescales through recurrent connectivity. We show that the multi-head curriculum increases training speed and stability to perturbations, and allows generalization to tasks beyond the training set. This curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks.  Our results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance."
EMNLP2024,emotional,Be Helpful but Don’t Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support,148,"Management and Organizational Studies,Innovation and Knowledge Management,Management Theory and Practice","Rhetoric,Rhetorical question,Construct (python library),Context (archaeology),Consistency (knowledge bases),Public relations,Business,Empirical research,Sociology,Political science,Epistemology,Computer science,Linguistics,Paleontology,Philosophy,Artificial intelligence,Biology,Programming language",https://github.com/CN-Eyetk/VLESA-ORL.git,https://aclanthology.org/2024.emnlp-main.118.pdf,"Junlin Li,Bo Peng,Yu-Yin Hsu,Chu-Ren Huang","For a conversation to help and support, speakers should maintain an “effect-effort” tradeoff. As outlined in the gist of “Cognitive Relevance Principle”, helpful speakers should optimize the “cognitive relevance” through maximizing the “cognitive effects” and minimizing the “processing effort” imposed on listeners. Although preference learning methods have given rise a boon of studies in pursuit of“effect-optimization”, none have delved into the critical “effort-optimiazation” to fully cultivate the awareness of “optimal relevance” into thecognition of conversation agents. To address this gap, we integrate the “Cognitive Relevance Principle” into emotional support agents in the environment of multi-turn conversation. The results demonstrate a significant and robust improvement against the baseline systems with respect to response quality, human-likedness and supportivenss. This study offers compelling evidence for the effectiveness of the “Relevance Principle” in generating human-like, helpful, and harmless emotional support conversations. The source code will be available at https://github.com/CN-Eyetk/VLESA-ORL.git"
EMNLP2022,affective,Affective Knowledge Enhanced Multiple-Graph Fusion Networks for Aspect-based Sentiment Analysis,12,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Computer science,Sentiment analysis,Leverage (statistics),Dependency (UML),Dependency graph,Graph,Syntax,Artificial intelligence,Natural language processing,Semantics (computer science),Construct (python library),Relation (database),Theoretical computer science,Data mining,Programming language",,https://aclanthology.org/2022.emnlp-main.359.pdf,"Siyu Tang,Heyan Chai,Ziyi Yao,Ye Ding,Cuiyun Gao,Binxing Fang,Qing Liao","Aspect-based sentiment analysis aims to identify sentiment polarity of social media users toward different aspects. Most recent methods adopt the aspect-centric latent tree to connect aspects and their corresponding opinion words, thinking that would facilitate establishing the relationship between aspects and opinion words.However, these methods ignore the roles of syntax dependency relation labels and affective semantic information in determining the sentiment polarity, resulting in the wrong prediction.In this paper, we propose a novel multi-graph fusion network (MGFN) based on latent graph to leverage the richer syntax dependency relation label information and affective semantic information of words.Specifically, we construct a novel syntax-aware latent graph (SaLG) to fully leverage the syntax dependency relation label information to facilitate the learning of sentiment representations. Subsequently, a multi-graph fusion module is proposed to fuse semantic information of surrounding contexts of aspects adaptively. Furthermore, we design an affective refinement strategy to guide the MGFN to capture significant affective clues. Extensive experiments on three datasets demonstrate that our MGFN model outperforms all state-of-the-art methods and verify the effectiveness of our model."
ECCV2022,emotion,Emotion Recognition for Multiple Context Awareness,32,"Emotion and Mood Recognition,Human Pose and Action Recognition,Video Surveillance and Tracking Methods","Computer science,Facial expression,Ambiguity,Artificial intelligence,Context model,Semantics (computer science),Context (archaeology),Spatial contextual awareness,Relevance (law),Context awareness,Human–computer interaction,Machine learning,Natural language processing,Object (grammar),Paleontology,Linguistics,Philosophy,Phone,Biology,Political science,Law,Programming language","https://heco2022.github.io/.""",https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970141.pdf,"Dingkang Yang, Shuai Huang, Shunli Wang, Yang Liu, Peng Zhai, Liuzhen Su, Mingcheng Li, Lihua Zhang","""Understanding emotion in context is a rising hotspot in the computer vision community. Existing methods lack reliable context semantics to mitigate uncertainty in expressing emotions and fail to model multiple context representations complementarily. To alleviate these issues, we present a context-aware emotion recognition framework that combines four complementary contexts. The first context is multimodal emotion recognition based on facial expression, facial landmarks, gesture and gait. Secondly, we adopt the channel and spatial attention modules to obtain the emotion semantics of the scene context. Inspired by sociology theory, we explore the emotion transmission between agents by constructing relationship graphs in the third context. Meanwhile, we propose a novel agent-object context, which aggregates emotion cues from the interactions between surrounding agents and objects in the scene to mitigate the ambiguity of prediction. Finally, we introduce an adaptive relevance fusion module for learning the shared representations among multiple contexts. Extensive experiments show that our approach outperforms the state-of-the-art methods on both EMOTIC and GroupWalk datasets. We also release a dataset annotated with diverse emotion labels, Human Emotion in Context (HECO). In practice, we compare with the existing methods on the HECO, and our approach obtains a higher classification average precision of 50.65% and a lower regression mean error rate of 0.7. The project is available at https://heco2022.github.io/."""
ICASSP2023,emotion,Deep Implicit Distribution Alignment Networks for cross-Corpus Speech Emotion Recognition.,8,"Emotion and Mood Recognition,Speech and Audio Processing,Sentiment Analysis and Opinion Mining","Computer science,Discriminative model,Speech recognition,Artificial intelligence,Regularization (linguistics),Speech corpus,Deep learning,Natural language processing,Emotion recognition,Pattern recognition (psychology),Speech synthesis",,,"Yan Zhao,Jincen Wang,Yuan Zong,Wenming Zheng,Hailun Lian,Li Zhao",
ICASSP2023,"emotion,emotional",Nonparallel Emotional Voice Conversion for Unseen Speaker-Emotion Pairs Using Dual Domain Adversarial Network & Virtual Domain Pairing.,2,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Discriminator,Speech recognition,Domain (mathematical analysis),Dual (grammatical number),Emotion classification,Generator (circuit theory),SIGNAL (programming language),Emotion recognition,Encoder,Adversarial system,Artificial intelligence,Linguistics,Mathematics,Power (physics),Physics,Quantum mechanics,Detector,Programming language,Operating system,Mathematical analysis,Telecommunications,Philosophy",,,"Nirmesh Shah,Mayank Kumar Singh,Naoya Takahashi,Naoyuki Onoe",
ICASSP2023,emotion,Speech Emotion Recognition Based on Low-Level Auto-Extracted Time-Frequency Features.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Discriminative model,Computer science,Feature extraction,Emotion recognition,Weighting,Feature (linguistics),Speech recognition,Artificial intelligence,Time–frequency analysis,Task (project management),Pattern recognition (psychology),Convolutional neural network,Engineering,Computer vision,Philosophy,Systems engineering,Filter (signal processing),Radiology,Medicine,Linguistics",,,"Ke Liu,Jingzhao Hu,Jun Feng",
ICASSP2023,emotion,DST: Deformable Speech Transformer for Emotion Recognition.,9,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Transformer,Speech recognition,Redundancy (engineering),Granularity,Window (computing),Artificial intelligence,Engineering,Voltage,Electrical engineering,Operating system",,,"Weidong Chen,Xiaofen Xing,Xiangmin Xu,Jianxin Pang,Lan Du",
ICASSP2023,emotion,SDTN: Speaker Dynamics Tracking Network for Emotion Recognition in Conversation.,1,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Music and Audio Processing","Conversation,Computer science,Dynamics (music),Speech recognition,Speaker recognition,Emotion recognition,Tracking (education),Artificial intelligence,Psychology,Communication,Pedagogy",,,"Jiawei Chen,Peijie Huang,Guotai Huang,Qianer Li,Yuhong Xu",
ICASSP2023,emotion,Multi-Scale Receptive Field Graph Model for Emotion Recognition in Conversations.,7,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies","Computer science,Graph,Perception,Field (mathematics),Receptive field,Artificial intelligence,Deep learning,Scale (ratio),Focus (optics),Machine learning,Natural language processing,Theoretical computer science,Physics,Mathematics,Quantum mechanics,Neuroscience,Pure mathematics,Biology,Optics",,,"Jie Wei,Guanyu Hu,Luu Anh Tuan,Xinyu Yang,Wenjing Zhu",
ICASSP2023,"emotion,emotional",Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition.,47,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Benchmark (surveying),Representation (politics),Focus (optics),Emotion recognition,Artificial intelligence,Scale (ratio),Code (set theory),Adaptation (eye),Speech recognition,Variation (astronomy),Psychology,Physics,Geodesy,Optics,Set (abstract data type),Quantum mechanics,Neuroscience,Politics,Political science,Astrophysics,Law,Programming language,Geography",,,"Jiaxin Ye,Xin-Cheng Wen,Yujie Wei,Yong Xu,Kunhong Liu,Hongming Shan",
EMNLP2022,emotion,Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction,11,"Sentiment Analysis and Opinion Mining,Topic Modeling,Text and Document Classification Technologies","Computer science,ENCODE,Graph,Feature extraction,Encoding (memory),Artificial intelligence,Construct (python library),Natural language processing,Pattern recognition (psychology),Theoretical computer science,Biochemistry,Chemistry,Gene,Programming language",,https://aclanthology.org/2022.emnlp-main.358.pdf,"Junlong Liu,Xichen Shang,Qianli Ma","Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and corresponding cause clauses, which have recently received growing attention. Previous methods sequentially encode features with a specified order. They first encode the emotion and cause features for clause extraction and then combine them for pair extraction. This lead to an imbalance in inter-task feature interaction where features extracted later have no direct contact with the former. To address this issue, we propose a novel **P**air-**B**ased **J**oint **E**ncoding (**PBJE**) network, which generates pairs and clauses features simultaneously in a joint feature encoding manner to model the causal relationship in clauses. PBJE can balance the information flow among emotion clauses, cause clauses and pairs. From a multi-relational perspective, we construct a heterogeneous undirected graph and apply the Relational Graph Convolutional Network (RGCN) to capture the multiplex relationship between clauses and the relationship between pairs and clauses. Experimental results show that PBJE achieves state-of-the-art performance on the Chinese benchmark corpus."
EMNLP2022,emotion,Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation,50,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Conversation,Computer science,Task (project management),Natural language processing,Emotion recognition,Semantics (computer science),Artificial intelligence,Curriculum,Function (biology),Psychology,Communication,Engineering,Biology,Programming language,Pedagogy,Systems engineering,Evolutionary biology",,https://aclanthology.org/2022.emnlp-main.347.pdf,"Xiaohui Song,Longtao Huang,Hui Xue,Songlin Hu","Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically depending on contexts or speakers. In this paper, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical Network, the SPCL targets at solving the imbalanced classification problem through contrastive learning and does not require a large batch size. Meanwhile, we design a difficulty measure function based on the distance between classes and introduce curriculum learning to alleviate the impact of extreme samples. We achieve state-of-the-art results on three widely used benchmarks. Further, we conduct analytical experiments to demonstrate the effectiveness of our proposed SPCL and curriculum learning strategy."
ICCV2023,"emotion,emoset",EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes,11,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Computer science,Scale (ratio),Artificial intelligence,Visual analytics,Visualization,Human–computer interaction,Cartography,Geography",https://vcc.tech/EmoSet.,https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.pdf,"Jingyuan Yang, Qirui Huang, Tingting Ding, Dani Lischinski, Danny Cohen-Or, Hui Huang","Visual Emotion Analysis (VEA) aims at predicting people's emotional responses to visual stimuli. This is a promising, yet challenging, task in affective computing, which has drawn increasing attention in recent years. Most of the existing work in this area focuses on feature design, while little attention has been paid to dataset construction. In this work, we introduce EmoSet, the first large-scale visual emotion dataset annotated with rich attributes, which is superior to existing datasets in four aspects: scale, annotation richness, diversity, and data balance. EmoSet comprises 3.3 million images in total, with 118,102 of these images carefully labeled by human annotators, making it five times larger than the largest existing dataset. EmoSet includes images from social networks, as well as artistic images, and it is well balanced between different emotion categories. Motivated by psychological studies, in addition to emotion category, each image is also annotated with a set of describable emotion attributes: brightness, colorfulness, scene type, object class, facial expression, and human action, which can help understand visual emotions in a precise and interpretable way. The relevance of these emotion attributes is validated by analyzing the correlations between them and visual emotion, as well as by designing an attribute module to help visual emotion recognition. We believe EmoSet will bring some key insights and encourage further research in visual emotion analysis and understanding. Project page: https://vcc.tech/EmoSet."
EMNLP2022,emotion,Efficient Nearest Neighbor Emotion Classification with BERT-whitening,4,"Sentiment Analysis and Opinion Mining,Topic Modeling,Text and Document Classification Technologies","Computer science,k-nearest neighbors algorithm,Artificial intelligence,Sentence,Nearest neighbor search,Semantics (computer science),Similarity (geometry),Pattern recognition (psychology),Parametric statistics,Natural language processing,Mathematics,Statistics,Image (mathematics),Programming language",,https://aclanthology.org/2022.emnlp-main.312.pdf,"Wenbiao Yin,Lin Shang","Retrieval-based methods have been proven effective in many NLP tasks. Previous methods use representations from the pre-trained model for similarity search directly. However, the sentence representations from the pre-trained model like BERT perform poorly in retrieving semantically similar sentences, resulting in poor performance of the retrieval-based methods. In this paper, we propose kNN-EC, a simple and efficient non-parametric emotion classification (EC) method using nearest neighbor retrieval. We use BERT-whitening to get better sentence semantics, ensuring that nearest neighbor retrieval works. Meanwhile, BERT-whitening can also reduce memory storage of datastore and accelerate retrieval speed, solving the efficiency problem of the previous methods. kNN-EC average improves the pre-trained model by 1.17 F1-macro on two emotion classification datasets."
EMNLP2024,emotion,Integrating Plutchik’s Theory with Mixture of Experts for Enhancing Emotion Classification,99,"Color perception and design,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Artificial intelligence,Image processing,Content analysis,Content (measure theory),Image (mathematics),Computer vision,Pattern recognition (psychology),Mathematics,Mathematical analysis,Social science,Sociology",,https://aclanthology.org/2024.emnlp-main.50.pdf,"Dongjun Lim,Yun-Gyung Cheong","Emotion significantly influences human behavior and decision-making processes. We propose a labeling methodology grounded in Plutchik’s Wheel of Emotions theory for emotion classification. Furthermore, we employ a Mixture of Experts (MoE) architecture to evaluate the efficacy of this labeling approach, by identifying the specific emotions that each expert learns to classify. Experimental results reveal that our methodology improves the performance of emotion classification."
ICASSP2024,emotion,Adaptive Speech Emotion Representation Learning Based On Dynamic Graph.,0,"Advanced Graph Neural Networks,Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies","Computer science,Graph,Sliding window protocol,Artificial intelligence,Computation,External Data Representation,Theoretical computer science,Feature learning,Algorithm,Pattern recognition (psychology),Window (computing),Operating system",,,"Yingxue Gao,Huan Zhao,Zixing Zhang",
ICASSP2023,affective,A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition.,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Emotion recognition,Valence (chemistry),Arousal,Computer science,Regression,Affective computing,Artificial intelligence,Speech recognition,Psychology,Natural language processing,Social psychology,Physics,Quantum mechanics,Psychoanalysis",,,"Jinchao Li,Xixin Wu,Kaitao Song,Dongsheng Li,Xunying Liu,Helen Meng",
ICASSP2023,emotion,Leveraging Label Correlations in a Multi-Label Setting: a Case Study in Emotion.,11,"Sentiment Analysis and Opinion Mining,Topic Modeling,Text and Document Classification Technologies","Computer science,Exploit,Natural language processing,Robustness (evolution),Regularization (linguistics),Artificial intelligence,Pairwise comparison,SemEval,Emotion detection,Language model,Task (project management),Machine learning,Emotion recognition,Management,Biochemistry,Chemistry,Computer security,Economics,Gene",,,"Georgios Chochlakis,Gireesh Mahajan,Sabyasachee Baruah,Keith Burghardt,Kristina Lerman,Shrikanth Narayanan",
ICASSP2023,emotion,Adapting a Self-Supervised Speech Representation for Noisy Speech Emotion Recognition by Using Contrastive Teacher-Student Learning.,4,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Discriminative model,Computer science,Speech recognition,Embedding,Artificial intelligence,Representation (politics),Task (project management),Natural language processing,Pattern recognition (psychology),Machine learning,Management,Politics,Political science,Law,Economics",,,"Seong-Gyun Leem,Daniel Fulford,Jukka-Pekka Onnela,David Gard,Carlos Busso",
ICASSP2023,emotion,Knowledge-Aware Bayesian Co-Attention for Multimodal Emotion Recognition.,7,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,EEG and Brain-Computer Interfaces","Computer science,Emotion recognition,Modalities,Artificial intelligence,Bayesian probability,Machine learning,Fuse (electrical),Affective computing,Bayesian network,Engineering,Social science,Sociology,Electrical engineering",,,"Zihan Zhao,Yu Wang,Yanfeng Wang",
ICASSP2023,emotion,Transferring Quantified Emotion Knowledge for the Detection of Depression in Alzheimer's Disease Using Forestnets.,-1,,,,,"Paula Andrea Pérez-Toro,Dalia Rodríguez-Salas,Tomás Arias-Vergara,Sebastian P. Bayerl,Philipp Klumpp,Korbinian Riedhammer,Maria Schuster,Elmar Nöth,Andreas K. Maier,Juan Rafael Orozco-Arroyave",
ICASSP2023,emotion,Unsupervised Domain Adaptation for Preference Learning Based Speech Emotion Recognition.,5,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Music and Audio Processing","Computer science,Adaptation (eye),Preference,Feature (linguistics),Preference learning,Artificial intelligence,Adversarial system,Domain (mathematical analysis),Representation (politics),Feature learning,Natural language processing,Domain adaptation,Artificial neural network,Function (biology),Speech recognition,Machine learning,Psychology,Linguistics,Mathematics,Mathematical analysis,Statistics,Philosophy,Neuroscience,Evolutionary biology,Politics,Political science,Classifier (UML),Law,Biology",,,"Abinay Reddy Naini,Mary A. Kohler,Carlos Busso",
ICASSP2024,emotion,Functional Emotion Transformer for EEG-Assisted Cross-Modal Emotion Recognition.,2,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces","Emotion recognition,Modal,Electroencephalography,Computer science,Transformer,Speech recognition,Psychology,Voltage,Engineering,Neuroscience,Electrical engineering,Materials science,Polymer chemistry",,,"Wei-Bang Jiang,Ziyi Li,Wei-Long Zheng,Bao-Liang Lu",
ICASSP2024,emotion,CEMOAE: A Dynamic Autoencoder with Masked Channel Modeling for Robust EEG-Based Emotion Recognition.,0,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Autoencoder,Computer science,Speech recognition,Artificial intelligence,Electroencephalography,Feature (linguistics),Channel (broadcasting),Pattern recognition (psychology),Feature extraction,Feature learning,Deep learning,Psychology,Telecommunications,Linguistics,Philosophy,Psychiatry",,,"Yu-Ting Lan,Wei-Bang Jiang,Wei-Long Zheng,Bao-Liang Lu",
ICASSP2024,emotion,EEG Emotion Recognition Based on Dynamical Graph Attention Network.,1,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Electroencephalography,Graph,Emotion recognition,Artificial neural network,Artificial intelligence,Pattern recognition (psychology),Feature (linguistics),Attention network,Machine learning,Speech recognition,Theoretical computer science,Psychology,Linguistics,Philosophy,Psychiatry",,,"Yi Guo,Chao Tang,Hao Wu,Badong Chen",
ICASSP2024,emotion,Multimodal Multi-View Spectral-Spatial-Temporal Masked Autoencoder for Self-Supervised Emotion Recognition.,0,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Gaze Tracking and Assistive Technology","Computer science,Autoencoder,Artificial intelligence,Pattern recognition (psychology),Feature (linguistics),Speech recognition,Process (computing),Feature extraction,Machine learning,Deep learning,Philosophy,Linguistics,Operating system",,,"Pengxuan Gao,Tianyu Liu,Jia-Wen Liu,Bao-Liang Lu,Wei-Long Zheng",
ICASSP2024,emotion,CEDNet: A Continuous Emotion Detection Network for Naturalistic Stimuli Using MEG Signals.,1,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Functional Brain Connectivity Studies","Computer science,Magnetoencephalography,Discriminator,Generalization,Artificial intelligence,Graph,Functional connectivity,Pattern recognition (psychology),Speech recognition,Electroencephalography,Psychology,Neuroscience,Telecommunications,Mathematical analysis,Mathematics,Theoretical computer science,Detector",,,"Zeming He,Gaoyan Zhang",
ICASSP2024,"emotvr,emotion",EmoTVR: A Hybrid Model to Estimate Continuous-Time and Continuous-Level Emotion from Electroencephalography.,1,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,ECG Monitoring and Analysis","Electroencephalography,Computer science,Feature extraction,Emotion recognition,Artificial intelligence,Convolutional neural network,Feature (linguistics),Pattern recognition (psychology),Speech recognition,Emotion classification,Psychology,Linguistics,Philosophy,Psychiatry",,,"Xinxu Zhou,Zhen Liang,Weishan Ye,Junqi Xue,Honghai Liu,Min Zhang,Zhiguo Zhang",
ICASSP2023,emotional,Period VITS: Variational Inference with Explicit Pitch Modeling for End-To-End Emotional Speech Synthesis.,6,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Naturalness,Speech recognition,Prosody,Speech synthesis,Waveform,Cascade,Generator (circuit theory),Jitter,Mean opinion score,End-to-end principle,Voice,Artificial intelligence,Power (physics),Telecommunications,Metric (unit),Radar,Chemistry,Physics,Operations management,Chromatography,Quantum mechanics,Economics",,,"Yuma Shirahata,Ryuichi Yamamoto,Eunwoo Song,Ryo Terashima,Jae-Min Kim,Kentaro Tachibana",
ICCV2023,emotional,EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation,10,"Face recognition and analysis,Speech and Audio Processing,Generative Adversarial Networks and Image Synthesis","Embedding,Expression (computer science),Facial expression,Computer science,Face (sociological concept),Emotional expression,Motion (physics),Speech recognition,Artificial intelligence,Inference,Key (lock),Matching (statistics),Computer vision,Psychology,Mathematics,Cognitive psychology,Social science,Statistics,Computer security,Sociology,Programming language",,https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf,"Shuai Tan, Bin Ji, Ye Pan","Synthesizing expression is essential to create realistic talking faces. Previous works consider expressions and mouth shapes as a whole and predict them solely from audio inputs. However, the limited information contained in audio, such as phonemes and coarse emotion embedding, may not be suitable as the source of elaborate expressions. Besides, since expressions are tightly coupled to lip motions, generating expression from other sources is tricky and always neglects expression performed on mouth region, leading to inconsistency between them. To tackle the issues, this paper proposes Emotional Motion Memory Net (EMMN) that synthesizes expression overall on the talking face via emotion embedding and lip motion instead of the sole audio. Specifically, we extract emotion embedding from audio and design Motion Reconstruction module to decompose ground truth videos into mouth features and expression features before training, where the latter encode all facial factors about expression. During training, the emotion embedding and mouth features are used as keys, and the corresponding expression features are used as values to create key-value pairs stored in the proposed Motion Memory Net. Hence, once the audio-relevant mouth features and emotion embedding are individually predicted from audio at inference time, we treat them as a query to retrieve the best-matching expression features, performing expression overall on the face and thus avoiding inconsistent results. Extensive experiments demonstrate that our method can generate high-quality talking face videos with accurate lip movements and vivid expressions on unseen subjects."
EMNLP2024,affect,A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers,1,Computational and Text Analysis Methods,"Perplexity,Affect (linguistics),Nationality,Computer science,Artificial intelligence,Natural language processing,Linguistics,History,Immigration,Language model,Philosophy,Archaeology",,https://aclanthology.org/2024.emnlp-main.34.pdf,"Valentin Barriere,Sebastian Cifuentes","In this paper, we apply a method to quantify biases associated with named entities from various countries. We create counterfactual examples with small perturbations on target-domain data instead of relying on templates or specific datasets for bias detection. On widely used classifiers for subjectivity analysis, including sentiment, emotion, hate speech, and offensive text using Twitter data, our results demonstrate positive biases related to the language spoken in a country across all classifiers studied. Notably, the presence of certain country names in a sentence can strongly influence predictions, up to a 23% change in hate speech detection and up to a 60% change in the prediction of negative emotions such as anger. We hypothesize that these biases stem from the training data of pre-trained language models (PLMs) and find correlations between affect predictions and PLMs likelihood in English and unknown languages like Basque and Maori, revealing distinct patterns with exacerbate correlations. Further, we followed these correlations in-between counterfactual examples from a same sentence to remove the syntactical component, uncovering interesting results suggesting the impact of the pre-training data was more important for English-speaking-country names."
EMNLP2021,emotion,Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes,46,"Topic Modeling,Natural Language Processing Techniques,Speech and dialogue systems","Utterance,Pragmatics,Empathy,Computer science,Perspective (graphical),Cognition,Leverage (statistics),Generative grammar,Perspective-taking,Artificial intelligence,Natural language processing,Cognitive psychology,Psychology,Cognitive science,Linguistics,Social psychology,Philosophy,Neuroscience",,https://aclanthology.org/2021.emnlp-main.170.pdf,"Hyunwoo Kim,Byeongchang Kim,Gunhee Kim","Empathy is a complex cognitive ability based on the reasoning of others’ affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other’s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level annotations, which can be demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on pragmatics to make dialogue models focus on targeted words in the input during generation. Our method is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation."
ICCV2023,"emotalk,emotional",EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation,34,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Facial Nerve Paralysis Treatment and Research","Computer science,Facial expression,Face (sociological concept),Computer facial animation,Speech recognition,Animation,Encoder,Feature (linguistics),Artificial intelligence,Computer animation,Linguistics,Philosophy,Computer graphics (images),Operating system",https://ziqiaopeng.,https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.pdf,"Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, Zhaoxin Fan","Speech-driven 3D face animation aims to generate realistic facial expressions that match the speech content and emotion. However, existing methods often neglect emotional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emotions in speech so as to generate rich 3D facial expressions. Specifically, we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face with enhanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to generate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate that our approach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watching the supplementary video: https://ziqiaopeng. github.io/emotalk"
ECCV2022,affected,ART-SS: An Adaptive Rejection Technique for Semi-Supervised Restoration for Adverse Weather-Affected Images,4,"Image Enhancement Techniques,Advanced Image Processing Techniques,Image and Signal Denoising Methods","Computer science,Code (set theory),Benchmark (surveying),Artificial intelligence,Adverse weather,Image (mathematics),Convolutional neural network,Pattern recognition (psychology),Labeled data,Domain (mathematical analysis),Machine learning,Mathematics,Physics,Set (abstract data type),Geodesy,Meteorology,Programming language,Geography,Mathematical analysis","https://github.com/rajeevyasarla/ART-SS}{https://github.com/rajeevyasarla/ART-SS}}""",https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780688.pdf,"Rajeev Yasarla, Carey E. Priebe, Vishal M. Patel","""In recent years, convolutional neural network-based single image adverse weather removal methods have achieved significant performance improvements on many benchmark datasets. However, these methods require large amounts of clean-weather degraded image pairs for training, which is often difficult to obtain in practice. Although various weather degradation synthesis methods exist in the literature, the use of synthetically generated weather degraded images often results in sub-optimal performance on the real weatherdegraded images due to the domain gap between synthetic and real world images. To deal with this problem, various semi-supervised restoration (SSR) methods have been proposed for deraining or dehazing which learn to restore clean image using synthetically generated datasets while generalizing better using unlabeled real-world images. The performance of a semi-supervised method is essentially based on the quality of the unlabeled data. In particular, if the unlabeled data characteristics are very different from that of the labeled data, then the performance of a semi-supervised method degrades significantly. We theoretically study the effect of unlabeled data on the performance of an SSR method and develop a technique that rejects the unlabeled images that degrade the performance. Extensive experiments and ablation study show that the proposed sample rejection method increases the performance of existing SSR deraining and dehazing methods significantly. Code is available at :\textit{\href{https://github.com/rajeevyasarla/ART-SS}{https://github.com/rajeevyasarla/ART-SS}}"""
EMNLP2023,affective,How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning,3,"Topic Modeling,Sentiment Analysis and Opinion Mining,Natural Language Processing Techniques","Interpretability,Autoencoder,Conversation,Computer science,Unobservable,Utterance,Artificial intelligence,Natural language processing,Noise (video),Process (computing),Task (project management),Abstraction,Deep learning,Psychology,Programming language,Communication,Philosophy,Epistemology,Management,Economics,Image (mathematics)",,https://aclanthology.org/2023.emnlp-main.33.pdf,"Hang Chen,Xinyu Yang,Jing Luo,Wenjing Zhu","Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of "
EMNLP2022,emotional,Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis,22,"Sentiment Analysis and Opinion Mining,Image Retrieval and Classification Techniques","Computer science,Sentiment analysis,Artificial intelligence,Facial expression,Natural language processing,Face (sociological concept),Modality (human–computer interaction),Machine translation,Key (lock),Matching (statistics),Linguistics,Philosophy,Statistics,Computer security,Mathematics",,https://aclanthology.org/2022.emnlp-main.219.pdf,"Hao Yang,Yanyan Zhao,Bing Qin","Aspect-level multimodal sentiment analysis, which aims to identify the sentiment of the target aspect from multimodal data, recently has attracted extensive attention in the community of multimedia and natural language processing. Despite the recent success in textual aspect-based sentiment analysis, existing models mainly focused on utilizing the object-level semantic information in the image but ignore explicitly using the visual emotional cues, especially the facial emotions. How to distill visual emotional cues and align them with the textual content remains a key challenge to solve the problem. In this work, we introduce a face-sensitive image-to-emotional-text translation (FITE) method, which focuses on capturing visual sentiment cues through facial expressions and selectively matching and fusing with the target aspect in textual modality. To the best of our knowledge, we are the first that explicitly utilize the emotional information from images in the multimodal aspect-based sentiment analysis task. Experiment results show that our method achieves state-of-the-art results on the Twitter-2015 and Twitter-2017 datasets. The improvement demonstrates the superiority of our model in capturing aspect-level sentiment in multimodal data with facial expressions."
ICASSP2024,emotion,Multi-Source Domain Adaptation with Transformer-Based Feature Generation for Subject-Independent EEG-Based Emotion Recognition.,0,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,ECG Monitoring and Analysis","Computer science,Electroencephalography,Pattern recognition (psychology),Artificial intelligence,Transfer of learning,Leverage (statistics),Feature extraction,Transformer,Speech recognition,Domain adaptation,Feature (linguistics),Psychology,Engineering,Voltage,Classifier (UML),Linguistics,Philosophy,Psychiatry,Electrical engineering",,,"Shadi Sartipi,Müjdat Çetin",
ICASSP2023,emotion,MGAT: Multi-Granularity Attention Based Transformers for Multi-Modal Emotion Recognition.,5,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,EEG and Brain-Computer Interfaces","Granularity,Computer science,Modal,Modalities,Modality (human–computer interaction),Computation,Transformer,Artificial intelligence,Algorithm,Engineering,Voltage,Social science,Chemistry,Sociology,Polymer chemistry,Electrical engineering,Operating system",,,"Weiquan Fan,Xiaofen Xing,Bolun Cai,Xiangmin Xu",
ICASSP2024,emotion,An Attention-Enhanced Retentive Broad Learning System for Subject-Generic Emotion Recognition from EEG Signals.,0,"EEG and Brain-Computer Interfaces,Blind Source Separation Techniques,Emotion and Mood Recognition","Computer science,Electroencephalography,Context (archaeology),Artificial intelligence,Speech recognition,Encoder,Subject (documents),Psychology,Neuroscience,Library science,Paleontology,Biology,Operating system",,,"Lang Wang,Peng Jiang,Wensi Duan,Dehua Cao,Baochuan Pang,Juan Liu",
ICASSP2023,emotion,Multilevel Transformer for Multimodal Emotion Recognition.,5,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Transformer,Embedding,Granularity,Utterance,Artificial intelligence,Modalities,Natural language processing,Speech recognition,Machine learning,Engineering,Social science,Voltage,Sociology,Electrical engineering,Operating system",,,"Junyi He,Meimei Wu,Meng Li,Xiaobo Zhu,Feng Ye",
ICASSP2024,emotion,A Supervised Information Enhanced Multi-Granularity Contrastive Learning Framework for EEG Based Emotion Recognition.,0,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Blind Source Separation Techniques","Computer science,Electroencephalography,Artificial intelligence,Robustness (evolution),Granularity,Pattern recognition (psychology),Machine learning,Emotion classification,Speech recognition,Supervised learning,Artificial neural network,Psychology,Psychiatry,Operating system,Biochemistry,Chemistry,Gene",,,"Chenyi Zhou,Hualiang Wang,Xiaomeng Li,Wanlu Liu,Zuozhu Liu",
ICASSP2023,emotion,Achieving Fair Speech Emotion Recognition via Perceptual Fairness.,4,"Emotion and Mood Recognition,Face recognition and analysis,Speech and Audio Processing","Adversarial system,Perception,Fairness measure,Emotion recognition,Computer science,Constraint (computer-aided design),Key (lock),Core (optical fiber),Speech recognition,Work (physics),Artificial intelligence,Natural language processing,Psychology,Computer security,Throughput,Mathematics,Wireless,Telecommunications,Geometry,Neuroscience,Mechanical engineering,Engineering",,,"Woan-Shiuan Chien,Chi-Chun Lee",
ICASSP2023,"emodiff,emotional",Emodiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance.,10,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Intensity (physics),Classifier (UML),Set (abstract data type),Artificial intelligence,Speech synthesis,Quality (philosophy),Process (computing),Philosophy,Physics,Epistemology,Quantum mechanics,Programming language,Operating system",,,"Yiwei Guo,Chenpeng Du,Xie Chen,Kai Yu",
IJCAI2024,emotion,VSGT: Variational Spatial and Gaussian Temporal Graph Models for EEG-based Emotion Recognition,0,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Functional Brain Connectivity Studies","Computer science,Exploit,Artificial intelligence,Gaussian,Pattern recognition (psychology),Electroencephalography,Dependency (UML),Graph,Encoder,Feature (linguistics),Machine learning,Theoretical computer science,Psychology,Linguistics,Philosophy,Physics,Computer security,Quantum mechanics,Psychiatry,Operating system",,https://www.ijcai.org/proceedings/2024/0341.pdf,"Chenyu Liu, Xinliang Zhou, Jiaping Xiao, Zhengri Zhu, Liming Zhai, Ziyu Jia, Yang Liu","Electroencephalogram (EEG), which directly reflects the emotional activity of the brain, has been increasingly utilized for emotion recognition. Most works exploit the spatial and temporal dependencies in EEG to learn emotional feature representations, but they still have two limitations to reach their full potential. First, prior knowledge is rarely used to capture the spatial dependency of brain regions. Second, the cross temporal dependency between consecutive time slices for different brain regions is ignored. To address these limitations, in this paper, we propose Variational Spatial and Gaussian Temporal (VSGT) graph models to investigate the spatial and temporal dependencies for EEG-based emotion recognition. The VSGT has two key components: Variational Spatial Encoder (VSE) and Gaussian Temporal Encoder (GTE). The VSE leverages the upper bound theorem to identify the dynamic spatial dependency based on prior knowledge by the variational Bayesian method. Besides, the GTE exploits the conditional Gaussian graph transform that computes comprehensive temporal dependency between consecutive time slices. Finally, the VSGT utilizes a recurrent structure to calculate the spatial and temporal dependencies for all time slices. Extensive experiments show the superiority of VSGT over state-of-the-art methods on multiple EEG datasets."
IJCAI2022,affect,How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?,6,"Adversarial Robustness in Machine Learning,Anomaly Detection Techniques and Applications,Integrated Circuits and Semiconductor Failure Analysis","Robustness (evolution),Computer science,Frequency domain,Jacobian matrix and determinant,Artificial intelligence,Adversarial system,Low frequency,Machine learning,Control theory (sociology),Pattern recognition (psychology),Computer vision,Mathematics,Telecommunications,Biochemistry,Chemistry,Control (management),Applied mathematics,Gene",,https://www.ijcai.org/proceedings/2022/0093.pdf,"Alvin Chan, Yew Soon Ong, Clement Tan","Model robustness is vital for the reliable deployment of machine learning models in real-world applications. Recent studies have shown that data augmentation can result in model over-relying on features in the low-frequency domain, sacrificing performance against low-frequency corruptions, highlighting a connection between frequency and robustness. Here, we take one step further to more directly study the frequency bias of a model through the lens of its Jacobians and its implication to model robustness. To achieve this, we propose Jacobian frequency regularization for models' Jacobians to have a larger ratio of low-frequency components. Through experiments on four image datasets, we show that biasing classifiers towards low (high)-frequency components can bring performance gain against high (low)-frequency corruption and adversarial perturbation, albeit with a tradeoff in performance for low (high)-frequency corruption. Our approach elucidates a more direct connection between the frequency bias and robustness of deep learning models."
CVPR2024,emotion,A Unified and Interpretable Emotion Representation and Expression Generation,1,"Emotion and Mood Recognition,Generative Adversarial Networks and Image Synthesis,Sentiment Analysis and Opinion Mining","Expression (computer science),Representation (politics),Computer science,Artificial intelligence,Natural language processing,Programming language,Politics,Political science,Law",,https://openaccess.thecvf.com/content/CVPR2024/papers/Paskaleva_A_Unified_and_Interpretable_Emotion_Representation_and_Expression_Generation_CVPR_2024_paper.pdf,"Reni Paskaleva, Mykyta Holubakha, Andela Ilic, Saman Motamed, Luc Van Gool, Danda Paudel",Canonical emotions such as happy sad and fear are easy to understand and annotate. However emotions are often compound e.g. happily surprised and can be mapped to the action units (AUs) used for expressing emotions and trivially to the canonical ones. Intuitively emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities --namely Canonical Compound AUs and AV-- is highly desirable for a better representation and understanding of emotions. However such unification remains to be unknown in the current literature. In this work we propose an interpretable and unified emotion model referred as C2A2. We also develop a method that leverages labels of the non-unified models to annotate the novel unified one. Finally we modify the text-conditional diffusion models to understand continuous numbers which are then used to generate continuous expressions using our unified emotion model. Through quantitative and qualitative experiments we show that our generated images are rich and capture subtle expressions. Our work allows a fine-grained generation of expressions in conjunction with other textual inputs and offers a new label space for emotions at the same time.
ICCV2023,affect,Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition,6,"Emotion and Mood Recognition,Human Pose and Action Recognition,Video Surveillance and Tracking Methods","Computer science,Affect (linguistics),Artificial intelligence,Salient,Cognitive psychology,Psychology,Social psychology,Communication",,https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.pdf,"Hongxia Xie, Ming-Xian Lee, Tzu-Jui Chen, Hung-Jen Chen, Hou-I Liu, Hong-Han Shuai, Wen-Huang Cheng","Group affect refers to the subjective emotion that is evoked by an external stimulus in a group, which is an important factor that shapes group behavior and outcomes. Recognizing group affect involves identifying important individuals and salient objects among a crowd that can evoke emotions. However, most existing methods lack attention to affective meaning in group dynamics and fail to account for the contextual relevance of faces and objects in group-level images. In this work, we propose a solution by incorporating the psychological concept of the Most Important Person (MIP), which represents the most noteworthy face in a crowd and has affective semantic meaning. We present the Dual-branch Cross-Patch Attention Transformer (DCAT) which uses global image and MIP together as inputs. Specifically, we first learn the informative facial regions produced by the MIP and the global context separately. Then, the Cross-Patch Attention module is proposed to fuse the features of MIP and global context together to complement each other. Our proposed method outperforms state-of-the-art methods on GAF 3.0, GroupEmoW, and HECO datasets. Moreover, we demonstrate the potential for broader applications by showing that our proposed model can be transferred to another group affect task, group cohesion, and achieve comparable results."
ECCV2022,emotional,BEAT: A Large-Scale Semantic and Emotional Multi-modal Dataset for Conversational Gestures Synthesis,67,"Human Pose and Action Recognition,Hand Gesture Recognition Systems,Multimodal Machine Learning Applications","Gesture,Computer science,Ground truth,Speech recognition,Natural language processing,Artificial intelligence,Modal,Chemistry,Polymer chemistry","https://pantomatrix.github.io/BEAT/""",https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf,"Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, Bo Zheng","""Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio, text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network (CaMN), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the semantic relevancy, we introduce a metric, Semantic Relevance Gesture Recall (SRGR). Qualitative and quantitative experiments demonstrate metrics’ validness, ground truth data quality, and baseline’s state-of-the-art performance. To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields, including controllable gesture synthesis, cross-modality analysis, and emotional gesture recognition. The data, code and model are available on https://pantomatrix.github.io/BEAT/"""
CVPR2022,"emoca,emotion",EMOCA: Emotion Driven Monocular Face Capture and Animation,96,"Face recognition and analysis,Face and Expression Recognition,Emotion and Mood Recognition","Computer science,Artificial intelligence,Facial expression,Computer vision,Computer facial animation,Face (sociological concept),Expression (computer science),Monocular,Animation,Facial recognition system,Emotional expression,Landmark,Pattern recognition (psychology),Computer animation,Computer graphics (images),Psychology,Cognitive psychology,Social science,Sociology,Programming language",https://emoca.is.tue.mpg.de.,https://openaccess.thecvf.com/content/CVPR2022/papers/Danecek_EMOCA_Emotion_Driven_Monocular_Face_Capture_and_Animation_CVPR_2022_paper.pdf,"Radek Daněček, Michael J. Black, Timo Bolkart","As 3D facial avatars become more widely used for communication, it is critical that they faithfully convey emotion. Unfortunately, the best recent methods that regress parametric 3D face models from monocular images are unable to capture the full spectrum of facial expression, such as subtle or extreme emotions. We find the standard reconstruction metrics used for training (landmark reprojection error, photometric error, and face recognition loss) are insufficient to capture high-fidelity expressions. The result is facial geometries that do not match the emotional content of the input image. We address this with EMOCA (EMOtion Capture and Animation), by introducing a novel deep perceptual emotion consistency loss during training, which helps ensure that the reconstructed 3D expression matches the expression depicted in the input image. While EMOCA achieves 3D reconstruction errors that are on par with the current best methods, it significantly outperforms them in terms of the quality of the reconstructed expression and the perceived emotional content. We also directly regress levels of valence and arousal and classify basic expressions from the estimated 3D face parameters. On the task of in-the-wild emotion recognition, our purely geometric approach is on par with the best image-based methods, highlighting the value of 3D geometry in analyzing human behavior. The model and code are publicly available at https://emoca.is.tue.mpg.de."
EMNLP2022,emotional,Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning,19,"Speech and dialogue systems,Topic Modeling,Language, Metaphor, and Cognition","Conversation,Computer science,Heuristics,Turn-taking,Facial expression,Human–computer interaction,Term (time),Artificial intelligence,Psychology,Communication,Operating system,Physics,Quantum mechanics",,https://aclanthology.org/2022.emnlp-main.195.pdf,"Yi Cheng,Wenge Liu,Wenjie Li,Jiashuo Wang,Ruihui Zhao,Bang Liu,Xiaodan Liang,Yefeng Zheng","Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES conversation systems can provide ES more effectively, but face several new technical challenges, including: (1) how to adopt appropriate support strategies to achieve the long-term dialogue goal of comforting the user’s emotion; (2) how to dynamically model the user’s state. In this paper, we propose a novel system MultiESC to address these issues. For strategy planning, drawing inspiration from the A* search algorithm, we propose lookahead heuristics to estimate the future user feedback after using particular strategies, which helps to select strategies that can lead to the best long-term effects. For user state modeling, MultiESC focuses on capturing users’ subtle emotional expressions and understanding their emotion causes. Extensive experiments show that MultiESC significantly outperforms competitive baselines in both dialogue generation and strategy planning."
EMNLP2022,affective,Affective Idiosyncratic Responses to Music,0,"Neuroscience and Music Perception,Mental Health Research Topics","Psychology,Musical,Cognitive psychology,Key (lock),Phenomenon,Music psychology,Music and emotion,Social psychology,Computer science,Music education,Music history,Art,Pedagogy,Computer security,Visual arts,Physics,Quantum mechanics",,https://aclanthology.org/2022.emnlp-main.80.pdf,"Sky CH-Wang,Evan Li,Oliver Li,Smaranda Muresan,Zhou Yu","Affective responses to music are highly personal. Despite consensus that idiosyncratic factors play a key role in regulating how listeners emotionally respond to music, precisely measuring the marginal effects of these variables has proved challenging. To address this gap, we develop computational methods to measure affective responses to music from over 403M listener comments on a Chinese social music platform. Building on studies from music psychology in systematic and quasi-causal analyses, we test for musical, lyrical, contextual, demographic, and mental health effects that drive listener affective responses. Finally, motivated by the social phenomenon known as 网抑云 (wǎng-yì-yún), we identify influencing factors of platform user self-disclosures, the social support they receive, and notable differences in discloser user activity."
EMNLP2021,affective,Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge,16,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Web Data Mining and Analysis","Computer science,Construct (python library),Sentiment analysis,Aspect-oriented programming,Graph,Benchmark (surveying),Knowledge graph,Context (archaeology),Perspective (graphical),Artificial intelligence,Commonsense knowledge,Natural language processing,Knowledge base,Theoretical computer science,Paleontology,Geodesy,Software,Biology,Programming language,Geography",,https://aclanthology.org/2021.emnlp-main.19.pdf,"Bin Liang,Hang Su,Rongdi Yin,Lin Gui,Min Yang,Qin Zhao,Xiaoqi Yu,Ruifeng Xu","In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the aspects in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the aspects in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the aspect from external affective commonsense knowledge. Then, we employ Beta Distribution to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct graphs for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods."
EMNLP2022,emoji,Curriculum Knowledge Distillation for Emoji-supervised Cross-lingual Sentiment Analysis,1,Educational Technology and Pedagogy,"Computer science,Emoji,Sentiment analysis,Natural language processing,Artificial intelligence,Social media,World Wide Web",,https://aclanthology.org/2022.emnlp-main.55.pdf,"Jianyang Zhang,Tao Liang,Mingyang Wan,Guowu Yang,Fengmao Lv","Existing sentiment analysis models have achieved great advances with the help of sufficient sentiment annotations. Unfortunately, many languages do not have sufficient sentiment corpus. To this end, recent studies have proposed cross-lingual sentiment analysis to transfer sentiment analysis models from resource-rich languages to low-resource languages. However, these studies either rely on external cross-lingual supervision (e.g., parallel corpora and translation model), or are limited by the cross-lingual gaps. In this work, based on the intuitive assumption that the relationships between emojis and sentiments are consistent across different languages, we investigate transferring sentiment knowledge across languages with the help of emojis. To this end, we propose a novel cross-lingual sentiment analysis approach dubbed Curriculum Knowledge Distiller (CKD). The core idea of CKD is to use emojis to bridge the source and target languages. Note that, compared with texts, emojis are more transferable, but cannot reveal the precise sentiment. Thus, we distill multiple Intermediate Sentiment Classifiers (ISC) on source language corpus with emojis to get ISCs with different attention weights of texts. To transfer them into the target language, we distill ISCs into the Target Language Sentiment Classifier (TSC) following the curriculum learning mechanism. In this way, TSC can learn delicate sentiment knowledge, meanwhile, avoid being affected by cross-lingual gaps. Experimental results on five cross-lingual benchmarks clearly verify the effectiveness of our approach."
CVPR2023,"affective,affection",Affection: Learning Affective Explanations for Real-World Visual Data,4,"Multimodal Machine Learning Applications,Video Analysis and Summarization,Sentiment Analysis and Opinion Mining","Computer science,Mood,Categorical variable,Cognitive psychology,Task (project management),Context (archaeology),Artificial intelligence,Psychology,Machine learning,Social psychology,Paleontology,Management,Economics,Biology",https://affective-explanations.org.,https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_Affection_Learning_Affective_Explanations_for_Real-World_Visual_Data_CVPR_2023_paper.pdf,"Panos Achlioptas, Maks Ovsjanikov, Leonidas Guibas, Sergey Tulyakov","In this work, we explore the space of emotional reactions induced by real-world images. For this, we first introduce a large-scale dataset that contains both categorical emotional reactions and free-form textual explanations for 85,007 publicly available images, analyzed by 6,283 annotators who were asked to indicate and explain how and why they felt when observing a particular image, with a total of 526,749 responses. Although emotional reactions are subjective and sensitive to context (personal mood, social status, past experiences) -- we show that there is significant common ground to capture emotional responses with a large support in the subject population. In light of this observation, we ask the following questions: i) Can we develop neural networks that provide plausible affective responses to real-world visual data explained with language? ii) Can we steer such methods towards producing explanations with varying degrees of pragmatic language, justifying different emotional reactions by grounding them in the visual stimulus? Finally, iii) How to evaluate the performance of such methods for this novel task? In this work, we take the first steps in addressing all of these questions, paving the way for more human-centric and emotionally-aware image analysis systems. Our code and data are publicly available at https://affective-explanations.org."
ICASSP2023,emotion,"Multi-View Learning for Speech Emotion Recognition with Categorical Emotion, Categorical Sentiment, and Dimensional Scores.",3,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Categorical variable,Valence (chemistry),Sentiment analysis,Arousal,Emotion recognition,Computer science,Predictive power,Cognitive psychology,Emotion detection,Psychology,Dominance (genetics),Speech recognition,Natural language processing,Artificial intelligence,Machine learning,Social psychology,Philosophy,Physics,Epistemology,Quantum mechanics,Biochemistry,Chemistry,Gene",,,"Teerapat Jenrungrot,Michael Chinen,W. Bastiaan Kleijn,Jan Skoglund,Zalán Borsos,Neil Zeghidour,Marco Tagliasacchi",
ICASSP2023,emotion,Knowledge-Aware Graph Convolutional Network with Utterance-Specific Window Search for Emotion Recognition In Conversations.,2,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Advanced Graph Neural Networks","Utterance,Computer science,Conversation,Graph,Artificial intelligence,Convolutional neural network,Natural language processing,Node (physics),Context (archaeology),Speech recognition,Theoretical computer science,Psychology,Communication,Paleontology,Structural engineering,Engineering,Biology",,,"Anup Singh,Kris Demuynck,Vipul Arora",
ICASSP2023,emotion,EEG Emotion Recognition Via Ensemble Learning Representations.,1,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Electroencephalography,Discriminative model,Computer science,Artificial intelligence,Emotion recognition,Arousal,Focus (optics),Pattern recognition (psychology),Valence (chemistry),Speech recognition,Feature extraction,Emotion classification,Feature (linguistics),Affective computing,Psychology,Neuroscience,Linguistics,Philosophy,Physics,Quantum mechanics,Optics",,,"Nesryne Mejri,Enjie Ghorbel,Djamila Aouada",
ICASSP2023,emotion,Elastic Graph Transformer Networks for EEG-Based Emotion Recognition.,10,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Transformer,Emotion recognition,Graph,Artificial intelligence,Pattern recognition (psychology),Electroencephalography,Speech recognition,Theoretical computer science,Engineering,Psychology,Voltage,Psychiatry,Electrical engineering",,,"Tara N. Sainath,Rohit Prabhavalkar,Diamantino Caseiro,Pat Rondon,Cyril Allauzen",
ICASSP2023,emotion,Towards Learning Emotion Information from Short Segments of Speech.,1,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Speech recognition,Waveform,Mel-frequency cepstrum,Relevance (law),Utterance,Feature (linguistics),Artificial intelligence,Artificial neural network,SIGNAL (programming language),Feature extraction,Pattern recognition (psychology),Telecommunications,Radar,Linguistics,Philosophy,Political science,Law,Programming language",,,"Yingrui Xu,Jingyuan Hu,Jingguo Ge,Yulei Wu,Tong Li,Hui Li",
ICASSP2023,emotion,Two-Phase Prototypical Contrastive Domain Generalization for Cross-Subject EEG-Based Emotion Recognition.,2,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Electroencephalography,Generalization,Artificial intelligence,Pattern recognition (psychology),Support vector machine,Brain–computer interface,Speech recognition,Convolutional neural network,Representation (politics),Block (permutation group theory),Domain adaptation,Domain (mathematical analysis),Residual,Emotion recognition,Property (philosophy),Machine learning,Mathematics,Psychology,Algorithm,Mathematical analysis,Philosophy,Geometry,Politics,Political science,Classifier (UML),Law,Epistemology,Psychiatry",,,"Yoshinao Sato,Narumitsu Ikeda,Hirokazu Takahashi",
CVPR2024,emotional,FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization,0,"Generative Adversarial Networks and Image Synthesis,Face recognition and analysis,Advanced Image Processing Techniques","Quantization (signal processing),Computer science,Face (sociological concept),Flow (mathematics),Quality (philosophy),Speech recognition,Artificial intelligence,Computer vision,Mathematics,Sociology,Geometry,Social science,Philosophy,Epistemology",,https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_FlowVQTalker_High-Quality_Emotional_Talking_Face_Generation_through_Normalizing_Flow_and_CVPR_2024_paper.pdf,"Shuai Tan, Bin Ji, Ye Pan",Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics encompassing expressions blinks poses should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However both aspects are frequently overlooked by existing methods. To this end this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically we develop a flowbased coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution guided by the accompanying audio enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore our designed vector-quantization image generator treats the creation of expressive facial images as a code query task utilizing a learned codebook to provide rich high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.
CVPR2023,emotion,Learning Emotion Representations From Verbal and Nonverbal Communication,7,"Emotion and Mood Recognition,Multimodal Machine Learning Applications,Sentiment Analysis and Opinion Mining","Nonverbal communication,Computer science,Context (archaeology),Cognitive psychology,Emotion classification,Artificial intelligence,Psychology,Communication,Paleontology,Biology",https://github.com/Xeaver/EmotionCLIP.,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf,"Sitao Zhang, Yimu Pan, James Z. Wang","Emotion understanding is an essential but highly challenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present EmotionCLIP, the first pre-training paradigm to extract visual emotion representations from verbal and nonverbal communication using only uncurated data. Compared to numerical labels or descriptions used in previous methods, communication naturally contains emotion information. Furthermore, acquiring emotion representations from communication is more congruent with the human learning process. We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emotion cues using sentiment-guided contrastive learning. Extensive experiments validate the effectiveness and transferability of EmotionCLIP. Using merely linear-probe evaluation protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and rivals many multimodal approaches across various benchmarks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related domains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP."
CVPR2023,emotions,How You Feelin'? Learning Emotions and Mental States in Movie Scenes,7,"Human Pose and Action Recognition,Emotion and Mood Recognition,Video Analysis and Summarization","Dialog box,Computer science,Mental state,Character (mathematics),Set (abstract data type),Emotion recognition,Affective computing,Natural language processing,Artificial intelligence,Cognitive psychology,Psychology,World Wide Web,Geometry,Mathematics,Programming language",,https://openaccess.thecvf.com/content/CVPR2023/papers/Srivastava_How_You_Feelin_Learning_Emotions_and_Mental_States_in_Movie_CVPR_2023_paper.pdf,"Dhruv Srivastava, Aditya Kumar Singh, Makarand Tapaswi","Movie story analysis requires understanding characters' emotions and mental states. Towards this goal, we formulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal Transformer-based architecture that ingests videos, multiple characters, and dialog utterances to make joint predictions. By leveraging annotations from the MovieGraphs dataset, we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted state-of-the-art emotion recognition approaches shows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores reveals that expressive emotions often look at character tokens while other mental states rely on video and dialog cues."
CVPR2024,emotion,Robust Emotion Recognition in Context Debiasing,8,"Emotion and Mood Recognition,Human Pose and Action Recognition,Anomaly Detection Techniques and Applications","Debiasing,Computer science,Context (archaeology),Artificial intelligence,Cognitive psychology,Human–computer interaction,Psychology,Cognitive science,Geology,Paleontology",,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Robust_Emotion_Recognition_in_Context_Debiasing_CVPR_2024_paper.pdf,"Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang",Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation causing severe performance bottlenecks and confounding valuable context priors. In this paper we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes resulting in bias mitigation and robust prediction. As a model-agnostic framework CLEF can be readily integrated into existing methods bringing consistent performance gains.
ICASSP2023,emotion,Designing and Evaluating Speech Emotion Recognition Systems: A Reality Check Case Study with IEMOCAP.,7,"Emotion and Mood Recognition,Music and Audio Processing,Speech Recognition and Synthesis","Computer science,Generalization,Implementation,Motion capture,Set (abstract data type),Emotion recognition,Variety (cybernetics),Focus (optics),Natural language processing,Human–computer interaction,Data science,Artificial intelligence,Motion (physics),Software engineering,Programming language,Mathematical analysis,Physics,Mathematics,Optics",,,"Damir Rakhimov,Martin Haardt",
ICASSP2024,emotion,Attribute-Aware Amplification of Facial Feature Sequences for Facial Emotion Recognition.,0,"Emotion and Mood Recognition,Face recognition and analysis,Face and Expression Recognition","Computer science,Facial expression,Artificial intelligence,Feature (linguistics),Emotion recognition,Face (sociological concept),Population,Speech recognition,Pattern recognition (psychology),Philosophy,Linguistics,Social science,Demography,Sociology",,,"Jiarun Song,Shengnan Wang,Fuzheng Yang",
ICASSP2023,emotion,Multiple Acoustic Features Speech Emotion Recognition Using Cross-Attention Transformer.,8,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Transformer,Spectrogram,Mel-frequency cepstrum,Speech recognition,Feature extraction,Salient,Artificial intelligence,Waveform,Pattern recognition (psychology),Engineering,Voltage,Telecommunications,Radar,Electrical engineering",,,"Naveed Ahmad,Malcolm Egan,Jean-Marie Gorce,Jilles Steeve Dibangoye,Frédéric Le Mouël",
ICASSP2023,emotion,Learning Cross-Modal Audiovisual Representations with Ladder Networks for Emotion Recognition.,4,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Modal,Computer science,Robustness (evolution),Encoder,Feature learning,Discriminative model,Artificial intelligence,Recall,Speech recognition,Representation (politics),Modality (human–computer interaction),Key (lock),Natural language processing,Machine learning,Biochemistry,Chemistry,Linguistics,Philosophy,Computer security,Politics,Political science,Polymer chemistry,Law,Gene,Operating system",,,"Tanuka Bhattacharjee,Yamini Belur,Atchayaram Nalini,Ravi Yadav,Prasanta Kumar Ghosh",
ICASSP2023,emotion,Role of Lexical Boundary Information in Chunk-Level Segmentation for Speech Emotion Recognition.,0,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Segmentation,Speech recognition,Sentence,Natural language processing,Speech segmentation,Boundary (topology),Feature (linguistics),Word (group theory),Artificial intelligence,Text segmentation,Process (computing),Linguistics,Mathematics,Mathematical analysis,Philosophy,Operating system",,,"Jhony H. Giraldo,Sajid Javed,Arif Mahmood,Fragkiskos D. Malliaros,Thierry Bouwmans",
ICASSP2023,emotion,An Empirical Study and Improvement for Speech Emotion Recognition.,4,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Emotion recognition,Fuse (electrical),Speech recognition,Modality (human–computer interaction),Focus (optics),Perspective (graphical),Task (project management),Artificial intelligence,Multimodality,Natural language processing,Human–computer interaction,Management,Optics,World Wide Web,Electrical engineering,Economics,Engineering,Physics",,,"Lixiang Lian,Ben Wang",
ICASSP2023,emotion,Exploring Wav2vec 2.0 Fine Tuning for Improved Speech Emotion Recognition.,163,"Music and Audio Processing,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Valence (chemistry),Concordance correlation coefficient,Transformer,Speech recognition,Robustness (evolution),Artificial intelligence,Natural language processing,Mathematics,Engineering,Biochemistry,Statistics,Chemistry,Quantum mechanics,Voltage,Electrical engineering,Gene,Physics",,,"Eloi Moliner,Jaakko Lehtinen,Vesa Välimäki",
CVPR2021,"affective,emotion,affect2mm",Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality,31,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Humor Studies and Applications","Computer science,Causality (physics),Causation,Representation (politics),Perception,Variety (cybernetics),Artificial intelligence,Action (physics),Content (measure theory),Visualization,Granger causality,Facial expression,Cognitive psychology,Human–computer interaction,Machine learning,Psychology,Mathematics,Mathematical analysis,Physics,Quantum mechanics,Neuroscience,Politics,Political science,Law",,https://openaccess.thecvf.com/content/CVPR2021/papers/Mittal_Affect2MM_Affective_Analysis_of_Multimedia_Content_Using_Emotion_Causality_CVPR_2021_paper.pdf,"Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha","We present Affect2MM, a learning method for time-series emotion prediction for multimedia content. Our goal is to automatically capture the varying emotions depicted by characters in real-life human-centric situations and behaviors. We use the ideas from emotion causation theories to computationally model and determine the emotional state evoked in clips of movies. Affect2MM explicitly models the temporal causality using attention-based methods and Granger causality. We use a variety of components like facial features of actors involved, scene understanding, visual aesthetics, action/situation description, and movie script to obtain an affective-rich representation to understand and perceive the scene. We use an LSTM-based learning model for emotion perception. To evaluate our method, we analyze and compare our performance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE dataset, and observe an average of 10-15% increase in the performance over SOTA methods for all three datasets."
CVPR2024,"emoportraits,emotion",EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars,0,Virtual Reality Applications and Impacts,"Shot (pellet),Head (geology),Human–computer interaction,Computer science,Multimodality,Single shot,Psychology,World Wide Web,Geology,Physics,Optics,Materials science,Geomorphology,Metallurgy",,https://openaccess.thecvf.com/content/CVPR2024/papers/Drobyshev_EMOPortraits_Emotion-enhanced_Multimodal_One-shot_Head_Avatars_CVPR_2024_paper.pdf,"Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic",Head avatars animated by visual signals have gained popularity particularly in cross-driving synthesis where the driver differs from the animated character a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model with a particular focus on its latent space for facial expression descriptors and uncover several limitations with its ability to express intense face motions. Head avatars animated by visual signals have gained popularity particularly in cross-driving synthesis where the driver differs from the animated character a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model with a particular focus on its latent space for facial expression descriptors and uncover several limitations with its ability to express intense face motions. To address these limitations we propose substantial changes in both training pipeline and model architecture to introduce our EMOPortraits model where we: Enhance the model's capability to faithfully support intense asymmetric face expressions setting a new state-of-the-art result in the emotion transfer task surpassing previous methods in both metrics and quality. Incorporate speech-driven mode to our model achieving top-tier performance in audio-driven facial animation making it possible to drive source identity through diverse modalities including visual signal audio or a blend of both.Furthermore we propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions filling the gap with absence of such data in existing datasets.
CVPR2021,emotion,A Circular-Structured Representation for Visual Emotion Distribution Learning,29,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Image Retrieval and Classification Techniques","Emotion classification,Computer science,Representation (politics),Construct (python library),Similarity (geometry),Artificial intelligence,Task (project management),Cognitive psychology,Pattern recognition (psychology),Psychology,Image (mathematics),Management,Politics,Political science,Law,Economics,Programming language",,https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_A_Circular-Structured_Representation_for_Visual_Emotion_Distribution_Learning_CVPR_2021_paper.pdf,"Jingyuan Yang, Jie Li, Leida Li, Xiumei Wang, Xinbo Gao","Visual Emotion Analysis (VEA) has attracted increasing attention recently with the prevalence of sharing images on social networks. Since human emotions are ambiguous and subjective, it is more reasonable to address VEA in a label distribution learning (LDL) paradigm rather than a single-label classification task. Different from other LDL tasks, there exist intrinsic relationships between emotions and unique characteristics within them, as demonstrated in psychological theories. Inspired by this, we propose a well-grounded circular-structured representation to utilize the prior knowledge for visual emotion distribution learning. To be specific, we first construct an Emotion Circle to unify any emotional state within it. On the proposed Emotion Circle, each emotion distribution is represented with an emotion vector, which is defined with three attributes (i.e., emotion polarity, emotion type, emotion intensity) as well as two properties (i.e., similarity, additivity). Besides, we design a novel Progressive Circular (PC) loss to penalize the dissimilarities between predicted emotion vector and labeled one in a coarse-to-fine manner, which further boosts the learning process in an emotion-specific way. Extensive experiments and comparisons are conducted on public visual emotion distribution datasets, and the results demonstrate that the proposed method outperforms the state-of-the-art methods."
NIPS2024,emotion,Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning,0,Emotion and Mood Recognition,"Emotion recognition,Computer science,Cognitive psychology,Human–computer interaction,Multimodal therapy,Emotion detection,Psychology,Natural language processing,Speech recognition,Artificial intelligence,Psychotherapist",,https://papers.nips.cc/paper_files/paper/2024/file/c7f43ada17acc234f568dc66da527418-Paper-Conference.pdf,"Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, Alexander Hauptmann","Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling.However, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset."
NIPS2024,emotional,Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans,-1,,,https://github.com/CUHK-ARISE/EmotionBench.,https://papers.nips.cc/paper_files/paper/2024/file/b0049c3f9c53fb06f674ae66c2cf2376-Paper-Conference.pdf,"Jen-Tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R Lyu","Evaluating Large Language Models’ (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench."
ICASSP2023,emotion,PAGE: A Position-Aware Graph-Based Model for Emotion Cause Entailment in Conversation.,5,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Graph Neural Networks","Conversation,Computer science,Inference,Natural language processing,Graph,Artificial intelligence,ENCODE,Psychology,Theoretical computer science,Communication,Biochemistry,Chemistry,Gene",,,"Farhad Pakdaman,Moncef Gabbouj",
ICASSP2024,emotion,Cubic Knowledge Distillation for Speech Emotion Recognition.,1,"Speech Recognition and Synthesis,Emotion and Mood Recognition","Distillation,Computer science,Speech recognition,Emotion recognition,Natural language processing,Artificial intelligence,Chemistry,Chromatography",,,"Yuxing Zhi,Junhuai Li,Huaijun Wang,Jing Chen,Ting Cao",
ICASSP2024,emotion,Conversation Clique-Based Model for Emotion Recognition In Conversation.,2,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Advanced Text Analysis Techniques","Conversation,Computer science,Utterance,Task (project management),Natural language processing,Hidden Markov model,Artificial intelligence,Speech recognition,Psychology,Communication,Management,Economics",,,"Gehang Zhang,Jiawei Sheng,Shicheng Wang,Tingwen Liu",
ICASSP2023,emotion,Exploiting Modality-Invariant Feature for Robust Multimodal Emotion Recognition with Missing Modalities.,24,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Face and Expression Recognition","Modalities,Modality (human–computer interaction),Computer science,Artificial intelligence,Robustness (evolution),Benchmark (surveying),Pattern recognition (psychology),Feature (linguistics),Missing data,Machine learning,Invariant (physics),Mathematics,Social science,Biochemistry,Chemistry,Linguistics,Philosophy,Geodesy,Sociology,Mathematical physics,Gene,Geography",,,Seungeun Lee,
ICASSP2023,emotion,Phonetic Anchor-Based Transfer Learning to Facilitate Unsupervised Cross-Lingual Speech Emotion Recognition.,1,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Phonetics and Phonology Research","Computer science,Leverage (statistics),Mandarin Chinese,Natural language processing,Artificial intelligence,Speech recognition,Recall,Adaptation (eye),Domain (mathematical analysis),Transfer of learning,Linguistics,Psychology,Neuroscience,Mathematical analysis,Philosophy,Mathematics",,,"Xinfa Zhu,Yi Lei,Kun Song,Yongmao Zhang,Tao Li,Lei Xie",
ICASSP2023,emotion,Constrained Dynamical Neural ODE for Time Series Modelling: A Case Study on Continuous Emotion Prediction.,1,"Time Series Analysis and Forecasting,Stock Market Forecasting Methods,Energy Load and Power Forecasting","Ode,Computer science,Series (stratigraphy),Artificial neural network,Ordinary differential equation,Time series,Recurrent neural network,Convergence (economics),Process (computing),Function (biology),Dynamical systems theory,Dynamical system (definition),Rate of convergence,Artificial intelligence,Machine learning,Differential equation,Applied mathematics,Mathematics,Key (lock),Computer security,Economic growth,Mathematical analysis,Biology,Operating system,Paleontology,Quantum mechanics,Evolutionary biology,Physics,Economics",,,"Jian Yang,Chen Li,Xuelong Li",
ICASSP2023,emotion,Domain Adaptation without Catastrophic Forgetting on a Small-Scale Partially-Labeled Corpus for Speech Emotion Recognition.,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Domain adaptation,Artificial intelligence,Domain (mathematical analysis),Labeled data,Task (project management),Speech recognition,Forgetting,Adversarial system,Natural language processing,Adaptation (eye),Task analysis,Scale (ratio),Training set,Linguistics,Philosophy,Mathematical analysis,Physics,Mathematics,Management,Quantum mechanics,Classifier (UML),Optics,Economics",,,"Zhouyuan Huo,Khe Chai Sim,Bo Li,Dongseong Hwang,Tara N. Sainath,Trevor Strohman",
CVPR2021,affective,The Affective Growth of Computer Vision,9,"Ethics and Social Impacts of AI,Visual Attention and Saliency Detection,Innovative Human-Technology Interaction","Enthusiasm,Feeling,Cynicism,Salient,Normative,Apathy,Psychology,Isolation (microbiology),Cognitive psychology,Computer science,Public relations,Social psychology,Artificial intelligence,Cognition,Political science,Microbiology,Neuroscience,Politics,Law,Biology",,https://openaccess.thecvf.com/content/CVPR2021/papers/Su_The_Affective_Growth_of_Computer_Vision_CVPR_2021_paper.pdf,"Norman Makoto Su, David J. Crandall","The success of deep learning has led to intense growth and interest in computer vision, along with concerns about its potential impact on society. Yet we know little about how these changes have affected the people that research and practice computer vision: we as a community spend so much effort trying to replicate the abilities of humans, but so little time considering the impact of this work on ourselves. In this paper, we report on a study in which we asked computer vision researchers and practitioners to write stories about emotionally-salient events that happened to them. Our analysis of over 50 responses found tremendous affective (emotional) strain in the computer vision community. While many describe excitement and success, we found strikingly frequent feelings of isolation, cynicism, apathy, and exasperation over the state of the field. This is especially true among people who do not share the unbridled enthusiasm for normative standards for computer vision research and who do not see themselves as part of the ""in-crowd."" Our findings suggest that these feelings are closely tied to the kinds of research and professional practices now expected in computer vision. We argue that as a community with significant stature, we need to work towards an inclusive culture that makes transparent and addresses the real emotional toil of its members."
CVPR2024,"emovit,emotion",EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning,0,"Online and Blended Learning,Innovative Teaching and Learning Methods,Educational Games and Gamification","Cognitive psychology,Psychology,Computer science",https://github.com/aimmemotion/EmoVIT.,https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_EmoVIT_Revolutionizing_Emotion_Insights_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf,"Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Wen-Huang Cheng",Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially we identify key visual clues critical to visual emotion recognition. Subsequently we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP our proposed EmoVIT architecture incorporates emotion-specific instruction data leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments our model showcases its proficiency in emotion classification adeptness in affective reasoning and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs providing valuable insights and opening avenues for future exploration in this domain. Our code is available at https://github.com/aimmemotion/EmoVIT.
ICASSP2023,emotion,DWFormer: Dynamic Window Transformer for Speech Emotion Recognition.,2,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Transformer,Window (computing),Speech recognition,Architecture,Artificial intelligence,Engineering,Art,Voltage,Electrical engineering,Visual arts,Operating system",,,Shanxiang Lyu,
ICASSP2023,emotion,A Topic-Enhanced Approach for Emotion Distribution Forecasting in Conversations.,1,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Utterance,Task (project management),Computer science,Construct (python library),Artificial intelligence,Natural language processing,Feature (linguistics),Emotion classification,Machine learning,Multi-task learning,Task analysis,Emotion recognition,Cognitive psychology,Speech recognition,Psychology,Linguistics,Philosophy,Management,Economics,Programming language",,,"Zhaowei Chen,Peng Li,Zeyong Wei,Honghua Chen,Haoran Xie,Mingqiang Wei,Fu Lee Wang",
ICASSP2023,emotion,Recursive Joint Attention for Audio-Visual Fusion in Regression Based Emotion Recognition.,7,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Modalities,Leverage (statistics),Computer science,Artificial intelligence,Audio visual,Speech recognition,Fusion,Machine learning,Pattern recognition (psychology),Multimedia,Social science,Linguistics,Philosophy,Sociology",,,"Zhengyuan Liu,Nancy F. Chen",
ICASSP2023,emotion,Mingling or Misalignment? Temporal Shift for Speech Emotion Recognition with Pre-Trained Representations.,11,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Benchmark (surveying),Feature extraction,Task (project management),Speech recognition,Paradigm shift,Time shifting,Feature (linguistics),Artificial intelligence,Recurrent neural network,Hidden Markov model,Emotion recognition,Code (set theory),Artificial neural network,Engineering,Telecommunications,Philosophy,Linguistics,Geodesy,Systems engineering,Epistemology,Set (abstract data type),Transmission (telecommunications),Programming language,Geography",,,"N. Shashaank,Berker Banar,Mohammad Rasool Izadi,Jeremy Kemmerer,Shuo Zhang,Chuan-Che Jeff Huang",
ICASSP2023,emotion,Textless Speech-to-Music Retrieval Using Emotion Similarity.,1,"Music and Audio Processing,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Similarity (geometry),Speech recognition,Embedding,Regularization (linguistics),Speech corpus,Domain (mathematical analysis),Space (punctuation),Natural language processing,Speech synthesis,Artificial intelligence,Mathematics,Mathematical analysis,Image (mathematics),Operating system",,,"Thibault Maho,Teddy Furon,Erwan Le Merrer",
ICASSP2023,"emotion,emotional",Fine-Grained Emotional Control of Text-to-Speech: Learning to Rank Inter- and Intra-Class Emotion Intensities.,0,"Speech Recognition and Synthesis,Speech and dialogue systems,Topic Modeling","Naturalness,Controllability,Computer science,Class (philosophy),Speech recognition,Speech synthesis,Control (management),Quality (philosophy),Rank (graph theory),Artificial intelligence,Mathematics,Philosophy,Physics,Epistemology,Quantum mechanics,Combinatorics,Applied mathematics",,,"Chan-Shuo Hu,Sung-Wei Tseng,Xin-Yun Fan,Chen-Kuo Chiang",
ICASSP2024,emotional,Speech-Driven Emotional 3d Talking Face Animation Using Emotional Embeddings.,0,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Human Motion and Animation","Naturalness,Animation,Computer facial animation,Computer science,Facial expression,Face (sociological concept),Focus (optics),Motion (physics),Dynamics (music),Natural (archaeology),Emotional expression,Computer animation,Facial motion capture,Human–computer interaction,Speech recognition,Artificial intelligence,Cognitive psychology,Psychology,Facial recognition system,Face detection,Feature extraction,Computer graphics (images),Social science,Pedagogy,Physics,Archaeology,Quantum mechanics,Sociology,Optics,History",,,"Yimo Ren,Jinfa Wang,Jie Liu,Peipei Liu,Hong Li,Hongsong Zhu,Limin Sun",
ICASSP2023,emotion,Exploring Complementary Features in Multi-Modal Speech Emotion Recognition.,10,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Modality (human–computer interaction),Exploit,Artificial intelligence,Speech recognition,Natural language processing,Machine learning,Computer security",,,"Minjun Zhu,Yixuan Weng,Shizhu He,Cunguang Wang,Kang Liu,Li Cai,Jun Zhao",
ICASSP2024,emotion,Fusing Modality-Specific Representations and Decisions for Multimodal Emotion Recognition.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Human Pose and Action Recognition","Modality (human–computer interaction),Computer science,Modalities,Task (project management),Benchmark (surveying),Artificial intelligence,Machine learning,Convolutional neural network,Natural language processing,Engineering,Social science,Systems engineering,Geodesy,Sociology,Geography",,,"Fan Yu,Haoxu Wang,Ziyang Ma,Shiliang Zhang",
ICASSP2023,emotion,Empathetic Response Generation via Emotion Cause Transition Graph.,8,"Topic Modeling,Natural Language Processing Techniques,Sentiment Analysis and Opinion Mining","Empathy,Transition (genetics),Perception,Affective science,Emotion perception,Computer science,Emotion classification,Cognitive psychology,Emotion work,Cognition,Graph,Context (archaeology),Emotion recognition,Two-factor theory of emotion,Focus (optics),Psychology,Artificial intelligence,Social psychology,Theoretical computer science,Paleontology,Biochemistry,Chemistry,Physics,Optics,Gene,Neuroscience,Biology",,,"Mingjie Tian,Fausto Giunchiglia,Rui Song,Xing Chen,Hao Xu",
ICASSP2024,emotion,AttA-NET: Attention Aggregation Network for Audio-Visual Emotion Recognition.,3,"Speech and Audio Processing,Music and Audio Processing,Emotion and Mood Recognition","Computer science,Modal,Audio visual,Leverage (statistics),Atta,Discriminative model,Artificial intelligence,Modalities,Classifier (UML),Net (polyhedron),Speech recognition,Natural language processing,Machine learning,Multimedia,Hymenoptera,Botany,Biology,Social science,Chemistry,Geometry,Mathematics,Sociology,Polymer chemistry",,,"Lingwei Wei,Dou Hu,Wei Zhou,Songlin Hu",
ICASSP2024,emotion,Emotion-Aligned Contrastive Learning Between Images and Music.,0,"Music and Audio Processing,Diverse Musicological Studies,Speech and Audio Processing","Computer science,Embedding,Pop music automation,Focus (optics),Metadata,Natural language processing,Artificial intelligence,Space (punctuation),Information retrieval,Speech recognition,World Wide Web,Musical composition,Music education,Psychology,Pedagogy,Physics,Optics,Operating system",,,"He Wang,Pengcheng Guo,Pan Zhou,Lei Xie",
ICASSP2024,emotion,MMRBN: Rule-Based Network for Multimodal Emotion Recognition.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Color perception and design","Computer science,Emotion recognition,Modalities,Modality (human–computer interaction),Representation (politics),Affective computing,Artificial intelligence,Expression (computer science),Natural language processing,Rule-based system,Speech recognition,Social science,Sociology,Politics,Political science,Law,Programming language",,,"Baogui Xu,Yafei Lu,Bing Su,Xiaoran Yan",
ICASSP2024,affective,Sec2Sec Co-Attention Transformer for Video-Based Apparent Affective Prediction.,0,Emotion and Mood Recognition,"Computer science,Transformer,Electrical engineering,Engineering,Voltage",,,"Sabyasachee Baruah,Shrikanth Narayanan",
ICASSP2024,"emotalker,emotionally",EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model.,1,"Face recognition and analysis,Generative Adversarial Networks and Image Synthesis,Human Motion and Animation","Computer science,Human–computer interaction,Facial expression,Generalization,Animation,Face (sociological concept),Inference,Artificial intelligence,Process (computing),Quality (philosophy),Identity (music),Field (mathematics),Social science,Physics,Sociology,Mathematical analysis,Philosophy,Computer graphics (images),Mathematics,Epistemology,Acoustics,Pure mathematics,Operating system",,,"Jincheng Wu,Ruixu Geng,Yadong Li,Dongheng Zhang,Zhi Lu,Yang Hu,Yan Chen",
ICASSP2024,emotion,"Conversational Co-Speech Gesture Generation via Modeling Dialog Intention, Emotion, and Context with Diffusion Models.",0,Speech and dialogue systems,"Dialog box,Gesture,Computer science,Context (archaeology),Dialog system,Speech recognition,Context model,Natural language processing,Diffusion,Human–computer interaction,Artificial intelligence,World Wide Web,History,Physics,Archaeology,Object (grammar),Thermodynamics",,,"Guolong Wang,Yike Tan,Hangyu Lin,Chuchun Zhang",
ICASSP2024,emotion,Inter-Modality and Intra-Sample Alignment for Multi-Modal Emotion Recognition.,2,Emotion and Mood Recognition,"Modality (human–computer interaction),Modal,Sample (material),Computer science,Emotion recognition,Artificial intelligence,Speech recognition,Pattern recognition (psychology),Computer vision,Materials science,Chemistry,Chromatography,Polymer chemistry",,,"Lingling Li,Weicong Li,Qiyuan Ding,Chengpei Tang,Keze Wang",
ICASSP2023,emotion,Privacy-Enhanced Federated Learning Against Attribute Inference Attack for Speech Emotion Recognition.,6,"Privacy-Preserving Technologies in Data,Internet Traffic Analysis and Secure E-voting,Face recognition and analysis","Computer science,Inference,Focus (optics),Feature (linguistics),Salient,Upload,Filter (signal processing),Server,Artificial intelligence,Feature learning,Machine learning,Artificial neural network,World Wide Web,Linguistics,Philosophy,Physics,Optics,Computer vision",,,"Mateusz Guzik,Konrad Kowalczyk",
ICASSP2023,emotion,A Generalized Subspace Distribution Adaptation Framework for Cross-Corpus Speech Emotion Recognition.,4,"Emotion and Mood Recognition,Speech and Audio Processing,Sentiment Analysis and Opinion Mining","Subspace topology,Computer science,Metric (unit),Transfer of learning,Divergence (linguistics),Artificial intelligence,Adaptation (eye),Feature (linguistics),Process (computing),Similarity (geometry),Speech recognition,Natural language processing,Pattern recognition (psychology),Machine learning,Image (mathematics),Linguistics,Operations management,Philosophy,Physics,Optics,Economics,Operating system",,,"Chenda Li,Yao Qian,Zhuo Chen,Dongmei Wang,Takuya Yoshioka,Shujie Liu,Yanmin Qian,Michael Zeng",
ICASSP2024,emotion,Speaker-Centric Multimodal Fusion Networks for Emotion Recognition in Conversations.,2,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining","Computer science,Utterance,Speech recognition,Focus (optics),Speaker recognition,Speaker diarisation,Feature (linguistics),Pooling,Emotion recognition,Graph,Feature extraction,Modal,Fusion,Artificial intelligence,Natural language processing,Linguistics,Philosophy,Physics,Chemistry,Theoretical computer science,Polymer chemistry,Optics",,,"Yanbin Zou,Liehu Wu,Yimao Sun",
ICASSP2023,affective,Evaluating Variants of wav2vec 2.0 on Affective Vocal Burst Tasks.,-1,,,,,"Hui Tang,Yao Lu,Qi Xuan",
CVPR2021,emotion,iMiGUE: An Identity-Free Video Dataset for Micro-Gesture Understanding and Emotion Analysis,60,"Human Pose and Action Recognition,Hand Gesture Recognition Systems,Multimodal Machine Learning Applications","Gesture,Computer science,Identity (music),Gesture recognition,Face (sociological concept),Artificial intelligence,Human–computer interaction,Natural language processing,Speech recognition,Linguistics,Physics,Philosophy,Acoustics",,https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_iMiGUE_An_Identity-Free_Video_Dataset_for_Micro-Gesture_Understanding_and_Emotion_CVPR_2021_paper.pdf,"Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, Guoying Zhao","We introduce a new dataset for the emotional artificial intelligence research: identity-free video dataset for micro-gesture understanding and emotion analysis (iMiGUE). Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity information, while the predominant researches of emotion analysis concern sensitive biometric data, like face and speech. Most importantly, iMiGUE focuses on micro-gestures, i,e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illustrative purposes. Furthermore, iMiGUE is designed to evaluate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separately (or isolatedly). This is because the real need for emotion AI is to understand the emotional states behind gestures in a holistic way. Moreover, to counter for the challenge of imbalanced samples distribution of this dataset, an unsupervised learning method is proposed to capture latent representations from the micro-gesture sequences themselves. We systematically investigate representative methods on this dataset, and comprehensive experimental results reveal several interesting insights from the iMiGUE, e,g., micro-gesture-based analysis can promote emotion understanding. We confirm that the new iMiGUE dataset could advance studies of micro-gesture and emotion AI."
ICASSP2023,emotion,Multimodal Emotion Recognition Based on Deep Temporal Features Using Cross-Modal Transformer and Self-Attention.,4,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Artificial intelligence,Deep learning,Speech recognition,Modal,Recurrent neural network,Transformer,Embedding,Convolutional neural network,Emotion recognition,Modalities,Natural language processing,Artificial neural network,Pattern recognition (psychology),Social science,Chemistry,Physics,Quantum mechanics,Voltage,Sociology,Polymer chemistry",,,"Rohith Aralikatti,Christoph Böddeker,Gordon Wichern,Aswin Shanmugam Subramanian,Jonathan Le Roux",
ICASSP2023,emotion,EMIX: A Data Augmentation Method for Speech Emotion Recognition.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Overfitting,Transformer,Emotion recognition,Task (project management),Artificial intelligence,Speech recognition,Machine learning,Constructive,Artificial neural network,Physics,Management,Process (computing),Quantum mechanics,Voltage,Economics,Operating system",,,"Minh Vu,Yuki Akiyama,Konstantinos Slavakis",
ICASSP2023,emotion,A Spatial-Temporal ECG Emotion Recognition Model Based on Dynamic Feature Fusion.,1,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,ECG Monitoring and Analysis","Emotion recognition,Computer science,Artificial intelligence,Feature (linguistics),Pattern recognition (psychology),Fusion,Feature extraction,Arousal,Valence (chemistry),Speech recognition,Psychology,Philosophy,Linguistics,Neuroscience,Physics,Quantum mechanics",,,"Ramon Sanabria,Nikolay Bogoychev,Nina Markl,Andrea Carmantini,Ondrej Klejch,Peter Bell",
ICASSP2023,emotion,Robust multi-modal speech emotion recognition with ASR error adaptation.,4,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Robustness (evolution),Speech recognition,Modal,Word error rate,Emotion recognition,Artificial intelligence,Natural language processing,Pattern recognition (psychology),Biochemistry,Chemistry,Polymer chemistry,Gene",,,"Kangkang Lu,Manh Cuong Nguyen,Xun Xu,Chuan Sheng Foo",
ICASSP2023,affect,Acoustically-Driven Phoneme Removal that Preserves Vocal Affect Cues.,0,"Speech and Audio Processing,Voice and Speech Disorders,Music and Audio Processing","Paralanguage,Vocal tract,Speech recognition,Computer science,Affect (linguistics),Perception,Filter (signal processing),Articulation (sociology),Filter bank,SIGNAL (programming language),Speech processing,Speech perception,Acoustics,Psychology,Communication,Computer vision,Neuroscience,Politics,Political science,Law,Programming language,Physics",,,"Wenting Li,Jiahong Yang,Haibo Cheng,Ping Wang,Kaitai Liang",
ICASSP2024,emotion,GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Accurate Speech Emotion Recognition.,4,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Speech recognition,Modality (human–computer interaction),Artificial intelligence,Task (project management),Natural language processing,Management,Economics",,,"Otavio Braga,Wei Xia,Keith Johnson,Alice Chuang,Yunfan Ye,Olivier Siohan,Tuan Anh Nguyen",
ICASSP2024,emotion,Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer.,0,Speech and Audio Processing,"Speech recognition,Style (visual arts),Computer science,Emotion recognition,Natural language processing,Artificial intelligence,History,Archaeology",,,"Takashi Shibuya,Yuhta Takida,Yuki Mitsufuji",
ICASSP2024,emotion,Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition.,6,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining","Computer science,Speech recognition,Utterance,Emotion classification,Emotion recognition,Artificial neural network,Leverage (statistics),Artificial intelligence,Natural language processing",,,"Qing-Tian Xu,Jie Zhang,Zhen-Hua Ling",
CVPR2021,emotion,Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences,99,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Modalities,Modality (human–computer interaction),Computer science,Multimodal learning,Artificial intelligence,Human–computer interaction,Sociology,Social science",,https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.pdf,"Fengmao Lv, Xiang Chen, Yanyong Huang, Lixin Duan, Guosheng Lin","Human multimodal emotion recognition involves time-series data of different modalities, such as natural language, visual motions, and acoustic behaviors. Due to the variable sampling rates for sequences from different modalities, the collected multimodal streams are usually unaligned. The asynchrony across modalities increases the difficulty on conducting efficient multimodal fusion. Hence, this work mainly focuses on multimodal fusion from unaligned multimodal sequences. To this end, we propose the Progressive Modality Reinforcement (PMR) approach based on the recent advances of crossmodal transformer. Our approach introduces a message hub to exchange information with each modality. The message hub sends common messages to each modality and reinforces their features via crossmodal attention. In turn, it also collects the reinforced features from each modality and uses them to generate a reinforced common message. By repeating the cycle process, the common message and the modalities' features can progressively complement each other. Finally, the reinforced features are used to make predictions for human emotion. Comprehensive experiments on different human multimodal emotion recognition benchmarks clearly demonstrate the superiority of our approach."
CVPR2021,emotional,Audio-Driven Emotional Video Portraits,139,"Face recognition and analysis,Speech and Audio Processing,Generative Adversarial Networks and Image Synthesis","Portrait,Computer science,Face (sociological concept),Artificial intelligence,Focus (optics),Computer vision,Facial expression,Dynamics (music),Space (punctuation),Duration (music),Speech recognition,Psychology,Art,Optics,Art history,Operating system,Literature,Social science,Pedagogy,Physics,Sociology",,https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Audio-Driven_Emotional_Video_Portraits_CVPR_2021_paper.pdf,"Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, Feng Xu","Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the correlation between speech content and the mouth shape. Facial emotion, which is one of the most important features on natural human faces, is always neglected in their methods. In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Specifically, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, i.e., a duration-independent emotion space and a duration dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced. Then we propose the Target-Adaptive Face Synthesis technique to generate the final high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualitatively and quantitatively."
ICASSP2024,emotion,Multi-Source Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition.,0,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Computer science,Domain (mathematical analysis),Benchmark (surveying),Transfer of learning,Artificial intelligence,Principal component analysis,Field (mathematics),Pattern recognition (psychology),SIGNAL (programming language),Speech recognition,Machine learning,Mathematics,Mathematical analysis,Geodesy,Pure mathematics,Programming language,Geography",,,"Chengwen Zhang,Yuhao Zhang,Bo Cheng",
ICASSP2024,"emotion,emo",RL-EMO: A Reinforcement Learning Framework for Multimodal Emotion Recognition.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Reinforcement learning,Computer science,Artificial intelligence,Context (archaeology),Machine learning,Task (project management),Emotion recognition,Modal,Paleontology,Chemistry,Management,Polymer chemistry,Economics,Biology",,,"Jian Huang,Yancheng Bai,Yang Cai,Wei Bian",
ICASSP2023,emotion,Speech-Based Emotion Recognition with Self-Supervised Models Using Attentive Channel-Wise Correlations and Label Smoothing.,10,"Emotion and Mood Recognition,Speech and Audio Processing,Infant Health and Development","Smoothing,Speech recognition,Computer science,Artificial intelligence,Emotion recognition,Pattern recognition (psychology),Channel (broadcasting),Computer vision,Computer network",,,"Cristian J. Vaca-Rubio,Pu Wang,Toshiaki Koike-Akino,Ye Wang,Petros Boufounos,Petar Popovski",
ICASSP2024,emotion,Zero Shot Audio To Audio Emotion Transfer With Speaker Disentanglement.,0,Speech and Audio Processing,"Computer science,Shot (pellet),Zero (linguistics),Speech recognition,Transfer (computing),Audio analyzer,Audio signal,Digital audio,Speech coding,Linguistics,Philosophy,Chemistry,Organic chemistry,Parallel computing",,,"Hendrik Laux,Emil Mededovic,Ahmed Hallawa,Lukas Martin,Arne Peine,Anke Schmeink",
ICASSP2023,affective,Parasympathetic-Sympathetic Causal Interactions and Perceived Workload for Varying Difficulty Affective Computing Tasks.,2,"Emotion and Mood Recognition,Neural and Behavioral Psychology Studies,EEG and Brain-Computer Interfaces","Workload,Computer science,Cognitive psychology,Human–computer interaction,Psychology,Operating system",,,"Aki Härmä,Ulf Großekathöfer,Okke Ouweltjes,Venkata Srikanth Nallanthighal",
ICASSP2023,emotion,Learning Robust Self-Attention Features for Speech Emotion Recognition with Label-Adaptive Mixup.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Artificial intelligence,Scheme (mathematics),Machine learning,Pattern recognition (psychology),Mathematical analysis,Mathematics",,,"Anuj Diwan,Ching-Feng Yeh,Wei-Ning Hsu,Paden Tomasello,Eunsol Choi,David Harwath,Abdelrahman Mohamed",
ICASSP2024,emotion,Self-Supervised Domain Exploration with an Optimal Transport Regularization for Open Set Cross-Domain Speech Emotion Recognition.,0,"Speech Recognition and Synthesis,Speech and Audio Processing","Regularization (linguistics),Computer science,Speech recognition,Domain (mathematical analysis),Emotion recognition,Open set,Artificial intelligence,Set (abstract data type),Natural language processing,Mathematics,Mathematical analysis,Discrete mathematics,Programming language",,,"Xinmeng Xu,Chang Han,Yiqun Zhang,Weiping Tu,Yuhong Yang",
ICASSP2024,emotion,Multi-Modal Emotion Recognition Using Multiple Acoustic Features and Dual Cross-Modal Transformer.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Modal,Transformer,Feature extraction,Speech recognition,Paralanguage,Artificial intelligence,Pattern recognition (psychology),Engineering,Voltage,Polymer chemistry,Electrical engineering,Chemistry,Communication,Sociology",,,"Hongcheng Liu,Zhe Chen,Hui Li,Pingjie Wang,Yanfeng Wang,Yu Wang",
ICASSP2023,emotional,Masking Speech Contents by Random Splicing: is Emotional Expression Preserved?,0,"Music and Audio Processing,Speech and Audio Processing,Speech Recognition and Synthesis","Speech recognition,Computer science,German,Perception,RNA splicing,Emotional expression,Natural language processing,Valence (chemistry),Arousal,Artificial intelligence,Psychology,Cognitive psychology,Linguistics,Social psychology,Biology,Philosophy,Physics,Quantum mechanics,Neuroscience,RNA,Biochemistry,Gene",,,"Tae-Jin Woo,Woo-Jeoung Nam,Yeong-Joon Ju,Seong-Whan Lee",
ICASSP2024,emotion,Hierarchical Emotion Prediction and Control in Text-to-Speech Synthesis.,1,"Speech and Audio Processing,Speech Recognition and Synthesis,Music and Audio Processing","Prosody,Utterance,Computer science,Speech synthesis,Speech recognition,Speech corpus,Natural language processing,Inference,Rendering (computer graphics),Artificial intelligence,Construct (python library),Programming language",,,"Fan Yu,Haoxu Wang,Xian Shi,Shiliang Zhang",
ICASSP2024,emotion,Learning Emotion-Invariant Speaker Representations for Speaker Verification.,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Robustness (evolution),Speaker recognition,Spectrogram,Speaker diarisation,Correlation,Invariant (physics),Encoder,Artificial intelligence,Cosine similarity,Emotion recognition,Pattern recognition (psychology),Mathematics,Mathematical physics,Biochemistry,Chemistry,Geometry,Gene,Operating system",,,"Mingxiu Cai,Daling Wang,Shi Feng,Yifei Zhang",
ICASSP2024,emotional,PECER: Empathetic Response Generation Via Dynamic Personality Extraction and Contextual Emotional Reasoning.,0,Social Robot Interaction and HRI,"Personality,Computer science,Extraction (chemistry),Psychology,Cognitive psychology,Social psychology,Chromatography,Chemistry",,,"Daijun Ding,Rong Chen,Liwen Jing,Bowen Zhang,Xu Huang,Li Dong,Xiaowen Zhao,Ge Song",
ICASSP2024,emotion,Towards Improving Speech Emotion Recognition Using Synthetic Data Augmentation from Emotion Conversion.,2,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Emotion recognition,Speech recognition,Training set,Speech synthesis,Emotion classification,Artificial intelligence,Natural language processing",,,"Yinru He,Guihua Wen,Pei Yang,Dongliang Chen",
ICASSP2024,emotion,Speech Relationship Learning for Cross-Corpus Speech Emotion Recognition.,1,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and Audio Processing","Computer science,Discriminator,Speech recognition,Artificial intelligence,Classifier (UML),Embedding,Smoothing,Emotion recognition,Natural language processing,Similarity (geometry),Telecommunications,Detector,Image (mathematics),Computer vision",,,"Varsha Suresh,Salah Aït-Mokhtar,Caroline Brun,Ioan Calapodescu",
ICASSP2024,"emoticons,emored",EmoRED: A Dataset for Relation Extraction in Texts with Emoticons.,0,"Natural Language Processing Techniques,Authorship Attribution and Profiling,Digital Communication and Language","Computer science,Relation (database),Relationship extraction,Task (project management),Natural language processing,Identification (biology),Comprehension,Artificial intelligence,Information extraction,Focus (optics),Natural language,Data mining,Botany,Physics,Management,Biology,Optics,Economics,Programming language",,,"Yihao Wang,Zhongdi Wu,Joseph Nese,Akihito Kamata,Vedant Nilabh,Eric C. Larson",
ICASSP2023,emotion,Knowledge Transfer for on-Device Speech Emotion Recognition With Neural Structured Learning.,3,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Embedding,Transfer of learning,Enhanced Data Rates for GSM Evolution,Constraint (computer-aided design),Artificial neural network,Artificial intelligence,Speech recognition,Deep learning,Edge device,Natural language processing,Machine learning,Mechanical engineering,Cloud computing,Engineering,Operating system",,,"Dianwen Ng,Ruixi Zhang,Jia Qi Yip,Zhao Yang,Jinjie Ni,Chong Zhang,Yukun Ma,Chongjia Ni,Eng Siong Chng,Bin Ma",
ICASSP2024,"emohrnet,emotion",Emohrnet: High-Resolution Neural Network Based Speech Emotion Recognition.,0,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Spectrogram,Computer science,Benchmark (surveying),Speech recognition,Emotion recognition,Artificial neural network,Domain (mathematical analysis),Adaptation (eye),High resolution,Architecture,Artificial intelligence,Resolution (logic),Art,Mathematical analysis,Physics,Remote sensing,Mathematics,Geodesy,Geology,Optics,Visual arts,Geography",,,"Shuai Wang,Qibing Bai,Qi Liu,Jianwei Yu,Zhengyang Chen,Bing Han,Yanmin Qian,Haizhou Li",
ICASSP2023,emotion,Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents.,1,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Naturalness,Speech recognition,Identity (music),Style (visual arts),Speaker recognition,Reading (process),Natural language processing,Linguistics,Philosophy,Physics,Archaeology,Quantum mechanics,Acoustics,History",,,"Tong Xia,Jing Han,Abhirup Ghosh,Cecilia Mascolo",
ICASSP2023,emotion,Using Auxiliary Tasks In Multimodal Fusion of Wav2vec 2.0 And Bert for Multimodal Emotion Recognition.,4,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Modal,Fuse (electrical),Modalities,Feature (linguistics),Emotion recognition,Residual,Modality (human–computer interaction),Artificial intelligence,Construct (python library),Affective computing,Speech recognition,Machine learning,Pattern recognition (psychology),Engineering,Algorithm,Social science,Linguistics,Chemistry,Philosophy,Sociology,Polymer chemistry,Electrical engineering,Programming language",,,"Yu-Jhe Li,Matthew O'Toole,Kris Kitani",
ICASSP2024,emotion,Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Overfitting,Computer science,Adaptation (eye),Domain adaptation,Artificial intelligence,Emotion recognition,Machine learning,Domain (mathematical analysis),Artificial neural network,Mathematical analysis,Mathematics,Classifier (UML),Physics,Optics",,,"Jiaming Zhou,Shiwan Zhao,Yaqi Liu,Wenjia Zeng,Yong Chen,Yong Qin",
ICASSP2024,"emotion,emotional",Large Language Model-Based Emotional Speech Annotation Using Context and Acoustic Feature for Speech Emotion Recognition.,2,"Music and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Annotation,Computer science,Natural language processing,Emotion recognition,Speech recognition,Conversation,Artificial intelligence,Context (archaeology),Transcription (linguistics),Feature (linguistics),Linguistics,Paleontology,Philosophy,Biology",,,"Shaoshi Ling,Yuxuan Hu,Shuangbei Qian,Guoli Ye,Yao Qian,Yifan Gong,Ed Lin,Michael Zeng",
CVPR2021,"affective,emotion",Affective Processes: Stochastic Modelling of Temporal Context for Emotion and Facial Expression Recognition,26,"Emotion and Mood Recognition,Human Pose and Action Recognition,Mental Health Research Topics","Computer science,Facial expression,Artificial intelligence,Hidden Markov model,Context model,Context (archaeology),Probabilistic logic,Machine learning,Task (project management),Feature (linguistics),Key (lock),Emotion recognition,Activity recognition,Pattern recognition (psychology),Paleontology,Linguistics,Philosophy,Computer security,Management,Object (grammar),Economics,Biology",,https://openaccess.thecvf.com/content/CVPR2021/papers/Sanchez_Affective_Processes_Stochastic_Modelling_of_Temporal_Context_for_Emotion_and_CVPR_2021_paper.pdf,"Enrique Sanchez, Mani Kumar Tellamekala, Michel Valstar, Georgios Tzimiropoulos","Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or self-attention models to enforce temporal consistency, work on the feature level, ignoring the task-specific temporal dependencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Processes to propose a method for apparent emotion recognition with three key novel components: (a) probabilistic contextual representation with a global latent variable model; (b) temporal context modelling using task-specific predictions in addition to features; and (c) smart temporal context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-of-the-art methods."
CVPR2021,emotions,GANmut: Learning Interpretable Conditional Space for Gamut of Emotions,14,"Emotion and Mood Recognition,Generative Adversarial Networks and Image Synthesis,Face recognition and analysis","Categorical variable,Gamut,Computer science,Space (punctuation),USable,Artificial intelligence,Conditional random field,Variety (cybernetics),Code (set theory),Machine learning,Natural language processing,Multimedia,Operating system,Set (abstract data type),Programming language",,https://openaccess.thecvf.com/content/CVPR2021/papers/dApolito_GANmut_Learning_Interpretable_Conditional_Space_for_Gamut_of_Emotions_CVPR_2021_paper.pdf,"Stefano d'Apolito, Danda Pani Paudel, Zhiwu Huang, Andres Romero, Luc Van Gool","Humans can communicate emotions through a plethora of facial expressions, each with its own intensity, nuances and ambiguities. The generation of such variety by means of conditional GANs is limited to the expressions encoded in the used label system. These limitations are caused either due to burdensome labeling demand or the confounded label space. On the other hand, learning from inexpensive and intuitive basic categorical emotion labels leads to limited emotion variability. In this paper, we propose a novel GAN-based framework which learns an expressive and interpretable conditional space (usable as a label space) of emotions, instead of conditioning on handcrafted labels. Our framework only uses the categorical labels of basic emotions to jointly learn the conditional space as well as the emotion manipulation. Such learning can benefit from the image variability within discrete labels, especially when the intrinsic labels reside beyond the discrete space of the defined. Our experiments demonstrate the effectiveness of the proposed framework, by allowing us to control and generate a gamut of complex and compound emotions, while using only the basic categorical emotion labels during training."
ICASSP2024,emotion,Fine-Grained Disentangled Representation Learning For Multimodal Emotion Recognition.,1,"Emotion and Mood Recognition,Advanced Computing and Algorithms","Modality (human–computer interaction),Modalities,Computer science,Linear subspace,Feature learning,Encoder,Component (thermodynamics),Redundancy (engineering),Subspace topology,Representation (politics),Artificial intelligence,Natural language processing,Mathematics,Social science,Physics,Geometry,Sociology,Politics,Political science,Law,Thermodynamics,Operating system",,,"Mengchao Zhang,Mei Tu,Fan Zhang,Song Liu",
ICASSP2024,emotion,"MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr Error Detection, and Asr Error Correction.",5,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Speech recognition,Task (project management),Modality (human–computer interaction),Artificial intelligence,Modalities,Error detection and correction,Natural language processing,Coherence (philosophical gambling strategy),Margin (machine learning),Pattern recognition (psychology),Machine learning,Algorithm,Social science,Physics,Management,Quantum mechanics,Sociology,Economics",,,"Yidi Jiang,Zhengyang Chen,Ruijie Tao,Liqun Deng,Yanmin Qian,Haizhou Li",
ICASSP2024,emotion,Investigating Salient Representations and Label Variance in Dimensional Speech Emotion Analysis.,0,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Robustness (evolution),Speech recognition,Encoder,Artificial intelligence,Curse of dimensionality,Generalization,Subspace topology,Dimensionality reduction,Salient,Pattern recognition (psychology),Mathematics,Mathematical analysis,Biochemistry,Chemistry,Gene,Operating system",,,"Xupeng Zha,Huan Zhao,Zixing Zhang",
ICASSP2024,emotion,Esihgnn: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition.,1,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Mental Health via Writing","Computer science,Conversation,Graph,Utterance,Event (particle physics),Artificial intelligence,Context (archaeology),Theoretical computer science,Natural language processing,Psychology,Communication,Physics,Quantum mechanics,Paleontology,Biology",,,"Suwon Shon,Kwangyoun Kim,Prashant Sridhar,Yi-Te Hsu,Shinji Watanabe,Karen Livescu",
ICASSP2024,"emotion,emotional","Leveraging Speech PTM, Text LLM, And Emotional TTS For Speech Emotion Recognition.",5,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining","Computer science,Speech recognition,Speech synthesis,Task (project management),Artificial intelligence,Natural language processing,Emotion recognition,Representation (politics),Management,Politics,Political science,Law,Economics",,,"Jae-Sung Bae,Joun Yeop Lee,Ji-Hyun Lee,Seongkyu Mun,Taehwa Kang,Hoon-Young Cho,Chanwoo Kim",
ICASSP2023,emotional,Brain Network Features Differentiate Intentions from Different Emotional Expressions of the Same Text.,1,"EEG and Brain-Computer Interfaces,Neural and Behavioral Psychology Studies,Emotion and Mood Recognition","Electroencephalography,Praise,Irony,Computer science,Brain waves,Psychology,Expression (computer science),Brain activity and meditation,Speech recognition,Natural language processing,Cognitive psychology,Linguistics,Social psychology,Neuroscience,Philosophy,Programming language",,,"Vikramjit Mitra,Vasudha Kowtha,Hsiang-Yun Sherry Chien,Erdrin Azemi,Carlos Avendaño",
conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ICML2024,emotions,"The Good, The Bad, and Why: Unveiling Emotions in Generative AI",1,"Topic Modeling,Language and cultural evolution","Generative grammar,Cognitive science,Computer science,Mechanism (biology),Generative model,Artificial intelligence,Cognitive psychology,Psychology,Epistemology,Philosophy",,,"CHENG LI,Jindong Wang,Yixuan Zhang,Kaijie Zhu,Xinyi Wang,Wenxin Hou,Jianxun Lian,Fang Luo,Qiang Yang,Xing Xie","Abstract:     Emotion significantly impacts our daily behaviors and interactions. While recent generative AI models, such as large language models, have shown impressive performance in various tasks, it remains unclear whether they truly comprehend emotions and why. This paper aims to address this gap by incorporating psychological theories to gain a holistic understanding of emotions in generative AI models. Specifically, we propose three approaches: 1) EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI model performance, and 3) EmotionDecode to explain the effects of emotional stimuli, both benign and malignant. Through extensive experiments involving language and multi-modal models on semantic understanding, logical reasoning, and generation tasks, we demonstrate that both textual and visual EmotionPrompt can boost the performance of AI models while EmotionAttack can hinder it. More importantly, EmotionDecode reveals that AI models can comprehend emotional stimuli akin to the mechanism of dopamine in the human brain. Our work heralds a novel avenue for exploring psychology to enhance our understanding of generative AI models, thus boosting the research and development of human-AI collaboration and mitigating potential risks. "
ICML2023,affect,How much does Initialization Affect Generalization?,32274,"Advanced Neural Network Applications,Domain Adaptation and Few-Shot Learning,Advanced Image and Video Retrieval Techniques","Softmax function,Computer science,Overfitting,Convolutional neural network,Pooling,Dropout (neural networks),Artificial intelligence,Pattern recognition (psychology),Regularization (linguistics),Convolution (computer science),Deep neural networks,Word error rate,Deep learning,Artificial neural network,Machine learning",,http://proceedings.mlr.press/v202/ramasinghe23a/ramasinghe23a.pdf,"Sameera Ramasinghe,Lachlan E. MacDonald,Moshiur Farazi,Hemanth Saratchandran,Simon Lucey","Abstract:     Characterizing the remarkable generalization properties of over-parameterized neural networks remains an open problem. A growing body of recent literature shows that the bias of stochastic gradient descent (SGD) and architecture choice implicitly leads to better generalization. In this paper, we show on the contrary that, independently of architecture, SGD can itself be the cause of poor generalization if one does not ensure good initialization. Specifically, we prove that  any  differentiably parameterized model, trained under gradient flow, obeys a weak spectral bias law which states that sufficiently high frequencies train arbitrarily slowly. This implies that very high frequencies present at initialization will remain after training, and hamper generalization. Further, we empirically test the developed theoretical insights using practical, deep networks. Finally, we contrast our framework with that supplied by the  flat-minima  conjecture and show that Fourier analysis grants a more reliable framework for understanding the generalization of neural networks. "
ICASSP2024,emotion,TRUST-SER: On The Trustworthiness Of Fine-Tuning Pre-Trained Speech Embeddings For Speech Emotion Recognition.,1,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Music and Audio Processing","Computer science,Software deployment,Trustworthiness,Adversarial system,Field (mathematics),Deep learning,Artificial intelligence,Code (set theory),Emotion recognition,Machine learning,Speech recognition,Natural language processing,Data science,Computer security,Mathematics,Set (abstract data type),Pure mathematics,Programming language,Operating system",,,"Yuzhu Wang,Archontis Politis,Tuomas Virtanen",
ICASSP2023,emotion,Pre-Trained Model Representations and Their Robustness Against Noise for Speech Emotion Analysis.,3,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Robustness (evolution),Acoustic model,Artificial intelligence,Valence (chemistry),Natural language processing,Speech processing,Biochemistry,Chemistry,Physics,Quantum mechanics,Gene",,,"Jee-Weon Jung,Hee-Soo Heo,Bong-Jin Lee,Jaesung Huh,Andrew Brown,Youngki Kwon,Shinji Watanabe,Joon Son Chung",
ICASSP2024,emotion,Enhancing Two-Stage Finetuning for Speech Emotion Recognition Using Adapters.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Transformer,Emotion recognition,Adaptation (eye),Language model,Task (project management),Natural language processing,Artificial intelligence,Style (visual arts),Psychology,Physics,Management,Archaeology,Quantum mechanics,Voltage,Neuroscience,Economics,History",,,"Zhichao Wu,Qiulin Li,Sixing Liu,Qun Yang",
ICASSP2024,emotional,Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation.,0,"Persona Design and Applications,AI in Service Interactions,Topic Modeling","Persona,Conversation,Computer science,Consistency (knowledge bases),Semantic similarity,Similarity (geometry),Natural language processing,Artificial intelligence,Psychology,Human–computer interaction,Communication,Image (mathematics)",,,"Shivam Mehta,Ruibo Tu,Jonas Beskow,Éva Székely,Gustav Eje Henter",
ICASSP2024,emotion,Customising General Large Language Models for Specialised Emotion Recognition Tasks.,2,"Topic Modeling,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Computer science,Language model,Emotion recognition,Adaptation (eye),Artificial intelligence,Natural language processing,Robustness (evolution),Transferability,Cognitive psychology,Machine learning,Psychology,Biochemistry,Chemistry,Logit,Neuroscience,Gene",,,"Li Zhou,Wenyu Chen,Yong Cao,Dingyi Zeng,Wanlong Liu,Hong Qu",
ICASSP2024,"emotion,emotional",Frame-Level Emotional State Alignment Method for Speech Emotion Recognition.,2,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Utterance,Computer science,Frame (networking),Speech recognition,Focus (optics),Transformer,State (computer science),Cluster analysis,Artificial intelligence,Natural language processing,Emotion recognition,Pattern recognition (psychology),Algorithm,Telecommunications,Physics,Quantum mechanics,Voltage,Optics",,,"Minjie Tang,Hao Huang,Wenbo Zhang,Liang He",
ICASSP2024,emotion,Gradient-Based Dimensionality Reduction for Speech Emotion Recognition Using Deep Networks.,0,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","Computer science,Python (programming language),Dimensionality reduction,Artificial intelligence,Deep learning,Artificial neural network,Speech recognition,Convolutional neural network,Reduction (mathematics),Curse of dimensionality,Classifier (UML),Machine learning,Pattern recognition (psychology),Geometry,Mathematics,Operating system",,,"Jiankai Zhu,Peijie Huang,Ziheng Ruan,Yuhui Zhu,Chaojie Liang,Yuhong Xu",
CVPR2021,affective,ArtEmis: Affective Language for Visual Art,92,"Multimodal Machine Learning Applications,Visual Attention and Saliency Detection,Generative Adversarial Networks and Image Synthesis","Closed captioning,Computer science,Focus (optics),Set (abstract data type),Painting,Contrast (vision),Artificial intelligence,Cognitive psychology,Natural language processing,Psychology,Image (mathematics),Visual arts,Art,Physics,Optics,Programming language",,https://openaccess.thecvf.com/content/CVPR2021/papers/Achlioptas_ArtEmis_Affective_Language_for_Visual_Art_CVPR_2021_paper.pdf,"Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed Elhoseiny, Leonidas J. Guibas","We present a novel large-scale dataset and accompanying machine learning models aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, we focus on the affective experience triggered by visual artworks and ask the annotators to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. As we demonstrate below, this leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., ""freedom"" or ""love""), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. We focus on visual art (e.g., paintings, artistic photographs) as it is a prime example of imagery created to elicit emotional responses from its viewers. Our dataset, termed ArtEmis, contains 455K emotion attributions and explanations from humans, on 80K artworks from WikiArt. Building on this data, we train and demonstrate a series of captioning systems capable of expressing and explaining emotions from visual stimuli. Remarkably, the captions produced by these systems often succeed in reflecting the semantic and abstract content of the image, going well beyond systems trained on existing datasets."
ICASSP2024,emotion,Improving Domain Generalization in Speech Emotion Recognition with Whisper.,3,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Transformer,Paralanguage,Computer science,Recall,Speech recognition,Generalization,Emotion recognition,Variety (cybernetics),Artificial intelligence,Machine learning,Natural language processing,Cognitive psychology,Communication,Mathematics,Engineering,Mathematical analysis,Psychology,Voltage,Sociology,Electrical engineering",,,"Navin Raj Prabhu,Bunlong Lay,Simon Welker,Nale Lehmann-Willenbrock,Timo Gerkmann",
ICASSP2024,emotion,Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition.,8,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Transformer,Speech recognition,Spectrogram,Phrase,Utterance,Artificial intelligence,Natural language processing,Pattern recognition (psychology),Engineering,Voltage,Electrical engineering",,,"Yu Xi,Baochen Yang,Hao Li,Jiaqi Guo,Kai Yu",
ICASSP2024,"emoconv,emotion",EMOCONV-Diff: Diffusion-Based Speech Emotion Conversion for Non-Parallel and in-the-Wild Data.,1,"Speech and Audio Processing,Emotion and Mood Recognition,Music and Audio Processing","Utterance,Computer science,Speech recognition,Focus (optics),Emotion classification,Embedding,Arousal,Dimension (graph theory),Artificial intelligence,Natural language processing,Psychology,Mathematics,Physics,Neuroscience,Pure mathematics,Optics",,,"Binbin Shen,Jie Wang,Meng Meng,Yujun Wang",
ICASSP2024,"emotion,emotional",Disentanglement Network: Disentangle the Emotional Features from Acoustic Features for Speech Emotion Recognition.,2,"Speech and Audio Processing,Emotion and Mood Recognition","Emotion recognition,Speech recognition,Computer science",,,"Hui Yan,Zhenchun Lei,Changhong Liu,Yong Zhou",
ICASSP2023,emotion,Emotion Recognition in Conversation from Variable-Length Context.,2,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques","Conversation,Context (archaeology),Computer science,Variable (mathematics),Speech recognition,Human–computer interaction,Artificial intelligence,Psychology,Communication,Mathematics,Geology,Paleontology,Mathematical analysis",,,"Daofeng Liu,Fan Lyu,Linyan Li,Zhenping Xia,Fuyuan Hu",
ICASSP2024,emotion,Improving Speaker-Independent Speech Emotion Recognition using Dynamic Joint Distribution Adaptation.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Discriminative model,Speech recognition,Computer science,Joint probability distribution,Adaptation (eye),Speaker recognition,Joint (building),Feature (linguistics),Conditional probability distribution,Marginal distribution,Domain adaptation,Pattern recognition (psychology),Artificial intelligence,Mathematics,Psychology,Classifier (UML),Linguistics,Engineering,Statistics,Architectural engineering,Neuroscience,Philosophy,Random variable",,,"Dario Albesano,Nicola Ferri,Felix Weninger,Puming Zhan",
ICASSP2024,emotion,Improving Multi-Modal Emotion Recognition Using Entropy-Based Fusion and Pruning-Based Network Architecture Optimization.,0,"Speech and Audio Processing,Emotion and Mood Recognition,Video Surveillance and Tracking Methods","Computer science,Artificial intelligence,Entropy (arrow of time),Fuse (electrical),Fusion,Cross entropy,Modal,Pattern recognition (psychology),Artificial neural network,Machine learning,Redundancy (engineering),Engineering,Linguistics,Philosophy,Physics,Chemistry,Quantum mechanics,Polymer chemistry,Electrical engineering,Operating system",,,"Guorui Yu,Yimin Hu,Yiqian Xu,Yuejie Zhang,Rui Feng,Tao Zhang,Shang Gao",
ICASSP2024,emotion,CLAP4Emo: ChatGPT-Assisted Speech Emotion Retrieval with Natural Language Supervision.,0,"Music and Audio Processing,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Natural language processing,Benchmark (surveying),Natural language,Artificial intelligence,Language model,Speech recognition,Geodesy,Geography",,,"Xin Zhang,Yu Liu,Zhehuan Zhao",
ICASSP2024,emotion,Comparing data-Driven and Handcrafted Features for Dimensional Emotion Recognition.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Discriminative model,Leverage (statistics),Feature learning,Artificial intelligence,Speech recognition,Natural language processing,Representation (politics),Domain (mathematical analysis),Transfer of learning,Machine learning,Mathematical analysis,Mathematics,Politics,Political science,Law",,,"Woan-Shiuan Chien,Shreya G. Upadhyay,Chi-Chun Lee",
ICASSP2024,emotion,Emotion-Aware Contrastive Adaptation Network for Source-Free Cross-Corpus Speech Emotion Recognition.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Adaptation (eye),Domain adaptation,Consistency (knowledge bases),Artificial intelligence,Natural language processing,Class (philosophy),Task (project management),Speech recognition,Physics,Management,Classifier (UML),Optics,Economics",,,"Rijul Gupta,Catherine J. Madill,Dhanshree R. Gunjawate,Duy Duong Nguyen,Craig T. Jin",
ICASSP2024,emotion,Balancing Speaker-Rater Fairness for Gender-Neutral Speech Emotion Recognition.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Emotion recognition,Neutrality,Joint (building),Speaker recognition,Dual (grammatical number),Work (physics),Linguistics,Law,Architectural engineering,Mechanical engineering,Philosophy,Political science,Engineering",,,"Meena M. Chandra Shekar,John H. L. Hansen",
ICASSP2024,emotion,Prompting Audios Using Acoustic Properties for Emotion Representation.,0,"Music and Audio Processing,Emotion and Mood Recognition,Speech and Audio Processing","Computer science,Speech recognition,Representation (politics),Emotion recognition,Articulation (sociology),Facial expression,Natural language processing,Artificial intelligence,Politics,Political science,Law",,,"Yuan Xu,Meng Yang",
ICASSP2024,emotion,MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition in Real-Time Conversation.,1,"Speech Recognition and Synthesis,Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Conversation,Utterance,Task (project management),Benchmark (surveying),Context (archaeology),Speech recognition,Focus (optics),Granularity,Natural language processing,Speaker recognition,Artificial intelligence,Exploit,Psychology,Communication,Paleontology,Biology,Operating system,Physics,Computer security,Management,Geodesy,Optics,Economics,Geography",,,"Debaditya Shome,Ali Etemad",
ICASSP2024,"emotion,affect",Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Feature extraction,Inference,Margin (machine learning),Benchmark (surveying),Embedding,Artificial intelligence,Computation,Natural language processing,Machine learning,Algorithm,Geodesy,Geography",,,"Min Zhang,Jianfeng He,Shuo Lei,Murong Yue,Linhan Wang,Chang-Tien Lu",
ICASSP2024,emotion,Generalization of Self-Supervised Learning-Based Representations for Cross-Domain Speech Emotion Recognition.,0,"Speech Recognition and Synthesis,Emotion and Mood Recognition,Speech and Audio Processing","Paralanguage,Computer science,Generalization,Speech recognition,Domain (mathematical analysis),Benchmark (surveying),Representation (politics),Natural language processing,Artificial intelligence,Emotion recognition,Adaptation (eye),Psychology,Communication,Mathematics,Mathematical analysis,Geodesy,Neuroscience,Politics,Political science,Law,Geography",,,"Alexander H. Liu,Sung-Lin Yeh,James R. Glass",
ICASSP2024,emotion,Dynamic Speech Emotion Recognition Using A Conditional Neural Process.,0,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech Recognition and Synthesis","Computer science,Speech recognition,Sentence,Artificial intelligence,Context (archaeology),Emotion recognition,Valence (chemistry),Process (computing),Artificial neural network,Hidden Markov model,Natural language processing,Machine learning,Cognitive psychology,Psychology,Operating system,Paleontology,Physics,Quantum mechanics,Biology",,,"Mingqiu Wang,Izhak Shafran,Hagen Soltau,Wei Han,Yuan Cao,Dian Yu,Laurent El Shafey",
ICASSP2024,"emotion,emotional",Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition.,3,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Emotion recognition,Leverage (statistics),Speaker diarisation,Speech recognition,Speaker recognition,Cluster analysis,Task (project management),Artificial intelligence,Natural language processing,Management,Economics",,,"Zhenchun Lei,Hui Yan,Changhong Liu,Yong Zhou,Minglei Ma",
ICASSP2024,emotion,"Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting.",5,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Music and Audio Processing","Annotation,Computer science,Transcription (linguistics),Task (project management),Emotion recognition,Natural language processing,Artificial intelligence,Deep learning,Speech technology,Speech recognition,Speech processing,Philosophy,Linguistics,Management,Economics",,,"Chien-Yu Huang,Ke-Han Lu,Shih-Heng Wang,Chi-Yuan Hsiao,Chun-Yi Kuan,Haibin Wu,Siddhant Arora,Kai-Wei Chang,Jiatong Shi,Yifan Peng,Roshan S. Sharma,Shinji Watanabe,Bhiksha Ramakrishnan,Shady Shehata,Hung-Yi Lee",
ICASSP2024,emotion,Learning Arousal-Valence Representation from Categorical Emotion Labels of Speech.,0,Speech and Audio Processing,"Valence (chemistry),Arousal,Categorical variable,Speech recognition,Computer science,Representation (politics),Cognitive psychology,Natural language processing,Psychology,Artificial intelligence,Machine learning,Social psychology,Physics,Quantum mechanics,Politics,Political science,Law",,,"Haobin Tang,Xulong Zhang,Ning Cheng,Jing Xiao,Jianzong Wang",
ICASSP2024,"emotion,emotional",ED-TTS: Multi-Scale Emotion Modeling Using Cross-Domain Emotion Diarization for Emotional Speech Synthesis.,4,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Utterance,Computer science,Embedding,Speech recognition,Emotion classification,Prosody,Speaker diarisation,Probabilistic logic,Scale (ratio),Domain (mathematical analysis),Natural language processing,Artificial intelligence,Speaker recognition,Mathematics,Physics,Mathematical analysis,Quantum mechanics",,,"Qihao Yang,Xuelin Wang,Yong Li,Lap-Kei Lee,Fu Lee Wang,Tianyong Hao",
ICASSP2023,emotional,QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis.,9,"Speech Recognition and Synthesis,Phonetics and Phonology Research,Topic Modeling","Intonation (linguistics),Computer science,Syllable,Sentence,Speech synthesis,Speech recognition,Focus (optics),Style (visual arts),Control (management),Utterance,Natural language processing,Linguistics,Artificial intelligence,Philosophy,Physics,Archaeology,Optics,History",,,"Shulin He,Wei Rao,Jinjiang Liu,Jun Chen,Yukai Ju,Xueliang Zhang,Yannan Wang,Shidong Shang",
ICASSP2023,emotion,Exploring Attention Mechanisms for Multimodal Emotion Recognition in an Emergency Call Center Corpus.,6,"Emotion and Mood Recognition,Speech Recognition and Synthesis,Speech and Audio Processing","Emotive,Computer science,Encoder,Speech recognition,Emotion detection,Modality (human–computer interaction),Emotion recognition,Component (thermodynamics),Transformer,Natural language processing,Artificial intelligence,Human–computer interaction,Engineering,Philosophy,Physics,Epistemology,Thermodynamics,Operating system,Voltage,Electrical engineering",,,"Van-Thinh Nguyen,Hung-Cuong Pham,Dang-Khoa Mac",
ICASSP2024,emotion,MS-SENet: Enhancing Speech Emotion Recognition Through Multi-Scale Feature Fusion with Squeeze-and-Excitation Blocks.,1,"Emotion and Mood Recognition,Speech and Audio Processing,Speech Recognition and Synthesis","Overfitting,Computer science,Fuse (electrical),Dropout (neural networks),Benchmark (surveying),Artificial intelligence,Feature (linguistics),Convolutional neural network,Focus (optics),Feature extraction,Pattern recognition (psychology),Kernel (algebra),Speech recognition,Fusion,Machine learning,Artificial neural network,Engineering,Linguistics,Philosophy,Physics,Mathematics,Geodesy,Optics,Combinatorics,Geography,Electrical engineering",,,"Rosy Southwell,Wayne H. Ward,Viet Anh Trinh,Charis Clevenger,Clay Clevenger,Emily Watts,Jason G. Reitman,Sidney D'Mello,Jacob Whitehill",
ICASSP2023,emotion,Hierarchical Network with Decoupled Knowledge Distillation for Speech Emotion Recognition.,3,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Computer science,Artificial intelligence,Distillation,Utterance,Focus (optics),Representation (politics),Artificial neural network,Convolutional neural network,Machine learning,Boltzmann machine,Speech recognition,Natural language processing,Pattern recognition (psychology),Chemistry,Physics,Organic chemistry,Politics,Law,Political science,Optics",,,"Dimitris N. Makropoulos,Antigoni Tsiami,Aristides Prospathopoulos,Dimitris Kassis,Alexandros Frantzis,Emmanuel K. Skarsoulis,George Piperakis,Petros Maragos",
ICASSP2024,emotion,A Robust Pitch-Fusion Model for Speech Emotion Recognition in Tonal Languages.,1,"Speech and Audio Processing,Emotion and Mood Recognition,Music and Audio Processing","Computer science,Vietnamese,Speech recognition,Pipeline (software),Natural language processing,Encoder,Task (project management),Speech processing,Artificial intelligence,Representation (politics),Linguistics,Philosophy,Management,Politics,Political science,Law,Economics,Programming language,Operating system",,,"Egor Lakomkin,Chunyang Wu,Yassir Fathullah,Ozlem Kalinli,Michael L. Seltzer,Christian Fuegen",
ICASSP2023,emotion,General or Specific? Investigating Effective Privacy Protection in Federated Learning for Speech Emotion Recognition.,2,"Privacy-Preserving Technologies in Data,Speech Recognition and Synthesis,Hate Speech and Cyberbullying Detection","Computer science,Emotion recognition,Emotion detection,Internet privacy,Speech act,Speech recognition,Human–computer interaction,Linguistics,Philosophy",,,"Zheyu Wu,Ya-Feng Liu,Bo Jiang,Yu-Hong Dai",
ICASSP2024,emotion,Mels-Tts : Multi-Emotion Multi-Lingual Multi-Speaker Text-To-Speech System Via Disentangled Style Tokens.,0,"Speech Recognition and Synthesis,Speech and Audio Processing,Emotion and Mood Recognition","Computer science,Security token,Speech recognition,Style (visual arts),Speaker diarisation,Identity (music),Natural language processing,Speech synthesis,Speaker recognition,Artificial intelligence,Archaeology,History,Physics,Computer security,Acoustics",,,"Yinlin Guo,Haofan Huang,Xi Chen,He Zhao,Yuehai Wang",
ICASSP2024,emotional,PAVITS: Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion.,2,"Speech and Audio Processing,Music and Audio Processing,Speech Recognition and Synthesis","Naturalness,Prosody,Computer science,Speech recognition,Emotional prosody,Speech synthesis,Perception,Modalities,Natural language processing,Artificial intelligence,Psychology,Social science,Physics,Quantum mechanics,Neuroscience,Sociology",,,"Yinlong Xiao,Zongcheng Ji,Jianqiang Li",
ICASSP2023,emotion,Cross-Modal Fusion Techniques for Utterance-Level Emotion Recognition from Text and Speech.,7,"Emotion and Mood Recognition,Speech and Audio Processing,Music and Audio Processing","Modalities,Computer science,Utterance,Conversation,Modal,Speech recognition,Artificial intelligence,Bridging (networking),Natural language processing,Emotion recognition,Expression (computer science),Communication,Psychology,Computer network,Social science,Chemistry,Sociology,Polymer chemistry,Programming language",,,"Costas Mavromatis,George Karypis",
ICASSP2023,emotional,NSV-TTS: Non-Speech Vocalization Modeling And Transfer In Emotional Text-To-Speech.,2,"Speech Recognition and Synthesis,Speech and Audio Processing,Music and Audio Processing","Computer science,Speech recognition,Masking (illustration),Task (project management),Security token,Artificial intelligence,Transfer of learning,Natural language processing,Transfer (computing),Engineering,Visual arts,Art,Computer security,Systems engineering,Parallel computing",,,"Dexin Liao,Tao Jiang,Feng Wang,Lin Li,Qingyang Hong",
ICASSP2024,emotion,Cross-Subject EEG Emotion Recognition Based on Interconnected Dynamic Domain Adaptation.,1,"Emotion and Mood Recognition,EEG and Brain-Computer Interfaces,Sentiment Analysis and Opinion Mining","Computer science,Domain adaptation,Pattern recognition (psychology),Electroencephalography,Artificial intelligence,Convolution (computer science),Speech recognition,Emotion recognition,Invariant (physics),Interconnection,Artificial neural network,Classifier (UML),Mathematics,Psychology,Computer network,Psychiatry,Mathematical physics",,,"Ling Zhao,Shuaiqi Liu,Bing Li,Wenjia Cai,Ping Liang,Jie Yu,Jie Zhao",
ICASSP2023,emotion,Speech Emotion Recognition via Heterogeneous Feature Learning.,2,"Emotion and Mood Recognition,Music and Audio Processing,Speech and Audio Processing","Computer science,Discriminative model,Feature (linguistics),Feature learning,Mel-frequency cepstrum,Artificial intelligence,Speech recognition,Feature extraction,Pattern recognition (psychology),Feature vector,Representation (politics),Machine learning,Philosophy,Linguistics,Politics,Political science,Law",,,"Tomoro Tanaka,Kohei Yatabe,Yasuhiro Oikawa",
ICASSP2023,emotion,Improving EEG-based Emotion Recognition by Fusing Time-Frequency and Spatial Representations.,4,"EEG and Brain-Computer Interfaces,Emotion and Mood Recognition,Gaze Tracking and Assistive Technology","Computer science,Artificial intelligence,Electroencephalography,Pattern recognition (psychology),Frequency domain,Domain (mathematical analysis),Feature (linguistics),Feature extraction,Emotion recognition,Feature selection,Time–frequency analysis,Time domain,Selection (genetic algorithm),Machine learning,Speech recognition,Computer vision,Mathematics,Psychology,Mathematical analysis,Linguistics,Philosophy,Filter (signal processing),Psychiatry",,,"Hui Guo,Xin Wang,Siwei Lyu",
CVPR2024,emotional,Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion,2,"Human Motion and Animation,Human Pose and Action Recognition,Face recognition and analysis","Animation,Computer science,Diffusion,Computer graphics (images),Physics,Thermodynamics",,https://openaccess.thecvf.com/content/CVPR2024/papers/Chhatre_Emotional_Speech-driven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_CVPR_2024_paper.pdf,"Kiran Chhatre, Radek Dan??ek, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart",Existing methods for synthesizing 3D human gestures from speech have shown promising results but they do not explicitly model the impact of emotions on the generated gestures. Instead these methods directly output animations from speech without control over the expressed emotion. To address this limitation we present AMUSE an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e. gestures related to speech rhythm and word utterances) emotion and personal style are separable. To account for this AMUSE maps the driving audio to three disentangled latent vectors: one for content one for emotion and one for personal style. A latent diffusion model trained to generate gesture motion sequences is then conditioned on these latent vectors. Once trained AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative quantitative and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech. Our code is available at amuse.is.tue.mpg.de.
ICASSP2023,emotion,Speech Emotion Recognition Via Two-Stream Pooling Attention With Discriminative Channel Weighting.,3,"Speech and Audio Processing,Speech Recognition and Synthesis,Emotion and Mood Recognition","Discriminative model,Weighting,Computer science,Pooling,Feature (linguistics),Convolution (computer science),Channel (broadcasting),Artificial intelligence,Pattern recognition (psychology),Speech recognition,Set (abstract data type),Feature extraction,Dimension (graph theory),Machine learning,Artificial neural network,Mathematics,Medicine,Computer network,Linguistics,Philosophy,Pure mathematics,Radiology,Programming language",,,"Hao Tan,Jianjun Wang,Weichao Kong",
ICASSP2023,"emotion,emotions","Using Emotion Embeddings to Transfer Knowledge between Emotions, Languages, and Annotation Formats.",3,"Topic Modeling,Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques","Computer science,Annotation,Pooling,Inference,Natural language processing,Transition (genetics),Artificial intelligence,Process (computing),Information retrieval,Programming language,Biochemistry,Chemistry,Gene",,,"Djordje Batic,Giulia Tanoni,Lina Stankovic,Vladimir Stankovic,Emanuele Principi",
ICASSP2023,affect,Contextually-Rich Human Affect Perception Using Multimodal Scene Information.,5,"Emotion and Mood Recognition,Face recognition and analysis,Human Pose and Action Recognition","Computer science,Perception,Salient,Affect (linguistics),Leverage (statistics),Context (archaeology),Artificial intelligence,Computer vision,Psychology,Communication,Paleontology,Neuroscience,Biology",,,"Çagkan Yapar,Fabian Jaensch,Ron Levie,Gitta Kutyniok,Giuseppe Caire",
NIPS2024,affect,What Variables Affect Out-of-Distribution Generalization in Pretrained Models?,0,"Forecasting Techniques and Applications,demographic modeling and climate adaptation,Hydrology and Drought Analysis","Generalization,Affect (linguistics),Distribution (mathematics),Econometrics,Computer science,Psychology,Mathematics,Communication,Mathematical analysis",,https://papers.nips.cc/paper_files/paper/2024/file/6701e9c94bc7c7d6b5fc47c0fc13ab5b-Paper-Conference.pdf,"Md Yousuf Harun, Kyungbok Lee, Gianmarco Gallardo, Giri Krishnan, Christopher Kanan","Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts."
NIPS2024,emotional,EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas,-1,,,,https://papers.nips.cc/paper_files/paper/2024/file/611e84703eac7cc03f78339df8aae2ed-Paper-Conference.pdf,"Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Ivan Nasonov, Daniil Orekhov, Pekhotin Vladislav, Ivan Makovetskiy, Mikhail Baklashkin, Vasily Lavrentyev, Akim Tsvigun, Denis Turdakov, Tatiana Shavrina, Andrey Savchenko, Ilya Makarov","One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification only in pure natural language processing benchmarks can be insufficient. Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs. We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in various strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. Our game-theoretic analysis revealed that LLMs are susceptible to emotional biases influenced by model size, alignment strategies, and primary pretraining language. Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence. Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment. Our framework provides a foundational basis for developing such benchmarks."
NIPS2022,affected,What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment,3173,"Global Energy and Sustainability Research,Sustainable Development and Environmental Policy,Environmental, Ecological, and Cultural Studies","Club,Pride,Pleasure,SPARK (programming language),Political science,Environmental ethics,Action (physics),Psychology,Law,Computer science,Medicine,Philosophy,Physics,Quantum mechanics,Neuroscience,Anatomy,Programming language",,https://papers.nips.cc/paper_files/paper/2022/file/666cccc6376058e251315b4de7e085b9-Paper-Conference.pdf,Nathan Kallus,"The fundamental problem of causal inference -- that we never observe counterfactuals -- prevents us from identifying how many might be negatively affected by a proposed intervention. If, in an A/B test, half of users click (or buy, or watch, or renew, etc.), whether exposed to the standard experience A or a new one B, hypothetically it could be because the change affects no one,  because the change positively affects half the user population to go from no-click to click while negatively affecting the other half, or something in between. While unknowable, this impact is clearly of material importance to the decision to implement a change or not, whether due to fairness, long-term, systemic, or operational considerations. We therefore derive the tightest-possible (i.e., sharp) bounds on the fraction negatively affected (and other related estimands) given data with only factual observations, whether experimental or observational. Naturally, the more we can stratify individuals by observable covariates, the tighter the sharp bounds. Since these bounds involve unknown functions that must be learned from data, we develop a robust inference algorithm that is efficient almost regardless of how and how fast these functions are learned, remains consistent when some are mislearned, and still gives valid conservative bounds when most are mislearned. Our methodology altogether therefore strongly supports credible conclusions: it avoids spuriously point-identifying this unknowable impact, focusing on the best bounds instead, and it permits exceedingly robust inference on these. We demonstrate our method in simulation studies and in a case study of career counseling for the unemployed."
NIPS2024,emotion,EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning,-1,,,https://melangelabiiitd.github.io/EEVR/.,https://papers.nips.cc/paper_files/paper/2024/file/1cba8502063fab9df252a63968691768-Paper-Datasets_and_Benchmarks_Track.pdf,"Pragya Singh, Ritvik Budhiraja, Ankush Gupta, Anshul Goswami, Mohan Kumar, Pushpendra Singh","EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos.Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/."
NIPS2021,affects,How Data Augmentation affects Optimization for Linear Regression,3,"Sparse and Compressive Sensing Techniques,Stochastic Gradient Optimization Techniques,Statistical Methods and Inference","Hyperparameter,Stochastic gradient descent,Mathematical optimization,Computer science,Rate of convergence,Convex optimization,Stochastic optimization,Gradient descent,Optimization problem,Proximal Gradient Methods,Ranging,Regular polygon,Algorithm,Artificial intelligence,Mathematics,Key (lock),Artificial neural network,Computer security,Telecommunications,Geometry",,https://papers.nips.cc/paper_files/paper/2021/file/442b548e816f05640dec68f497ca38ac-Paper.pdf,"Boris Hanin, Yi Sun","Though data augmentation has rapidly emerged as a key tool for optimization in modern machine learning, a clear picture of how augmentation schedules affect optimization and interact with optimization hyperparameters such as learning rate is nascent. In the spirit of classical convex optimization and recent work on implicit bias, the present work analyzes the effect of augmentation on optimization in the simple convex setting of linear regression with MSE loss.We find joint schedules for learning rate and data augmentation scheme under which augmented gradient descent provably converges and characterize the resulting minimum. Our results apply to arbitrary augmentation schemes, revealing complex interactions between learning rates and augmentations even in the convex setting. Our approach interprets augmented (S)GD as a stochastic optimization method for a time-varying sequence of proxy losses. This gives a unified way to analyze learning rate, batch size, and augmentations ranging from additive noise to random projections. From this perspective, our results, which also give rates of convergence, can be viewed as Monro-Robbins type conditions for augmented (S)GD. "
ICML2021,affect,How Does Loss Function Affect Generalization Performance of Deep Learning? Application to Human Age Estimation,11,"Face recognition and analysis,Human Pose and Action Recognition,Video Surveillance and Tracking Methods","Affect (linguistics),Generalization,Computer science,Estimation,Artificial intelligence,Function (biology),Deep learning,Machine learning,Psychology,Mathematics,Engineering,Biology,Mathematical analysis,Communication,Systems engineering,Evolutionary biology",,,"Ali Akbari,Muhammad Awais,Manijeh Bashar,Josef Kittler","                         Abstract:                              Good generalization performance across a wide variety of domains caused by many external and internal factors is the fundamental goal of any machine learning algorithm.  This paper theoretically proves that the choice of loss function matters for improving the generalization performance of deep learning-based systems.  By deriving the generalization error bound for deep neural models trained by stochastic gradient descent, we pinpoint the characteristics of the loss function that is linked to the generalization error and can therefore be used for guiding the loss function selection process.  In summary, our main statement in this paper is: choose a stable loss function, generalize better. Focusing on human age estimation from the face which is a challenging topic in computer vision, we then propose a novel loss function for this learning problem. We theoretically prove that the proposed loss function achieves stronger stability, and consequently a tighter generalization error bound, compared to the other common loss functions for this problem. We have supported our findings theoretically, and demonstrated the merits of the guidance process experimentally, achieving significant improvements.                                                 "
NIPS2024,emotion,FindingEmo: An Image Dataset for Emotion Recognition in the Wild,0,Emotion and Mood Recognition,"Image (mathematics),Emotion recognition,Artificial intelligence,Computer science,Pattern recognition (psychology),Psychology,Cognitive psychology",,https://papers.nips.cc/paper_files/paper/2024/file/08a7229eaba3b35cd8f933ac678f2096-Paper-Datasets_and_Benchmarks_Track.pdf,"Laurent Mertens, Elahe Yargholi, Hans Op de Beeck, Jan Van den Stock, Joost Vennekens","We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code."
NIPS2024,affect,How does PDE order affect the convergence of PINNs?,243,"Model Reduction and Neural Networks,Fluid Dynamics and Vibration Analysis,Fluid Dynamics and Turbulent Flows","Physical system,Robustness (evolution),Computer science,Embedding,Artificial neural network,Partial differential equation,Nonlinear system,Complex system,Uncertainty quantification,Representation (politics),Theoretical computer science,Artificial intelligence,Machine learning,Mathematics,Physics,Mathematical analysis,Biochemistry,Chemistry,Quantum mechanics,Politics,Political science,Law,Gene",,https://papers.nips.cc/paper_files/paper/2024/file/00532321a253959cedc4f971b5524131-Paper-Conference.pdf,"Chang hoon Song, Yesom Park, Myungjoo Kang","This paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims."
NIPS2024,"affective,emotion",To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation,715,"Natural Language Processing Techniques,Topic Modeling,Speech and dialogue systems","Natural language generation,Computer science,Core (optical fiber),Relation (database),Field (mathematics),Task (project management),Artificial intelligence,State (computer science),Natural (archaeology),Emphasis (telecommunications),Natural language processing,Natural language,Data science,Programming language,Database,Mathematics,Systems engineering,Telecommunications,Archaeology,Pure mathematics,Engineering,History",https://github.com/ZhaoChenxi-nku/ECC.,https://papers.nips.cc/paper_files/paper/2024/file/f2bb120e9a2cb9c2a50921b7f865c421-Paper-Conference.pdf,"Chenxi Zhao, Jinglei Shi, Liqiang Nie, Jufeng Yang","Accuracy is a commonly adopted performance metric in various classification tasks, which measures the proportion of correctly classified samples among all samples. It assumes equal importance for all classes, hence equal severity for misclassifications. However, in the task of emotional classification, due to the psychological similarities between emotions, misclassifying a certain emotion into one class may be more severe than another, e.g., misclassifying 'excitement' as 'anger' apparently is more severe than as 'awe'. Albeit high meaningful for many applications, metrics capable of measuring these cases of misclassifications in visual emotion recognition tasks have yet to be explored. In this paper, based on Mikel's emotion wheel from psychology, we propose a novel approach for evaluating the performance in visual emotion recognition, which takes into account the distance on the emotion wheel between different emotions to mimic the psychological nuances of emotions. Experimental results in semi-supervised learning on emotion recognition and user study have shown that our proposed metrics is more effective than the accuracy to assess the performance and conforms to the cognitive laws of human emotions. The code is available at https://github.com/ZhaoChenxi-nku/ECC."
