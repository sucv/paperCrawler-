conf,matched_queries,title,citation_count,categories,concepts,code_url,pdf_url,authors,abstract
ACL2021,emotion,DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations,137,"Topic Modeling,Sentiment Analysis and Opinion Mining,Natural Language Processing Techniques","Computer science,Joint (building),Volume (thermodynamics),Natural language processing,Computational linguistics,Artificial intelligence,Linguistics,Association (psychology),Emotion recognition,Cognitive science,Psychology,Engineering,Philosophy,Architectural engineering,Physics,Quantum mechanics,Psychotherapist",,https://aclanthology.org/2021.acl-long.547.pdf,"Dou Hu,Lingwei Wei,Xiaoyong Huai","Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model."
ACL2021,emotion,MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation,145,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Multimodal Machine Learning Applications","Conversation,Computer science,Convolution (computer science),Artificial intelligence,Natural language processing,Joint (building),Graph,Computational linguistics,Emotion recognition,Speech recognition,Linguistics,Artificial neural network,Theoretical computer science,Engineering,Philosophy,Architectural engineering",,https://aclanthology.org/2021.acl-long.440.pdf,"Jingwen Hu,Yuchen Liu,Jinming Zhao,Qin Jin","Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting."
ACL2021,emotional,Supporting Cognitive and Emotional Empathic Writing of Students,11,"Topic Modeling,Text Readability and Simplification,Intelligent Tutoring Systems and Adaptive Learning","Computer science,Computational linguistics,Joint (building),Cognition,Cognitive science,Natural language processing,Volume (thermodynamics),Linguistics,Artificial intelligence,Psychology,Engineering,Philosophy,Neuroscience,Architectural engineering,Physics,Quantum mechanics",,https://aclanthology.org/2021.acl-long.314.pdf,"Thiemo Wambsganss,Christina Niklaus,Matthias Söllner,Siegfried Handschuh,Jan Marco Leimeister","We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German. We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. The obtained inter-rater agreement of α=0.79 for the components and the multi-π=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement. Moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location. We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use. Finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems."
ACL2021,emotional,Towards Emotional Support Dialog Systems,117,"Topic Modeling,Sentiment Analysis and Opinion Mining,Mental Health via Writing","Dialog box,Computational linguistics,Computer science,Natural language processing,Joint (building),Artificial intelligence,Linguistics,Library science,World Wide Web,Engineering,Philosophy,Architectural engineering",,https://aclanthology.org/2021.acl-long.269.pdf,"Siyang Liu,Chujie Zheng,Orianna Demasi,Sahand Sabour,Yu Li,Zhou Yu,Yong Jiang,Minlie Huang","Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems."
ACL2021,emotion,Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction,43,"Multimodal Machine Learning Applications,Topic Modeling,Sentiment Analysis and Opinion Mining","Computer science,Position paper,Association (psychology),Natural language processing,Joint (building),Position (finance),Computational linguistics,Graph,Artificial intelligence,Cognitive science,World Wide Web,Psychology,Engineering,Theoretical computer science,Architectural engineering,Psychotherapist,Finance,Economics",,https://aclanthology.org/2021.acl-long.261.pdf,"Hanqi Yan,Lin Gui,Gabriele Pergola,Yulan He","The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models."
ACL2021,emotion,Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities,93,"Emotion and Mood Recognition,Human Pose and Action Recognition,Generative Adversarial Networks and Image Synthesis","Modalities,Modality (human–computer interaction),Computer science,Joint (building),Volume (thermodynamics),Association (psychology),Natural language processing,Artificial intelligence,Cognitive science,Linguistics,Psychology,Engineering,Sociology,Philosophy,Social science,Architectural engineering,Physics,Quantum mechanics,Psychotherapist",,https://aclanthology.org/2021.acl-long.203.pdf,"Jinming Zhao,Ruichen Li,Qin Jin","Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions. Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at "
ACL2021,affects,Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?,8,"Topic Modeling,Speech and dialogue systems,Sentiment Analysis and Opinion Mining","Granularity,Computer science,Context (archaeology),Volume (thermodynamics),Computational linguistics,Joint (building),Tracking (education),State (computer science),Natural language processing,Data science,Artificial intelligence,Linguistics,Information retrieval,Sociology,History,Programming language,Engineering,Philosophy,Architectural engineering,Pedagogy,Physics,Archaeology,Quantum mechanics",,https://aclanthology.org/2021.acl-long.193.pdf,"Puhai Yang,Heyan Huang,Xian-Ling Mao","Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. First, we explore how greatly different granularities affect dialogue state tracking. Then, we further discuss how to combine multiple granularities for dialogue state tracking. Finally, we apply the findings about context granularity to few-shot learning scenario. Besides, we have publicly released all codes."
ACL2021,emotion,Distributed Representations of Emotion Categories in Emotion Space,12,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Joint (building),Computer science,Space (punctuation),Natural language processing,Computational linguistics,Volume (thermodynamics),Cognitive science,Association (psychology),Linguistics,Artificial intelligence,Psychology,Epistemology,Engineering,Philosophy,Physics,Operating system,Architectural engineering,Quantum mechanics",,https://aclanthology.org/2021.acl-long.184.pdf,"Xiangyu Wang,Chengqing Zong","Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space."
ACL2021,emotion,Directed Acyclic Graph Network for Conversational Emotion Recognition,183,"Sentiment Analysis and Opinion Mining,Topic Modeling,Emotion and Mood Recognition","Computer science,Directed acyclic graph,Natural language processing,Joint (building),Computational linguistics,Emotion recognition,Association (psychology),Artificial intelligence,Graph,Speech recognition,Theoretical computer science,Psychology,Algorithm,Engineering,Architectural engineering,Psychotherapist",,https://aclanthology.org/2021.acl-long.123.pdf,"Weizhou Shen,Siyue Wu,Yunyi Yang,Xiaojun Quan","The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC."
ACL2021,emotion,Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection,104,"Sentiment Analysis and Opinion Mining,Topic Modeling,Advanced Text Analysis Techniques","Computer science,Transformer,Joint (building),Computational linguistics,Natural language processing,Linguistics,Artificial intelligence,Engineering,Philosophy,Architectural engineering,Voltage,Electrical engineering",,https://aclanthology.org/2021.acl-long.125.pdf,"Lixing Zhu,Gabriele Pergola,Lin Gui,Deyu Zhou,Yulan He","Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories."
ACL2021,emoji,Assessing Emoji Use in Modern Text Processing Tools,7,"Digital Communication and Language,Sentiment Analysis and Opinion Mining,Hate Speech and Cyberbullying Detection","Emoji,Computational linguistics,Computer science,Natural language processing,Joint (building),Volume (thermodynamics),Artificial intelligence,Linguistics,Programming language,World Wide Web,Engineering,Social media,Philosophy,Architectural engineering,Physics,Quantum mechanics",,https://aclanthology.org/2021.acl-long.110v2.pdf,"Abu Awal Md Shoeb,Gerard de Melo","Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors. This also leads to an increased need for systems and tools to operate on text containing emojis. In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them. In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis. Our findings show that many systems still have notable shortcomings when operating on text containing emojis."
ACL2022,emotional,M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database,30,Sentiment Analysis and Opinion Mining,"Zhàng,Modal,Computer science,Natural language processing,Volume (thermodynamics),Artificial intelligence,Linguistics,Speech recognition,Information retrieval,History,Philosophy,Archaeology,Chemistry,Physics,Quantum mechanics,Polymer chemistry,China",,https://aclanthology.org/2022.acl-long.391.pdf,"Jinming Zhao,Tenggan Zhang,Jingwen Hu,Yuchen Liu,Qin Jin,Xinchao Wang,Haizhou Li","The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M"
ACL2022,emotions,SRL4E – Semantic Role Labeling for Emotions: A Unified Evaluation Framework,7,"Sentiment Analysis and Opinion Mining,Topic Modeling,Humor Studies and Applications","Computer science,Benchmark (surveying),Task (project management),Field (mathematics),Sentence,Domain (mathematical analysis),Sentiment analysis,Semantic role labeling,Natural language processing,Artificial intelligence,Semantics (computer science),Scheme (mathematics),Information retrieval,Data science,Mathematics,Management,Geodesy,Pure mathematics,Economics,Programming language,Geography,Mathematical analysis",,https://aclanthology.org/2022.acl-long.314.pdf,"Cesare Campagnano,Simone Conia,Roberto Navigli","In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of automatic systems not only to distinguish between emotions, but also to capture their semantic constituents. However, currently available gold datasets are heterogeneous in size, domain, format, splits, emotion categories and role labels, making comparisons across different works difficult and hampering progress in the area. In this paper, we tackle this issue and present a unified evaluation framework focused on Semantic Role Labeling for Emotions (SRL4E), in which we unify several datasets tagged with emotions and semantic roles by using a common labeling scheme. We use SRL4E as a benchmark to evaluate how modern pretrained language models perform and analyze where we currently stand in this task, hoping to provide the tools to facilitate studies in this complex area."
ACL2022,emotional,MISC: A Mixed Strategy-Aware Model integrating COMET for Emotional Support Conversation,49,"Mental Health via Writing,Emotion and Mood Recognition,Machine Learning in Healthcare","Conversation,Empathy,Computer science,Benchmark (surveying),Focus (optics),Emotional distress,Human–computer interaction,Emotion detection,Artificial intelligence,Emotion recognition,Psychology,Social psychology,Communication,Anxiety,Physics,Geodesy,Psychiatry,Optics,Geography",,https://aclanthology.org/2022.acl-long.25.pdf,"Quan Tu,Yanran Li,Jianwei Cui,Bin Wang,Ji-Rong Wen,Rui Yan","Applying existing methods to emotional support conversation—which provides valuable assistance to people who are in need—has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user’s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually reducing user’s distress. To address the problems, we propose a novel model "
ACL2023,emotion,Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression,1,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Face and Expression Recognition","Utterance,Computer science,Artificial intelligence,Regression,Bayesian probability,Prior probability,Perception,Voting,Pattern recognition (psychology),Natural language processing,Machine learning,Speech recognition,Mathematics,Statistics,Psychology,Neuroscience,Politics,Political science,Law",,https://aclanthology.org/2023.acl-long.873.pdf,"Wen Wu,Chao Zhang,Philip Woodland","In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground truth, it ignores the intrinsic uncertainty revealed by the inconsistent labels. This paper proposes a Bayesian approach, deep evidential emotion regression (DEER), to estimate the uncertainty in emotion attributes. Treating the emotion attribute labels of an utterance as samples drawn from an unknown Gaussian distribution, DEER places an utterance-specific normal-inverse gamma prior over the Gaussian likelihood and predicts its hyper-parameters using a deep neural network model. It enables a joint estimation of emotion attributes along with the aleatoric and epistemic uncertainties. AER experiments on the widely used MSP-Podcast and IEMOCAP datasets showed DEER produced state-of-the-art results for both the mean values and the distribution of emotion attributes."
ACL2023,emotion,A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations,9,Emotion and Mood Recognition,"Computer science,Utterance,Facial expression,Speech recognition,Task (project management),Artificial intelligence,Modalities,Face (sociological concept),Benchmark (surveying),Pipeline (software),Cluster analysis,Facial recognition system,Focus (optics),Expression (computer science),Pattern recognition (psychology),Social science,Physics,Management,Geodesy,Sociology,Optics,Economics,Programming language,Geography",,https://aclanthology.org/2023.acl-long.861.pdf,"Wenjie Zheng,Jianfei Yu,Rui Xia,Shijin Wang","Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people’s faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expressionaware Multimodal Multi-Task learning (FacialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at "
ACL2023,emotion,MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations,15,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition","Computer science,Modalities,Conversation,Complementarity (molecular biology),Artificial intelligence,Natural language processing,Benchmark (surveying),Focus (optics),Multimodality,Correlation,Task (project management),Emotion recognition,Psychology,Communication,Social science,Physics,Geometry,Mathematics,Management,Geodesy,Sociology,Biology,World Wide Web,Optics,Economics,Genetics,Geography",,https://aclanthology.org/2023.acl-long.824.pdf,"Tao Shi,Shao-Lun Huang","Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping relationships across different modalities. Furthermore, existing state-of-the-art ERC models have difficulty classifying minority and semantically similar emotion categories. To address these challenges, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross-attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion categories on both datasets, the improvements in minority and semantically similar emotions are especially significant."
ACL2023,emotion,A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation,11,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Conversation,Natural language processing,Artificial intelligence,Modalities,Transformer,Locality,Modality (human–computer interaction),Linguistics,Social science,Philosophy,Physics,Quantum mechanics,Voltage,Sociology",,https://aclanthology.org/2023.acl-long.732.pdf,"Xiaoheng Zhang,Yang Li","Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these limitations, we propose a cross-modality context fusion and semantic refinement network (CMCF-SRNet). Specifically, we first design a cross-modal locality-constrained transformer to explore the multimodal interaction. Second, we investigate a graph-based semantic refinement transformer, which solves the limitation of insufficient semantic relationship information between utterances. Extensive experiments on two public benchmark datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, indicating its potential application in emotion recognition. Our model will be available at "
ACL2023,emotion,PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent,2,"Mental Health via Writing,Sentiment Analysis and Opinion Mining,Digital Mental Health Interventions","Politeness,Feeling,Empathy,Psychology,Applied psychology,Function (biology),Internet privacy,Psychotherapist,Social psychology,Computer science,Philosophy,Linguistics,Evolutionary biology,Biology",,https://aclanthology.org/2023.acl-long.685.pdf,"Kshitij Mishra,Priyanshu Priya,Asif Ekbal","The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client’s emotions and ensure client’s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client’s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client’s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need."
ACL2023,emotion,Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification,2,"Sentiment Analysis and Opinion Mining,Emotion and Mood Recognition,Text and Document Classification Technologies","Computer science,Hyperbolic space,Benchmark (surveying),Task (project management),Space (punctuation),Code (set theory),Artificial intelligence,Euclidean geometry,Euclidean space,Euclidean distance,Entropy (arrow of time),Source code,Semantic space,Theoretical computer science,Machine learning,Mathematics,Physics,Geometry,Management,Geodesy,Set (abstract data type),Quantum mechanics,Pure mathematics,Economics,Programming language,Geography,Operating system",,https://aclanthology.org/2023.acl-long.613.pdf,"Chih Yao Chen,Tun Min Hung,Yi-Li Hsu,Lun-Wei Ku","Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate hyperbolic embeddings to improve the FEC task. First, we learn label embeddings in the hyperbolic space to better capture their hierarchical structure, and then our model projects contextualized representations to the hyperbolic space to compute the distance between samples and labels. Experimental results show that incorporating such distance to weight cross entropy loss substantially improve the performance on two benchmark datasets, with around 3% improvement compared to previous state-of-the-art, and could even improve up to 8.6% when the labels are hard to distinguish. Code is available at "
ACL2023,affect,How Do In-Context Examples Affect Compositional Generalization?,9,"Topic Modeling,Neurobiology of Language and Bilingualism,Computational and Text Analysis Methods","Generalization,Zhàng,Chen,Context (archaeology),Affect (linguistics),Computer science,Linguistics,Natural language processing,Epistemology,History,Philosophy,Geology,China,Archaeology,Paleontology",,https://aclanthology.org/2023.acl-long.618.pdf,"Shengnan An,Zeqi Lin,Qiang Fu,Bei Chen,Nanning Zheng,Jian-Guang Lou,Dongmei Zhang","Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm."
ACL2023,emotion,Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations,21,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Anomaly Detection Techniques and Applications","Adversarial system,Computer science,Artificial intelligence,Robustness (evolution),Class (philosophy),Machine learning,Context (archaeology),Consistency (knowledge bases),Feature learning,Boosting (machine learning),Feature (linguistics),Natural language processing,Pattern recognition (psychology),Linguistics,Paleontology,Biochemistry,Chemistry,Philosophy,Biology,Gene",,https://aclanthology.org/2023.acl-long.606.pdf,"Dou Hu,Yinan Bao,Lingwei Wei,Wei Zhou,Songlin Hu","Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model’s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT."
ACL2023,emotion,Unsupervised Extractive Summarization of Emotion Triggers,2,"Sentiment Analysis and Opinion Mining,Advanced Text Analysis Techniques,Topic Modeling","Automatic summarization,Computer science,Context (archaeology),Artificial intelligence,Code (set theory),Labeled data,Natural language processing,PageRank,Machine learning,Data science,Information retrieval,Paleontology,Set (abstract data type),Biology,Programming language",,https://aclanthology.org/2023.acl-long.531.pdf,"Tiberiu Sosea,Hongli Zhan,Junyi Jessy Li,Cornelia Caragea","Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting (Zhan et al., 2022)’s abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from external sources combined with a language understanding module, and outperforms strong baselines. We release our data and code at "
ACL2023,affection,CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,9,"Topic Modeling,Multimodal Machine Learning Applications,Natural Language Processing Techniques","Affection,Empathy,Cognition,Conversation,Computer science,Graph,Cognitive science,Isolation (microbiology),Psychology,Cognitive psychology,Social psychology,Communication,Theoretical computer science,Neuroscience,Microbiology,Biology",,https://aclanthology.org/2023.acl-long.457.pdf,"Jinfeng Zhou,Chujie Zheng,Bo Wang,Zheng Zhang,Minlie Huang","Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user’s cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses."
ACL2023,emotion,DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations,21,"Emotion and Mood Recognition,Sentiment Analysis and Opinion Mining,Speech and dialogue systems","Computer science,Conversation,Dual (grammatical number),Context (archaeology),Graph,Natural language processing,Emotion recognition,Artificial intelligence,Human–computer interaction,Theoretical computer science,Linguistics,Paleontology,Philosophy,Biology",,https://aclanthology.org/2023.acl-long.408.pdf,"Duzhen Zhang,Feilong Chen,Xiuyi Chen","Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT) module to incorporate discourse structural information by analyzing the discourse dependencies between utterances. Additionally, we develop a Speaker-aware GAT (SpkGAT) module to incorporate speaker-aware contextual information by considering the speaker dependencies between utterances. Furthermore, we design an interaction module that facilitates the integration of the DisGAT and SpkGAT modules, enabling the effective interchange of relevant information between the two modules. We extensively evaluate our method on four datasets, and experimental results demonstrate that our proposed DualGATs surpass state-of-the-art baselines on the majority of the datasets."
ACL2023,emotional,Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations,11,"Speech and dialogue systems,AI in Service Interactions,Topic Modeling","Conversation,Schema (genetic algorithms),Computer science,Empathy,Knowledge graph,Multimethodology,Knowledge management,Psychology,Artificial intelligence,Social psychology,Information retrieval,Mathematics education,Communication",,https://aclanthology.org/2023.acl-long.225.pdf,"Yang Deng,Wenxuan Zhang,Yifei Yuan,Wai Lam","Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimental results on two ESC datasets show the superiority of KEMI in both content-preserving evaluation and mixed initiative related analyses."
ACL2023,"emotion,emotional",Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach,9,"Mental Health via Writing,Stuttering Research and Treatment,Speech and dialogue systems","Conversation,Supporter,Coherence (philosophical gambling strategy),Reinforcement learning,Computer science,Turn-taking,Process (computing),Cognitive psychology,Psychology,Human–computer interaction,Artificial intelligence,Communication,Physics,Archaeology,Quantum mechanics,History,Operating system",,https://aclanthology.org/2023.acl-long.96.pdf,"Jinfeng Zhou,Zhuang Chen,Bo Wang,Minlie Huang","Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., "
ACL2023,emotion,Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction,3,"Sentiment Analysis and Opinion Mining,Text and Document Classification Technologies,Advanced Text Analysis Techniques","Robustness (evolution),Computer science,Graph,Relationship extraction,Artificial intelligence,Decision boundary,Representation (politics),Machine learning,Labeled data,Data mining,Theoretical computer science,Support vector machine,Information extraction,Biochemistry,Chemistry,Politics,Political science,Law,Gene",,https://aclanthology.org/2023.acl-long.62.pdf,"Huawen Feng,Junlong Liu,Junhao Zheng,Haibin Chen,Xichen Shang,Qianli Ma","Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using the same graph structure. In this paper, we propose a **J**oint **C**onstrained Learning framework with **B**oundary-adjusting for Emotion-Cause Pair Extraction (**JCB**). Specifically, through constrained learning, we summarize the prior rules existing in the data and force the model to take them into consideration in optimization, which helps the model learn a better representation from unbalanced data. Furthermore, we adjust the decision boundary of classifiers according to the relations between subtasks, which have always been ignored. No longer working independently as in the previous framework, the classifiers corresponding to three subtasks cooperate under the relation constraints. Experimental results show that **JCB** obtains competitive results compared with state-of-the-art methods and prove its robustness on unbalanced data."
ACL2023,emotion,Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition,10,Emotion and Mood Recognition,"Ping (video games),Modal,Modality (human–computer interaction),Computer science,Artificial intelligence,Independence (probability theory),Natural language processing,Linguistics,Speech recognition,Mathematics,Philosophy,Statistics,Computer security,Chemistry,Polymer chemistry",,https://aclanthology.org/2023.acl-long.39v2.pdf,"Jun Sun,Shoukang Han,Yu-Ping Ruan,Xiaoning Zhang,Shu-Kai Zheng,Yulong Liu,Yuxin Huang,Taihao Li","Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset with Modality-wise Annotions, abbreviated as CHERMA, provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. The model consists of uni-modal transformer modules that learn representations for each modality, and a multi-modal transformer module that fuses all modalities. All the modules are supervised by their corresponding labels separately, and the forward information flow is uni-directionally from the uni-modal modules to the multi-modal module. The supervision strategy and the model architecture guarantee each individual modality learns its representation independently, and meanwhile the multi-modal module aggregates all information. Extensive empirical results demonstrate that our proposed scheme outperforms state-of-the-art alternatives, corroborating the importance of modality independence in multi-modal emotion recognition. The dataset and codes are availabel at "
ACL2024,emotional,Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation,2,Mental Health via Writing,"Supporter,Conversation,Preference,Computer science,Psychology,Communication,Microeconomics,Archaeology,Economics,History",,https://aclanthology.org/2024.acl-long.813.pdf,"Dongjin Kang,Sunghwan Kim,Taeyoon Kwon,Seungjun Moon,Hyunsouk Cho,Youngjae Yu,Dongha Lee,Jinyoung Yeo","Emotional Support Conversation (ESC) is a task aimed at alleviating individuals’ emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs."
ACL2024,emotion,The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition,0,"Speech and Audio Processing,Speech Recognition and Synthesis","Computer science,Speech recognition,Transformer,Emotion recognition,Artificial intelligence,Natural language processing,Engineering,Electrical engineering,Voltage",,https://aclanthology.org/2024.acl-long.752.pdf,"Enshi Zhang,Rafael Trujillo,Christian Poellabauer","Research in the field of speech emotion recognition (SER) relies on the availability of comprehensive datasets to make it possible to design accurate emotion detection models. This study introduces the Multimodal Emotion Recognition and Sentiment Analysis (MERSA) dataset, which includes both natural and scripted speech recordings, transcribed text, physiological data, and self-reported emotional surveys from 150 participants collected over a two-week period. This work also presents a novel emotion recognition approach that uses a transformer-based model, integrating pre-trained wav2vec 2.0 and BERT for feature extractions and additional LSTM layers to learn hidden representations from fused representations from speech and text. Our model predicts emotions on dimensions of arousal, valence, and dominance. We trained and evaluated the model on the MSP-PODCAST dataset and achieved competitive results from the best-performing model regarding the concordance correlation coefficient (CCC). Further, this paper demonstrates the effectiveness of this model through cross-domain evaluations on both IEMOCAP and MERSA datasets."
ACL2024,emotional,ESCoT: Towards Interpretable Emotional Support Dialogue Systems,0,"Speech and dialogue systems,Topic Modeling,Natural Language Processing Techniques","Computer science,Human–computer interaction,Artificial intelligence",,https://aclanthology.org/2024.acl-long.723.pdf,"Tenggan Zhang,Xinjie Zhang,Jinming Zhao,Li Zhou,Qin Jin","Understanding the reason for emotional support response is crucial for establishing connections between users and emotional support dialogue systems. Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. To empower the system with better interpretability, we propose an emotional support response generation scheme, named "
ACL2024,emotional,Self-chats from Large Language Models Make Small Emotional Support Chatbot Better,0,AI in Service Interactions,"Computer science,Chatbot,Natural language processing",https://github.com/pandazzh2020/ExTES.,https://aclanthology.org/2024.acl-long.611.pdf,"Zhonghua Zheng,Lizi Liao,Yang Deng,Libo Qin,Liqiang Nie","Large Language Models (LLMs) have shown strong generalization abilities to excel in various tasks, including emotion support conversations. However, deploying such LLMs like GPT-3 (175B parameters) is resource-intensive and challenging at scale. In this study, we utilize LLMs as “Counseling Teacher” to enhance smaller models’ emotion support response abilities, significantly reducing the necessity of scaling up model size. To this end, we first introduce an iterative expansion framework, aiming to prompt the large teacher model to curate an expansive emotion support dialogue dataset. This curated dataset, termed ExTES, encompasses a broad spectrum of scenarios and is crafted with meticulous strategies to ensure its quality and comprehensiveness. Based on this, we then devise a Diverse Response Inpainting (DRI) mechanism to harness the teacher model to produce multiple diverse responses by filling in the masked conversation context. This richness and variety serve as instructive examples, providing a robust foundation for fine-tuning smaller student models. Experiments across varied scenarios reveal that the teacher-student scheme with DRI notably improves the response abilities of smaller models, even outperforming the teacher model in some cases. The dataset and codes are available in https://github.com/pandazzh2020/ExTES."
ACL2024,affected,VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model,2,"Web Data Mining and Analysis,Digital and Cyber Forensics,Spam and Phishing Detection","Computer science,Vulnerability (computing),Natural language processing,Language model,Programming language,Artificial intelligence,Computer security",,https://aclanthology.org/2024.acl-long.527.pdf,"Tianyu Chen,Lin Li,ZhuLiuchuan ZhuLiuchuan,Zongyang Li,Xueqing Liu,Guangtai Liang,Qianxiang Wang,Tao Xie","Security practitioners maintain vulnerability reports (e.g., GitHub Advisory) to help developers mitigate security risks. An important task for these databases is automatically extracting structured information mentioned in the report, e.g., the affected software packages, to accelerate the defense of the vulnerability ecosystem.However, it is challenging for existing work on affected package identification to achieve high precision. One reason is that all existing work focuses on relatively smaller models, thus they cannot harness the knowledge and semantic capabilities of large language models.To address this limitation, we propose VulLibGen, the first method to use LLM for affected package identification. In contrast to existing work, VulLibGen proposes the novel idea to directly generate the affected package. To improve the precision, VulLibGen employs supervised fine-tuning (SFT), retrieval augmented generation (RAG) and a local search algorithm. The local search algorithm is a novel post-processing algorithm we introduce for reducing the hallucination of the generated packages. Our evaluation results show that VulLibGen has an average precision of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best average precision in previous work is 0.721. Additionally, VulLibGen has high value to security practice: we submitted 60 <vulnerability, affected package> pairs to GitHub Advisory (covers four ecosystems) and 34 of them have been accepted and merged."
ACL2024,emotion,"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",1,"Gender, Feminism, and Media,Resilience and Mental Health,Populism, Right-Wing Movements","Attribution,Psychology,Social psychology,Computer science",,https://aclanthology.org/2024.acl-long.415.pdf,"Flor Miriam Plaza-del-Arco,Amanda Cercas Curry,Alba Curry,Gavin Abercrombie,Dirk Hovy","Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men’s anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like ‘When I had a serious argument with a dear person’. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications."
